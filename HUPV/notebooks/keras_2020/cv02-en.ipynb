{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv02-en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kocurvik/edu/blob/master/PNSPV/notebooky/keras_2020/cv02-en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFF-Tzky4FpK"
      },
      "source": [
        "# **2nd Lab** - Linear algebra with NumPy, Fully connected neural network\n",
        "\n",
        "In this notebook we will go over basic linear algebra operations with NumPy and build our own neural network from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFCxQ5FfX-GE"
      },
      "source": [
        "#Linear Algebra with NumPy\n",
        "\n",
        "To implement a fully connected neural network we will need to utlize matrix multiplication. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFOOIwOB5mN-"
      },
      "source": [
        "## Matrix multiplication\n",
        "The most important matrix operation we will use is matrix multipllication. We will start with a definition:\n",
        "\n",
        "Let $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n \\times l}, \\mathbf{C} \\in \\mathbb{R}^{m \\times l} $ then $\\mathbf{A}\\mathbf{B} = \\mathbf{C} \\iff (\\forall i \\in \\hat{m})(\\forall j \\in \\hat{l})(c_{i,j} = \\sum_{k=1}^{n} a_{i,k} \\cdot b_{k,j})$ \n",
        "\n",
        "It is important to remember that we the first subscript of a matrix element indicates the row and the second one indicates column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzL2kACy7hDF"
      },
      "source": [
        "### Úloha 1 (On the whiteboard)\n",
        "\n",
        "Calculate $\\mathbf{A}\\mathbf{B}$, $\\mathbf{B}\\mathbf{C}$, $\\mathbf{C}\\mathbf{B}$\n",
        "\n",
        "$\\mathbf{A} = \\begin{bmatrix} \n",
        "3 & 5 & -1 \\\\\n",
        "2 & -4 & 2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{B} = \\begin{bmatrix} \n",
        "5 & 2 & 1 \\\\\n",
        "-6 & 5 & 2 \\\\\n",
        "3 & 4 & -1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{C} = \\begin{bmatrix} \n",
        "4 & -4 & 3 \\\\\n",
        "-6 & -3 & 4 \\\\\n",
        "-1 & 1 & 0\n",
        "\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJcOg7TqNbU1"
      },
      "source": [
        "In NumPy we can multiply matrices by using np.matmul(a, b), or by using the @ operator. We can also use np.dot. The difference between these two approaches is in broadcasting. Test this out for $\\mathbf{BC}$ a $\\mathbf{CB}$\n",
        "\n",
        "*Note:* Vectors can be though of as matrices with one of the dimensions being one. The singleton dimension determines whether it is a row or a column vector. Theoretically this affects the multiplication, but this might not be reflected in when working with NumPy. For example when using an array with len(np.shape) == 1 it is considered to be a row or a column vector based on what is better applicable in the current call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fMUJVrOAqW"
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[3,5,-1],[2,-4,2]])\n",
        "b = np.array([[5,2,1],[-6,5,2],[3,4,-1]])\n",
        "print(np.matmul(a,b))\n",
        "d = a @ b\n",
        "print(d)\n",
        "v = np.array([10,20,30])\n",
        "print(v.shape)\n",
        "u = np.array([5,25])\n",
        "print(u.shape)\n",
        "print(a @ v)\n",
        "print(u @ a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aoY3A8ORqHp"
      },
      "source": [
        "If we try to multiply a tensor of rank greater than 2 matmul will consider it as a list of matrices. If we need to perform something more fancy we can utilize np.einsum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl-b9KOdXf_6"
      },
      "source": [
        "## Scalar multiplication\n",
        "\n",
        "We can easily multiply a matrix by a scalar. This can be done with either np.multiply or with the * operator. If both of the operands are tensors then the multiplication will be carried out elementwise with broadcasting if applicable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQsMp1YfYYeo"
      },
      "source": [
        "print(a * d)\n",
        "print(5 * a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XExe4ma_Qlyv"
      },
      "source": [
        "## Transposed matrices\n",
        "\n",
        "Definition: Let $\\mathbf{A} \\in \\mathbb{R}^{m,n}$ then $\\mathbf{A}^T \\in \\mathbb{R}^{n,m}$ is its transposed matrix $\\iff (\\forall i \\in \\hat{m})(\\forall j \\in \\hat{n})(a_{i,j} = a^T_{j,i})$ \n",
        "\n",
        "In NumPy we simply call the the method .T of any np.array. We can also use the function np.transpose. We can also use this to convert a row vector to a column vector and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjACfi6Ro84"
      },
      "source": [
        "print(a.T)\n",
        "print(np.transpose(a))\n",
        "\n",
        "r = np.array([[100,10,1]])\n",
        "print(r.shape)\n",
        "\n",
        "c = np.array([[3],[7],[8]])\n",
        "print(c.shape)\n",
        "\n",
        "print(a @ c)\n",
        "print(a @ r.T)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUgGLfLYXggG"
      },
      "source": [
        "## Sums and statistics\n",
        "\n",
        "NumPy enables us to calculate various statistics and sums of tensors. We can use np.sum, np.mean, np.std etc. Sometimes we desire to calculate the value only over one dimension. In that case we can use the axis keyword. Many other methods exist in NumPy which work in a similar way such as np.median or np.prod."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caACn0c6-fOo"
      },
      "source": [
        "print(np.sum(a))\n",
        "print(np.sum(a, axis=0))\n",
        "print(np.sum(a, axis=1))\n",
        "\n",
        "print(np.mean(a))\n",
        "print(np.mean(a, axis=0))\n",
        "print(np.mean(a, axis=1))\n",
        "\n",
        "\n",
        "print(np.std(a))\n",
        "print(np.std(a, axis=0))\n",
        "print(np.std(a, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ScSyiORa836"
      },
      "source": [
        "# Fully connected neural networks\n",
        "A neural network is a biologically inspired model used to perform various functions. Today we will use it as classifier.\n",
        "\n",
        "In general, we model the network as an orientede graph with valued edges nodes of which are the so-called neurons. Every neuron has its own activation, which is computed based on activations of neurons it is connected to. Any activation can then also affect further neurons. In the most general case we can describe the network activations as:\n",
        "\n",
        "$$a_p = f \\left( \\sum_{q \\in p_{in}} \\left( w_{p,q} a_q \\right) + b_p \\right) = f\\left(z_p\\right),$$\n",
        "\n",
        "where $a_p$ is the activation of a given node, $w_{p,q}$ is the weight between the node $q$ and $p$, $b_p$ is the bias of the node $p$, $f$ is the activation function, $z_p$ is a simplified expression we will use later. When using this broad definition it is possible for the neurons to be connected in a cyclic way which is undesirable. We also do not want the structure of the network to be too complicated. We will therefore introduce a fully connected neural network sometimes also denoted as a Multi-Layer-Perceptron. The added structure lies in the fact that every neuron has belongs to a layer and each neuron in a layer receives its inputs from all of the neurons in the previous layer.\n",
        "\n",
        "![Fully connected neural network](https://raw.githubusercontent.com/kocurvik/edu/master/PNSPV/supplementary/ntb_images/NN1.jpg)\n",
        "\n",
        "Activations in the network can be then expressed as:\n",
        "\n",
        "$$a_j^l = f \\left( \\sum_{k} \\left( a_k^{l-1} w_{k,j}^l \\right) + b_j^l \\right) = f \\left(z_j^l \\right),$$\n",
        "\n",
        "or in vector form:\n",
        "\n",
        "$$a^l = f \\left( a^{l-1}w^l+ b^l \\right) =  f \\left(z^l \\right),$$\n",
        "\n",
        "where $a^l$ is the **row** vector of activations, $w^l$ is a weight matrix if $size(l-1) \\times size(l)$, $b_l$ is a **column** vector of biases, $f$ is the scalar activation function, which is applied elementwise in the vector case, $z_l$ is also a row vector which is useful for simplifying expressions. The superscript indicates which layer the neuron belongs to.\n",
        "\n",
        "For a better understanding see the following images:\n",
        "\n",
        "![Indexy aktivácie a prahu](https://raw.githubusercontent.com/kocurvik/edu/master/PNSPV/supplementary/ntb_images/activation_bias.jpg)\n",
        "![Indexy váh](https://raw.githubusercontent.com/kocurvik/edu/master/PNSPV/supplementary/ntb_images/weight.jpg)\n",
        "\n",
        "*Note.:* The vectors could also be column vectors (this would result in the weight matrix being transposed), but this would create a few problems during implementation which arise due to broadcasting.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIld0vL36Aqs"
      },
      "source": [
        "### Activation functions\n",
        "\n",
        "We can use various activation functions. Today we will only use the sigmoid, but in future we will also use the other ones.\n",
        "\n",
        "1. Sigmoid: $f(z) = \\frac{1}{1 + e^{-z}}$\n",
        "2. Tanh: $f(z) = tanh(z)$\n",
        "3. ReLU: $f(z) = max(x,0)$\n",
        "4. SoftPlus: $f(z) = ln(1 + e^z)$\n",
        "5. LeakyReLU: $f(z) = max(x,ax), a \\le 1$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwwrWrgO6-j_"
      },
      "source": [
        "### Interpreting the last layer\n",
        "\n",
        "Our goal for this exercise is classification. Therefore we will interpret the last layer as numeric means of classification. If we intend to only classify our data into two classes we could only use one neuron with the sigmoid activation in the final layer. If the neuron outputs values less than 0.5 we will consider the input to be classified as the first class and if the output is greater than 0.5 we will consider the input to be classified as the second class.\n",
        "\n",
        "With more $n \\gt 2$ classes we need more neurons. We can use the rule $k = {argmax}_{i \\in \\hat{n}}(a_i^L(x))$, where $L$ is the amount of layers in our netowork, $x$ is the input vecor $n$ is the amount of classes and $k$ is the class determined by the classifier.\n",
        "\n",
        "A bit more sophisticated approach is for example the softmax layer at the end instead of the standard activation. $P(k|x) = \\frac{e^{z^L_k(x)}}{\\sum_{i \\in \\hat n} e^{z^L_i(x)}}$, where $P(k|x)$ is the probability, that for the input $x$ the correct class is $k$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B0Wf7O19FZ2"
      },
      "source": [
        "## Training\n",
        "\n",
        "Our model is so far nice, but we need to be able to find the activations which make it work. To do that we need to train the model using training data. We will use pairs $\\left(x, y\\right)$, where $x$ is the input and $y$ is the vector denoting the correct class. We will represent the output vector with **one-hot** encoding so that the vector $y$ will contain 1 in the $k$-th position and zeros everywhere else.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhi87Qqa-Fnu"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Training is similar to general optimization. We wanto to find the network paramters (in our cas $w$ and $b$) which will work best for our problem. To do this we will optimize for the value of so-called loss function. Ideally we construct the loss function in such a way in which a high loss value on some data indicates that the network is performing badly. We also want the loss function to be differentiable. Today we will use these functions for $N$ pairs of $\\left(x^i, y^i\\right)$:\n",
        "\n",
        "1. MSE: $C = \\frac{1}{N} \\sum_i ||a^L(x^i) - y^i||_2^2$, where $||v||_2$ is the L2 norm of $v$.\n",
        "2. Cross-Entropy:  $C = -\\frac{1}{N} \\sum_{i,j} y_j^i ln\\left(a_j^L(x)\\right) + \\left(1-y_j^i\\right) ln\\left(1 - a_i^L(x) \\right)$\n",
        "3. CE + Softmax: $C = \\frac{1}{N} ln(z_y^L(x))$, where $z_y^L(x)$ is the element o $z^L(x)$, for which the class is correct. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xeWYehnAcjk"
      },
      "source": [
        "### Gradient descent\n",
        "\n",
        "For optimization we will use a simple gradient descent rule with a step $\\eta$ for parameter $p$ and loss function $C$.\n",
        "\n",
        "$$p := p - \\eta \\frac{\\partial C}{\\partial p}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyRPBWdLBMN8"
      },
      "source": [
        "### SGD\n",
        "\n",
        "In each step of training we will use a random subset of the training data. In the following step we will use a different subset and so on. The subsets are called minibatches. This benefits the training since we introduce noise into the optimization process which can help us get out of local minima. Also the found minimum may have some desirable properties as opposed to standard optimization. This also reduces the necessary spatial requirements of our algorithm which is often the most limiting factor when dealing with neural networks.\n",
        "\n",
        "This approach is called **stochastic gradient descent (SGD)**.\n",
        "\n",
        "*Note:* By choosing a minibatch we are actually approximating the function $C$ for the whole training set. Statistically the error of this estimation decreases with  $\\sim \\frac{1}{\\sqrt{M}}$, where $M$ is the size of the minibatch. Therefore it is not necessarily a good idea to use large minibatches even when the memory would be underutilized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQU12OCyGN3j"
      },
      "source": [
        "### Calculating partial derivatives\n",
        "\n",
        "We will now derive how to calculate the partial derivatives for the parameters of our fully connected neural network.\n",
        "\n",
        "Let us first define a helper variable:\n",
        "\n",
        "$ d_i^l = \\frac{\\partial C}{\\partial z_i^l}$\n",
        "\n",
        "Based on the choice of $C$ the $\\odot$ operator means elementwise multiplication (* in NumPy).\n",
        "\n",
        "for MSE: $d_i^L = (a_i^L - y_i) \\cdot f^{'} (z_i^L)$, \n",
        "\n",
        "for CE: $d_i^L = (a_i^L - y_i)$\n",
        "\n",
        "MSE (vectorized): $d_i^L = (a^L - y) \\odot f^{'} (z^L)$\n",
        "\n",
        "CE (vectorized): $d^L = a^L - y$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcoHRfNISgK"
      },
      "source": [
        "Based on $d^{l+1}$ we can compute $d^l$. This approach is also called **backpropagation** since the gradient is calculated in the opposite order as when calculating the values in forward calculation of activations.\n",
        "\n",
        "$ d^l = \\left(d^{l+1} \\left( w^{l+1} \\right)^T\\right) \\odot f^{'}(z^l)$\n",
        "\n",
        "Proof:\n",
        "\n",
        "$z_j^{l+1} = \\sum_k \\left( f \\left( z^l_k\\right) w_{kj}^{l+1} \\right) + b_j^{l+1}$\n",
        "\n",
        "$\\frac{\\partial z_j^{l+1}}{\\partial z_k^l} = f^{'} \\left( z^l_k\\right) w_{kj}^{l+1}$\n",
        "\n",
        "$d_k^l = \\frac{\\partial C}{\\partial z_k^l} = \\sum_j \\frac{\\partial C}{\\partial z_j^{l+1}} \\frac{\\partial z_j^{l+1}}{\\partial z_k^l} = \\sum_j d_j^{l+1} f^{'} \\left( z^l_k\\right) w_{kj}^l$\n",
        "\n",
        "For the network parameters:\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b_j^l} = d_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{kj}^l} = d_j^l a_k^{l-1}$\n",
        "\n",
        "Vectorized:\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b^l} = d^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{}^l} = (a^{l-1})^T d^l $\n",
        "\n",
        "Proof:\n",
        "\n",
        "$z_j^l = \\sum_k a_k^{l-1} w_{kj}^l + b_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b_j^l} = \\sum_k \\frac{\\partial C}{\\partial z_k^l}\\frac{\\partial z_k^l}{\\partial b_j^l} = \\sum_k \\delta_{kj} d_k^l = d_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{kj}^l} = \\sum_p \\frac{\\partial C}{\\partial z_p^l}\\frac{\\partial z_p^l}{\\partial w_{kj}^l} = \\sum_p d_p^l \\delta_{pj} a_k^{l-1} = d_j^l a_k^{l-1}$  \n",
        "\n",
        "where $\\delta_{jk} = 1$ if $j = k$, otherwise $ 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWA-YWo1uQpJ"
      },
      "source": [
        "### Algorithm\n",
        "\n",
        "One step of our SGD algorithm is then:\n",
        "1. We perform the forward computation - we save $z^l$ and $a^l$ values.\n",
        "2. We calculate $d^L$ based on the loss function.\n",
        "3. We backpropagate the error $d^l$ using the backpropagation rule.\n",
        "4. We compute the partial derivatives from the previous step.\n",
        "5. We repeat this for the whole minibatch and average the partial derivatives in order to update the parameter values according to the gradient descent rules.\n",
        "  \n",
        "**Attention:** it is still important to initialize the parameters correctly. If we initialize all of them to the same values all of the partial derivatives will be the same and the network will thus be unable to create represantations which require more than one neuron per layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xZS20luwdq"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "The following code contains the implementation of the base algorithm structure. Some of the code is missing and your job will be to implement the missing code segments. Instructions along with some test are presented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRbPiw8t210R"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    # returns the value after applying the sigmoid function\n",
        "    ...\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # returns the derivative of the sigmoid function\n",
        "    ...\n",
        "\n",
        "def get_one_hot(targets, nb_classes):\n",
        "    # returns the one-hot encoded vector in the sape it was loaded in\n",
        "    ...\n",
        "\n",
        "class Network():\n",
        "    def __init__(self, arg):\n",
        "        if isinstance(arg, str):\n",
        "            # If our input is string we load a file\n",
        "            self.load(arg)\n",
        "        else:\n",
        "            # Othewise we expect a list and use it to initialize the layers\n",
        "            if len(arg) < 2:\n",
        "                raise ValueError(\"Sizes must be at least 2!\")\n",
        "\n",
        "            self.w_list = []\n",
        "            self.b_list = []\n",
        "            # Here we initialize the weights\n",
        "            for i in range(1, len(arg)):\n",
        "                self.w_list.append(np.random.randn(arg[i - 1], arg[i]))\n",
        "                self.b_list.append(0.1 * np.ones((arg[i])))\n",
        "\n",
        "    def save(self, filename):\n",
        "        # saves the network as a numpy file\n",
        "        dict = np.array([self.w_list, self.b_list])\n",
        "        np.save(filename, dict)\n",
        "\n",
        "    def load(self, filename):\n",
        "        # loads the network from a numpy file\n",
        "        d = np.load(filename, allow_pickle=True)\n",
        "        self.w_list = d[0].tolist()\n",
        "        self.b_list = d[1].tolist()\n",
        "\n",
        "    def fwd(self, a):\n",
        "        # implement the forward pass\n",
        "        ...\n",
        "\n",
        "    def _step(self, X, y, batch_size, eta):\n",
        "        # fill these lists with new values after one step\n",
        "        new_w_list = [np.empty_like(w) for w in self.w_list]\n",
        "        new_b_list = [np.empty_like(b) for b in self.b_list]\n",
        "\n",
        "        # It is important to first perform the forward pass through the network\n",
        "        # while remembering the values of a and z. It is possible that \n",
        "        # len(a_list) = len(z_list) + 1, since the first a is our input\n",
        "        # This needs to be taken into consideration when idexing, but you can\n",
        "        # also implement this in another way\n",
        "\n",
        "        z_list = []\n",
        "        a_list = [X]\n",
        "\n",
        "        # Here we first fill the a_list and z_list\n",
        "\n",
        "        # We calculate the error for a given step\n",
        "        d_list = [[] for _ in z_list]\n",
        "        err = 0.0\n",
        "        # d_list[-1] = (a_list[-1] - y)*sigmoid_prime(z_list[-1])\n",
        "\n",
        "        # We calculate the d\n",
        "\n",
        "        # Then we do backprop\n",
        "\n",
        "        # Finally we update the parameters\n",
        "        self.w_list = new_w_list\n",
        "        self.b_list = new_b_list\n",
        "        return err\n",
        "\n",
        "    def eval(self, X, y):\n",
        "        # This method runs the network on input X and calculates the output\n",
        "        \n",
        "        # Note: in a realistic case we would implement this to run on smaller\n",
        "        # chunks of data, but since our model is small we can simply do it at once\n",
        "        out = self.fwd(X)\n",
        "        err = np.mean((out - y)**2)\n",
        "        out_best = np.argmax(out, axis=-1)\n",
        "        y_best = np.argmax(y, axis=-1)\n",
        "        acc = np.sum(np.where(out_best == y_best, 1, 0))/y.shape[0]\n",
        "        return acc, err\n",
        "\n",
        "    def sgd(self, X, y, val_X, val_y, epochs, steps, batch_size, eta):\n",
        "        for epoch in range(epochs):\n",
        "            err_tot = 0\n",
        "            for step in range(steps):\n",
        "                batch_mask = np.random.choice(X.shape[0], batch_size)\n",
        "                batch_X = X[batch_mask,:]\n",
        "                batch_y = y[batch_mask,:]\n",
        "                err = self._step(batch_X, batch_y, batch_size, eta)\n",
        "                if step == 0:\n",
        "                    err_tot = err\n",
        "                else:\n",
        "                    err_tot = 0.1 * err + 0.9 * err_tot\n",
        "                print(\"At step: {}, running average training error: {}\".format(step, err_tot))\n",
        "\n",
        "            acc, err = self.eval(val_X, val_y)\n",
        "            print(\"At Epoch {}, validation error: {}, validation accuracy {}\".format(epoch, err, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH8VKEA75r37"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Implement the function sigmoid which applies the activation function on the input in an elementwise fashion.\n",
        "\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "Implemnt the derivative as well.\n",
        "\n",
        "$$\\sigma (z)^{'} = \\sigma (z)\\left(1 - \\sigma (z) \\right)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yqkMFx7zzH"
      },
      "source": [
        "assert(sigmoid(0) == 0.5)\n",
        "assert(sigmoid(50) > 1.0-1e-10)\n",
        "assert(sigmoid(-50) <= 2e-22)\n",
        "assert((sigmoid(np.array([10, 20]))==np.array([sigmoid(10), sigmoid(20)])).all())\n",
        "assert(sigmoid_prime(0) == 0.25)\n",
        "assert(sigmoid_prime(-50) > 0 )\n",
        "assert(sigmoid_prime(30) > 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6d73t044mf"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Load the MNIST data and feed some of the images to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCf2S6UF_TYP"
      },
      "source": [
        "We will first load the dataset and display one of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI8hPz2W_S21"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "X = mnist.data.astype('float32')/255\n",
        "y = mnist.target.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT_nIXO9_jr_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.reshape(X[0,],(28,28)), cmap='gray')\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7H25CE-kaf"
      },
      "source": [
        "Implement the fwd method. The first test checks the dimension and the second one loads a pre-trained network and runs it on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxW9P3YJ4AmX"
      },
      "source": [
        "net = Network([28*28,30,20,10])\n",
        "out = net.fwd(np.random.randn(32,784))\n",
        "assert(out.shape == (32,10))\n",
        "print(\"Passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiD5eCtMJYM6"
      },
      "source": [
        "!wget https://github.com/kocurvik/edu/raw/master/PNNPPV/supplementary/test_net.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwMeAHDcAt1k"
      },
      "source": [
        "net = Network(\"test_net.npy\")\n",
        "\n",
        "R = net.fwd(X[0:3,:])\n",
        "\n",
        "correct_list = []\n",
        "\n",
        "for i in range(3):\n",
        "  plt.imshow(np.reshape(X[i,:],(28,28)), cmap = 'gray')\n",
        "  plt.show()\n",
        "  print(R[i])\n",
        "  print(np.argmax(R[i,:]))\n",
        "  \n",
        "print(\"Passed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0wkQuKFJ7lC"
      },
      "source": [
        "### Exercise 4 \n",
        "Implement the _step method, which performs SGD for MSE or CE loss function. This method won't be tested directly, but we will try to apply it to training. These parameters should work well enough so that the network learns at least something."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5kRZuWKZBS"
      },
      "source": [
        "train_X = X[:50000, :]\n",
        "train_y = get_one_hot(y[:50000], 10)\n",
        "\n",
        "\n",
        "val_X = X[50000:60000,:]\n",
        "val_y = get_one_hot(y[50000:60000],10)\n",
        "#\n",
        "net = Network([28*28,30,20,10])\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.01)\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.03)\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.001)\n",
        "net.save(\"net.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyuzyE9LRra"
      },
      "source": [
        "We can also evaluate the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGOxv9_eKyk7"
      },
      "source": [
        "R = net.fwd(X[60000:60010,:])\n",
        "\n",
        "for i in range(10):\n",
        "  plt.imshow(np.reshape(X[60000 + i,:],(28,28)), cmap = 'gray')\n",
        "  plt.show()\n",
        "  print(R[i])\n",
        "  print(np.argmax(R[i,:]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}