{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kocurvik/edu/blob/master/PNNPPV/notebooky/cv02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFF-Tzky4FpK"
      },
      "source": [
        "# **2. cvičenie** - Linárna algebra s NumPy, Plne prepojená sieť\n",
        "\n",
        "V tomto notebooku si prejdeme operácie lineárnej algebry v NumPy. Tie potom využijeme aby sme naprogramovali plne prepojenú sieť."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFCxQ5FfX-GE"
      },
      "source": [
        "#Lineárna Algebra s NumPy\n",
        "\n",
        "Na implementáciu základnej plne prepojenej siete budeme potrebovať maticové násobenie. V skutočnosti budeme násobiť tenzory, ale najprv si precvičíme násobenie matíc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFOOIwOB5mN-"
      },
      "source": [
        "## Maticové násobenie\n",
        "Najdôležitejšia operácia, ktorú budeme potrebovať je maticové násobenie. Najprv definícia:\n",
        "\n",
        "Nech $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n \\times l}, \\mathbf{C} \\in \\mathbb{R}^{m \\times l} $ potom $\\mathbf{A}\\mathbf{B} = \\mathbf{C} \\iff (\\forall i \\in \\hat{m})(\\forall j \\in \\hat{l})(c_{i,j} = \\sum_{k=1}^{n} a_{i,k} \\cdot b_{k,j})$ \n",
        "\n",
        "K definícii je dobré si pripomenúť, že prvý index označuje riadok a druhý index označuje stĺpec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzL2kACy7hDF"
      },
      "source": [
        "### Úloha 1 (Na tabuľu)\n",
        "\n",
        "Spočítajte súčiny $\\mathbf{A}\\mathbf{B}$, $\\mathbf{B}\\mathbf{C}$, $\\mathbf{C}\\mathbf{B}$\n",
        "\n",
        "$\\mathbf{A} = \\begin{bmatrix} \n",
        "3 & 5 & -1 \\\\\n",
        "2 & -4 & 2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{B} = \\begin{bmatrix} \n",
        "5 & 2 & 1 \\\\\n",
        "-6 & 5 & 2 \\\\\n",
        "3 & 4 & -1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{C} = \\begin{bmatrix} \n",
        "4 & -4 & 3 \\\\\n",
        "-6 & -3 & 4 \\\\\n",
        "-1 & 1 & 0\n",
        "\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJcOg7TqNbU1"
      },
      "source": [
        "V numpy násobíme matice pomocou príkazu np.matmul(a,b), alebo pomocou operátora @. Dá sa použiť aj np.dot. Rozdiel medzi multiply a dot je v broadcastingu. Otestujte aj súčin $\\mathbf{BC}$ a $\\mathbf{CB}$\n",
        "\n",
        "*Pozn:* Vektory môžeme považovať za matice s jednou dimenziou jedna. Podľa toho či je stĺpcový, alebo riadkový. Formálne to mení možnosti násobenia. Toto ale v NumPy nieje tak vždy. Pri násobení sa tak arrays ktoré majú len jeden rád, tj. len(np.shape) == 1 upravia podľa toho čo sa viac hodí, či riadkový, alebo stĺpcový vektor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fMUJVrOAqW"
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[3,5,-1],[2,-4,2]])\n",
        "b = np.array([[5,2,1],[-6,5,2],[3,4,-1]])\n",
        "print(np.matmul(a,b))\n",
        "d = a @ b\n",
        "print(d)\n",
        "v = np.array([10,20,30])\n",
        "print(v.shape)\n",
        "u = np.array([5,25])\n",
        "print(u.shape)\n",
        "print(a @ v)\n",
        "print(u @ a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aoY3A8ORqHp"
      },
      "source": [
        "V prípade, že máme jeden tenzor vyššieho rádu ako 2, tak matmul ich bude brať ako zoznam matíc. Ak potrebujeme robiť exotickejšie operácie, tak môžeme použiť príkaz np.einsum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl-b9KOdXf_6"
      },
      "source": [
        "## Skalárne násobenie\n",
        "\n",
        "Skalárom môžeme jednoducho násobiť matícu. To sa robí pomocou np.multiply, alebo operátorom *. Ak nenásobíme skalárom ale tenzorom, tak dôjde k násobeniu po elementoch s príšlušným broadcastingom. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQsMp1YfYYeo"
      },
      "source": [
        "print(a * d)\n",
        "print(5 * a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XExe4ma_Qlyv"
      },
      "source": [
        "## Transponované matice\n",
        "\n",
        "Definícia: Nech $\\mathbf{A} \\in \\mathbb{R}^{m,n}$ potom $\\mathbf{A}^T \\in \\mathbb{R}^{n,m}$ je jej transponovaná matica $\\iff (\\forall i \\in \\hat{m})(\\forall j \\in \\hat{n})(a_{i,j} = a^T_{j,i})$ \n",
        "\n",
        "V numpy jednoducho voláme metódu np.array .T, alternatívne možeme použiť funkciu np.transpose. Opäť platí, že rovnako môžeme postupovať aj v prípade vektorov zapísaných ako matice.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjACfi6Ro84"
      },
      "source": [
        "print(a.T)\n",
        "print(np.transpose(a))\n",
        "\n",
        "r = np.array([[100,10,1]])\n",
        "print(r.shape)\n",
        "\n",
        "c = np.array([[3],[7],[8]])\n",
        "print(c.shape)\n",
        "\n",
        "print(a @ c)\n",
        "print(a @ r.T)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUgGLfLYXggG"
      },
      "source": [
        "## Súčty a štatistické hodnoty\n",
        "\n",
        "NumPy nám taktiež umožňuje spočítavať rôzne štatistické hodnoty alebo súčty vrámci tenzorov. Na sčítanie používame np.sum na priemer np.mean pre štandardnú odchýlku np.std. Niekedy chceme výpočet realizovať len cez jednu dimenziu. Vtedy je dôležité použiť keyword axis. Podobne funguje veľa iných metód ako napr. np.median np.prod atď. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe2REngs86jV"
      },
      "source": [
        "print(np.sum(a))\n",
        "print(np.sum(a, axis=0))\n",
        "print(np.sum(a, axis=1))\n",
        "\n",
        "print(np.mean(a))\n",
        "print(np.mean(a, axis=0))\n",
        "print(np.mean(a, axis=1))\n",
        "\n",
        "\n",
        "print(np.std(a))\n",
        "print(np.std(a, axis=0))\n",
        "print(np.std(a, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ScSyiORa836"
      },
      "source": [
        "# Plne prepojená neurónová sieť\n",
        "Neurónová sieť je biologicky inšpirovaný model pre realizáciu výpočtov rôznych funkcií. Dnes ju budeme využívať na ako klasifikátor. \n",
        "\n",
        "Sieť obecne modelujeme ako orientovaný graf s ohodnotenými hranami ktorého vrcholy sú tzv. neuróny. Každý neurón má svoju aktiváciu, ktorá sa počíta na základe aktivácii neurónov s ktorými je prepojený. Táto aktivácia zas ovplivňuje ďalšie neuróny. Najobecnejšie tak môžeme popísať sieť ako:\n",
        "$$a_p = f \\left( \\sum_{q \\in p_{in}} w_{p,q} a_q + b_p \\right) = f\\left(z_p\\right),$$\n",
        "\n",
        "kde $a_p$ je aktivácia daného vrcholu, $w_{p,q}$ je váha vrcholu $q$ pre vrchol $p$, $b_p$ je prah vrcholu $p$, $f$ je aktivačná funckia, $z_p$ je zjednodušený zápis vstupu aktivačnej funkcie. V takomto zápise je však možné aby boli neuróny prepojené cyklicky, čo nechceme. Takisto nechceme, aby bola štruktúra siete komplikovaná. Preto zavedieme tzv. plne prepojenú neurónovú sieť, tiež označovaný ako Multi-Layer-Perceptron. Táto organizácia spočíva v tom, že každý neurón je v nejakej vrstve a každý neurón v jednej vrstve je prepojený s každým z predchádzajúcej.\n",
        "\n",
        "![Plne prepojená sieť](https://raw.githubusercontent.com/kocurvik/edu/master/PNNPPV/supplementary/ntb_images/NN1.jpg)\n",
        "\n",
        "Vyhodnotenie siete potom môžeme zapísať ako:\n",
        "\n",
        "$$a_j^l = f \\left( \\sum_{k} a_k^{l-1} w_{k,j}^l + b_j^l \\right) = f \\left(z_j^l \\right),$$\n",
        "\n",
        "resp. vektorovo:\n",
        "\n",
        "$$a^l = f \\left( a^{l-1}w^l + b^l \\right) =  f \\left(z^l \\right),$$\n",
        "\n",
        "kde $a^l$ je **riadkový** vektor aktivácií, $w^l$ je matica váh tvaru $size(l-1) \\times size(l)$, $a_l$ je **riadkový** vektor prahov, $f$ je skalárna aktivačná funkcia, ktorá je vo vektorovom prípade aplikovaná po elementoch, $z_l$ je taktiež riadkový vektor, ktorý použijeme na zjednodušenie výrazov. Horný index pri každom výraze značí, ku ktorej vrstve daný objekt partrí.\n",
        "\n",
        "Pre lepšiu predstavu o indexácií viď obrázky:\n",
        "\n",
        "![Indexy aktivácie a prahu](https://raw.githubusercontent.com/kocurvik/edu/master/PNNPPV/supplementary/ntb_images/activation_bias.jpg)\n",
        "![Indexy váh](https://raw.githubusercontent.com/kocurvik/edu/master/PNNPPV/supplementary/ntb_images/weight.jpg)\n",
        "\n",
        "*Pozn.:* Vektory by mohli byť aj stĺpcové (a matica tak transponovaná a násobenie v opačnom poradí), ale to by nám v NumPy skomplikovalo prácu kvôli broadcastingu.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIld0vL36Aqs"
      },
      "source": [
        "### Aktivačné funkcie\n",
        "\n",
        "Môžeme použiť rôzne aktivačné funkcie. Dnes ostaneme pri sigmoide, ale nabudúce si implementujeme aj ďalšie.\n",
        "\n",
        "1. Sigmoid: $f(z) = \\frac{1}{1 + e^{-z}}$\n",
        "2. Tanh: $f(z) = tanh(z)$\n",
        "3. ReLU: $f(z) = max(x,0)$\n",
        "4. SoftPlus: $f(z) = ln(1 + e^z)$\n",
        "5. LeakyReLU: $f(z) = max(x,ax), a \\le 1$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwwrWrgO6-j_"
      },
      "source": [
        "### Interpretácia poslednej vrstvy\n",
        "\n",
        "Zameriame sa na úlohu klasifikácie. Preto budeme interpretovať poslednú vrstvu v tomto zmysle. Ak chcem klasifikovať len do dvoch tried, tak nám stačí jeden neurón so sigmoidom. Ak je jeho hodnota menšia ako 0.5 klasifikujeme objekt ako prvú triedu a inak ako druhú.\n",
        "\n",
        "Pri viacerých $n \\gt 2$ triedach potrebujeme viacero neurónov môžeme použiť pravidlo: $k = {argmax}_{i \\in \\hat{n}}(a_i^L(x))$, kde $L$ je počet vrstiev, $x$ je vstupný vektor $n$ je počet tried a $k$ je trieda ktorú klasifikátor určil.\n",
        "\n",
        "Trocha sofistikvanejší postup je použitie. Tzv. softmax vrstvy na konci namiesto bežnej aktivácie. $P(k|x) = \\frac{e^{z^L_k(x)}}{\\sum_{i \\in \\hat n} e^{z^L_i(x)}}$, kde $P(k|x)$ je pravdepodobnosť, že pre vstup $x$ je správna trieda $k$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B0Wf7O19FZ2"
      },
      "source": [
        "## Trénovanie\n",
        "\n",
        "Model je síce pekný, ale je nutné nájsť také aktivácie aby robil to, čo chceme. V našom prípade je to klasifikácia do $n$ tried. Na trénovanie budeme potrebovať trénovacie dáta. Teda páry $\\left(x, y\\right)$, kde $x$ je vstupný vektor a $y$ je výstupný vektor označujúci správnu triedu. Aby sa nám to hodilo do interpretácie poslednej vrstvy, tak $y$ bude vektor, ktorý bude mať na $k$-tom mieste jednotku a všade inde nuly. Toto je tzv. **one-hot** kódovanie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhi87Qqa-Fnu"
      },
      "source": [
        "### Cenová funkcia\n",
        "\n",
        "Trénovanie je podobné optimalizácii. Chceme nájsť také parametre siete (v našom prípade matice $w$ a vektory $b$), ktoré budú fungovať čo najlepšie pre náš problém. To sa budeme snažiť docieliť tým že budeme optimalizovať hodnotu tzv. ceny (loss function, cost function). Ideálne platí, že cím vyššia cena na nejakej množine, tým horšie na nej naša sieť funguje. Zároveň chceme aby bola cena pekne diferencovateľná. Dnes použijeme cenové funkcie pre $N$ párov $\\left(x^i, y^i\\right)$:\n",
        "\n",
        "1. MSE: $C = \\frac{1}{N} \\sum_i ||a^L(x^i) - y^i||_2^2$, kde $||v||_2$ je L-2 norma vektoru $v$.\n",
        "2. Cross-Entropy:  $C = -\\frac{1}{N} \\sum_{i,j} y_j^i ln\\left(a_j^L(x)\\right) + \\left(1-y_j^i\\right) ln\\left(1 - a_i^L(x) \\right)$\n",
        "3. CE + Softmax: $C = \\frac{1}{N} ln(z_y^L(x))$, kde $z_y^L(x)$ je ten element vektoru $z^L(x)$, pre ktorý je trieda správne. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xeWYehnAcjk"
      },
      "source": [
        "### Gradientný zostup\n",
        "\n",
        "Na optimalizáciu použijeme jednoduché pravidlo gradientného zostupu s krokom $\\eta$ pre parameter $p$ a cenovú funkciu $C$.\n",
        "\n",
        "$$p := p - \\eta \\frac{\\partial C}{\\partial p}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyRPBWdLBMN8"
      },
      "source": [
        "### SGD\n",
        "\n",
        "Na každý krok pri trénovaní vždy použijeme náhodnú podmnožinu z trénovacej množiny. Na ďalší krok ďalšiu atď. Toto má výhody v tom, že vnášame do procesu šum, ktorý nás vie dostať z lokálnych miním. Takisto miesta v parametrickom priestore kam sa algoritmus dostane majú často lepšie vlastnosti, čo sa týka generalizácie ako čistá optimalizácia. Navyše tým šetríme miesto v pamäti, čo je jedna zo základných.\n",
        "\n",
        "Tento prístup sa volá **stochastic gradient descent (SGD)**.\n",
        "\n",
        "*Pozn.:* Výberom minibatchu vlastne approximujeme priestor funkciu $C$ pre celú trénovaciu množinu. \"Chyba\" tejto aproximácie klesá približne $\\sim \\frac{1}{\\sqrt{M}}$, kde $M$ je veľkosť minibatchu. Preto nieje úplne vhodné používať veľký minibatch, aj keď to pamäť dovoluje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQU12OCyGN3j"
      },
      "source": [
        "### Výpočet parciálnych derivácii\n",
        "Teraz si odvodíme postup ako vypočítať analyticky parciálne derivácie pre našu plne prepojenú sieť.\n",
        "\n",
        "Najprv si zavedieme pomocnú deriváciu.\n",
        "\n",
        "$ d_i^l = \\frac{\\partial C}{\\partial z_i^l}$\n",
        "\n",
        "V závislosti na výbere $C$ platí, značka $\\odot$ značí násobenie po elementoch (Hadamardov súčin)\n",
        "\n",
        "pre MSE: $d_i^L = (a_i^L - y_i) \\cdot f^{'} (z_i^L)$, \n",
        "\n",
        "pre CE: $d_i^L = (a_i^L - y_i)$\n",
        "\n",
        "Vektorovo MSE: $d_i^L = (a^L - y) \\odot f^{'} (z^L)$\n",
        "\n",
        "Vektorovo CE: $d^L = a^L - y$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcoHRfNISgK"
      },
      "source": [
        "Na základe $d^{l+1}$ môžeme určiť $d^l$. Tento postup je tzv. **backpropagation**, keďže deriváciu (gradient) propagujeme v sieti opačným smerom ako pri doprednom výpočte.\n",
        "\n",
        "$ d^l = \\left(d^{l+1} \\left( w^{l+1} \\right)^T\\right) \\odot f^{'}(z^l)$\n",
        "\n",
        "Dôkaz:\n",
        "\n",
        "$z_j^{l+1} = \\sum_k \\left( f \\left( z^l_k\\right) w_{kj}^{l+1} \\right) + b_j^{l+1}$\n",
        "\n",
        "$\\frac{\\partial z_j^{l+1}}{\\partial z_k^l} = f^{'} \\left( z^l_k\\right) w_{kj}^{l+1}$\n",
        "\n",
        "$d_k^l = \\frac{\\partial C}{\\partial z_k^l} = \\sum_j \\frac{\\partial C}{\\partial z_j^{l+1}} \\frac{\\partial z_j^{l+1}}{\\partial z_k^l} = \\sum_j d_j^{l+1} f^{'} \\left( z^l_k\\right) w_{kj}^l$\n",
        "\n",
        "Pre parametre platí:\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b_j^l} = d_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{kj}^l} = d_j^l a_k^{l-1}$\n",
        "\n",
        "Vektorovo:\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b^l} = d^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{}^l} = (a^{l-1})^T d^l $\n",
        "\n",
        "Dôkaz:\n",
        "\n",
        "$z_j^l = \\sum_k a_k^{l-1} w_{kj}^l + b_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial b_j^l} = \\sum_k \\frac{\\partial C}{\\partial z_k^l}\\frac{\\partial z_k^l}{\\partial b_j^l} = \\sum_k \\delta_{kj} d_k^l = d_j^l$\n",
        "\n",
        "$\\frac{\\partial C}{\\partial w_{kj}^l} = \\sum_p \\frac{\\partial C}{\\partial z_p^l}\\frac{\\partial z_p^l}{\\partial w_{kj}^l} = \\sum_p d_p^l \\delta_{pj} a_k^{l-1} = d_j^l a_k^{l-1}$  \n",
        "\n",
        "kde $\\delta_{jk} = 1$ ak $j = k$, inak $ 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWA-YWo1uQpJ"
      },
      "source": [
        "### Algoritmus\n",
        "Jeden krok algoritmu SGD potom vyzerá následovne\n",
        "1. Realizujeme dopredný výpočet - pamätáme si $z^l$ a $a^l$.\n",
        "2. Vypočitame $\\delta^L$ podľa cenovej funkcie\n",
        "3. Spätne propagujeme chybu $\\delta^l$ pomocou backpropagation pravidla.\n",
        "4. Vypočítame parciálne derivácie podľa rovníc z predchádzajúceho bloku.\n",
        "5. Opakujeme pre celý minibatch, derivácie spriemerujeme a updatneme parametre podľa gradientného zostupu.\n",
        "  \n",
        "**Pozor:** je nutné ešte nejako nainicializovať váhy. Ak ich nainicializujeme narovnako, tak derivácie pre jednotlivé vrstvy budú vždy rovnaké a tak sa sieť ani nemôže naučit komplikovanejšie reprezentácie. Tj. je to ako sieť kde v každej vrstve je len jeden neurón."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xZS20luwdq"
      },
      "source": [
        "## Implementácia\n",
        "\n",
        "V nasledujúcom kóde je implementácia základnej štruktúry kódu. Pre jednoduchú plne prepojenú sieť. V kóde budete musieť doplniť veci v jednotlivých úlohách. K úlohám je aj kód, ktorý Vám pomôže overiť či ste správne doimplementovali časť riešenia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRbPiw8t210R"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    # vráti hodnotu sigmoidu\n",
        "    ...\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # vráti gradient sigmoidu\n",
        "    ...\n",
        "\n",
        "def get_one_hot(targets, nb_classes):\n",
        "    # vráti one-hot vektor pre vektor tried v tvare akom ho načítame\n",
        "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[nb_classes])\n",
        "\n",
        "class Network():\n",
        "    def __init__(self, arg):\n",
        "        if isinstance(arg, str):\n",
        "            # Ak máme na vstup string, tak načítame súbor\n",
        "            self.load(arg)\n",
        "        else:\n",
        "            # Inak čakáme zoznam a podľa neho určíme počty neurónov\n",
        "            if len(arg) < 2:\n",
        "                raise ValueError(\"Sizes must be at least 2!\")\n",
        "\n",
        "            self.w_list = []\n",
        "            self.b_list = []\n",
        "            # Je dôležité neuróny nainicializovať\n",
        "            for i in range(1, len(arg)):\n",
        "                self.w_list.append(np.random.randn(arg[i - 1], arg[i]))\n",
        "                self.b_list.append(0.1 * np.ones((arg[i])))\n",
        "\n",
        "    def save(self, filename):\n",
        "        # uloženie siete ako numpy súboru\n",
        "        dict = np.array([self.w_list, self.b_list])\n",
        "        np.save(filename, dict)\n",
        "\n",
        "    def load(self, filename):\n",
        "        # čítanie uložených npy súborov\n",
        "        d = np.load(filename, allow_pickle=True)\n",
        "        self.w_list = d[0].tolist()\n",
        "        self.b_list = d[1].tolist()\n",
        "\n",
        "    def fwd(self, a):\n",
        "        # doimplementujte dopredný beh\n",
        "        ...\n",
        "\n",
        "    def _step(self, X, y, batch_size, eta):\n",
        "        # doplne tieto zoznamy podľa SGD kroku na minibatchi\n",
        "        new_w_list = [np.empty_like(w) for w in self.w_list]\n",
        "        new_b_list = [np.empty_like(b) for b in self.b_list]\n",
        "\n",
        "        # najprv bude nutné realizovať dopredný krok a pamätať si pomocné hodnoty\n",
        "        # je dosť možne, že len(a_list) = len(z_list) + 1, lebo ako prvé a máme\n",
        "        # vstup treba to potom zohľadniť pri indexovaní môžete to ale urobiť aj\n",
        "        # inak\n",
        "        z_list = []\n",
        "        a_list = [X]\n",
        "        # a_list = []\n",
        "\n",
        "        # tu najprv naplníme a_list a z_list\n",
        "\n",
        "        # vypočítame chybu v danom kroku\n",
        "        err = 0\n",
        "        d_list = [[] for _ in z_list]\n",
        "        # výpočet d^L podľa cenovej funkcie\n",
        "\n",
        "        # backprop výpočet d\n",
        "\n",
        "        # zápis do new_w_list a new_b_list podľa SGD\n",
        "\n",
        "        self.w_list = new_w_list\n",
        "        self.b_list = new_b_list\n",
        "        return err\n",
        "\n",
        "    def eval(self, X, y):\n",
        "        # táto metóda spustí sieť na dáta X a spočíta presnosť a chybu na týchto\n",
        "        # dátach\n",
        "        \n",
        "        # Pozn: normálne by bolo fajn robiť to po častiach kvôli pamäti. Náš\n",
        "        # model bude ale malý tak to môžeme spraviť naraz\n",
        "        out = self.fwd(X)\n",
        "        err = np.mean((out - y)**2)\n",
        "        out_best = np.argmax(out, axis=-1)\n",
        "        y_best = np.argmax(y, axis=-1)\n",
        "        acc = np.sum(np.where(out_best == y_best, 1, 0))/y.shape[0]\n",
        "        return acc, err\n",
        "\n",
        "    def sgd(self, X, y, val_X, val_y, epochs, steps, batch_size, eta):\n",
        "        for epoch in range(epochs):\n",
        "            err_tot = 0\n",
        "            for step in range(steps):\n",
        "                batch_mask = np.random.choice(X.shape[0], batch_size)\n",
        "                batch_X = X[batch_mask,:]\n",
        "                batch_y = y[batch_mask,:]\n",
        "                err = self._step(batch_X, batch_y, batch_size, eta)\n",
        "                if step == 0:\n",
        "                    err_tot = err\n",
        "                else:\n",
        "                    err_tot = 0.1 * err + 0.9 * err_tot\n",
        "                print(\"At step: {}, running average training error: {}\".format(step, err_tot))\n",
        "\n",
        "            acc, err = self.eval(val_X, val_y)\n",
        "            print(\"At Epoch {}, validation error: {}, validation accuracy {}\".format(epoch, err, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH8VKEA75r37"
      },
      "source": [
        "### Úloha 2\n",
        "\n",
        "Doimplementujte funkciu sigmoid, ktorá vráti výsledok funkcie sigmoid po elementoch pre každý vstupný tenzor.\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "Takisto doimplementujte aj deriváciu\n",
        "\n",
        "$$\\sigma (z)^{'} = \\sigma (z)\\left(1 - \\sigma (z) \\right)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yqkMFx7zzH"
      },
      "source": [
        "assert(sigmoid(0) == 0.5)\n",
        "assert(sigmoid(50) > 1.0-1e-10)\n",
        "assert(sigmoid(-50) <= 2e-22)\n",
        "assert((sigmoid(np.array([10, 20]))==np.array([sigmoid(10), sigmoid(20)])).all())\n",
        "assert(sigmoid_prime(0) == 0.25)\n",
        "assert(sigmoid_prime(-50) > 0 )\n",
        "assert(sigmoid_prime(30) > 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6d73t044mf"
      },
      "source": [
        "### Úloha 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCf2S6UF_TYP"
      },
      "source": [
        "Pred treťou úlohou načítame mnist dataset a zobrazíme si jeden obrázok. Je dosť možné, že to chvílu potrvá."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI8hPz2W_S21"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "X = mnist.data.astype('float32')/255\n",
        "y = mnist.target.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT_nIXO9_jr_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.reshape(X[0,],(28,28)), cmap='gray')\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7H25CE-kaf"
      },
      "source": [
        "Doimplementujte metódu fwd. V prvom teste sa testuje dimenzia výstupu a v druhuom sa stihane predtrénovaná sieť a spustí sa na obrázkoch z datasetu MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxW9P3YJ4AmX"
      },
      "source": [
        "net = Network([28*28,30,20,10])\n",
        "out = net.fwd(np.random.randn(32,784))\n",
        "assert(out.shape == (32,10))\n",
        "print(\"Prvý test prešiel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiD5eCtMJYM6"
      },
      "source": [
        "!wget https://github.com/kocurvik/edu/raw/master/PNNPPV/supplementary/test_net.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwMeAHDcAt1k"
      },
      "source": [
        "net = Network(\"test_net.npy\")\n",
        "\n",
        "R = net.fwd(X[0:3,:])\n",
        "\n",
        "correct_list = []\n",
        "\n",
        "for i in range(3):\n",
        "  plt.imshow(np.reshape(X[i,:],(28,28)), cmap = 'gray')\n",
        "  plt.show()\n",
        "  print(R[i])\n",
        "  print(np.argmax(R[i,:]))\n",
        "  \n",
        "print(\"Druhý test prešiel!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0wkQuKFJ7lC"
      },
      "source": [
        "### Úloha 4 \n",
        "Doimplementujte metódu _step\n",
        "\n",
        "Ktorá výkoná krok SGD pre MSE, alebo CE cenovú funkciu. Túto metódu nebude testovať priamo, ale skúsime je apllikovať na trénovanie. Tieto parametre by mali fungovať aspoň tak, že model sa bude zlepšovať na validačnej množine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5kRZuWKZBS"
      },
      "source": [
        "train_X = X[:50000, :]\n",
        "train_y = get_one_hot(y[:50000], 10)\n",
        "\n",
        "\n",
        "val_X = X[50000:60000,:]\n",
        "val_y = get_one_hot(y[50000:60000],10)\n",
        "#\n",
        "net = Network([28*28,30,20,10])\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.01)\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.03)\n",
        "net.sgd(train_X, train_y, val_X, val_y, 10, 10000, 32, 0.001)\n",
        "net.save(\"net.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyuzyE9LRra"
      },
      "source": [
        "Môžeme si sieť aj otestovať!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGOxv9_eKyk7"
      },
      "source": [
        "R = net.fwd(X[60000:60010,:])\n",
        "\n",
        "for i in range(10):\n",
        "  plt.imshow(np.reshape(X[60000 + i,:],(28,28)), cmap = 'gray')\n",
        "  plt.show()\n",
        "  print(R[i])\n",
        "  print(np.argmax(R[i,:]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
