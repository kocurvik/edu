{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWA6tpBW4ooT6Gx7/dKhR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kocurvik/edu/blob/master/PNSPV/notebooky/cv04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE0hnvRAoZDT"
      },
      "source": [
        "# 4. cvičenie - Plne prepojená sieť v PyTorchi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eHMFqSy5GgH"
      },
      "source": [
        "V tomto notebooku si vyskúšame trénovanie v kerase. Prejdeme si základ delenia dát na trénovacie, validačné a testovacie. Skontrolujeme účinky over a underfittingu. Otestujeme rôzne aktivačné funkcie a cenové funkcie. Otestujeme možnosti regularizácie.\n",
        "\n",
        "Pri práci budeme používať framework PyTorch, ktorého dokumentáciu nájdete na https://pytorch.org/docs/stable/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA_kYUQZ6fV7"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xL_UoTE5lzs"
      },
      "source": [
        "Budeme pracovať s jednoduchým datasetom MNIST, ktorý obsahuje obrázky čísel, ktoré sa vypĺňajú do PSČ políčok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfiZQAcXoZcT"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "x_np = mnist.data.astype('float32')/255\n",
        "labels_np = mnist.target.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJy9c10xpbc1"
      },
      "source": [
        "Dataset si môžeme zobraziť."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2pwwGIHpavk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78272108-f856-4947-8111-453e9b732002"
      },
      "source": [
        "for i in range(10):\n",
        "  plt.imshow(np.reshape(x_np[i,],(28,28)), cmap='gray')\n",
        "  plt.title(labels_np[i])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv9gWJQC6Oil"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "Doteraz sme pracovali iba s dvoma triedami. V takom prípade bola reprezentácia relatívne jednoduchá. Avšak pri viac triedach je nutné zvoliť vhodný spôsob ako tieto triedy reprezentovať. Dalo by sa napr. to jedným číslom a to by sieť mala vyhodiť. Avšak to je problém, lebo napr. ak máme tri kategórie: mačka (0), pes (1), škrečok (2), tak by sa sieť v podstate musela naučiť reprezentáciu v ktorej platí, že priemer mačky a škrečka je pes, lebo (0 + 2) / 2 = 1. Toto nedáva zmysel. Preto bude lepšie ak sieť bude na výstupe mať napr. vektor s rovnakou dĺžkou ako počet tried a každý prvok bude určovať pravdepodobnosť danej triedy. Pri anotovaných dátach presne vieme, že len jedna kategória má pravdebodobnosť jedna a ostatné nula, takže bude vyzerať asi takto $(0, 0, 0, 1, 0, 0)$. Takáto reprezentácia sa volá one-hot encoding. Na konverziu labelov, môžeme použiť pytorchovskú funkciu onehot.\n",
        "\n",
        "Keďže ďalej budeme pracovať s tenzormy, tak si prekonvertujeme aj naše vstupné dáta na torch Tensory pomocou torch.from_numpy(). Funkcia one_hot urobí konverziu automaticky."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgM0XNRq8z_m",
        "outputId": "aa081a23-ce40-4837-8c5e-25aec3d47903"
      },
      "source": [
        "from torch.nn.functional import one_hot\n",
        "\n",
        "x = torch.from_numpy(x_np)\n",
        "\n",
        "y = one_hot(torch.from_numpy(labels_np))\n",
        "\n",
        "labels = torch.from_numpy(labels_np)\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"labels: \", labels[i])\n",
        "  print(\"one-hot: \", y[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  tensor(5)\n",
            "one-hot:  tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
            "labels:  tensor(0)\n",
            "one-hot:  tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(4)\n",
            "one-hot:  tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(1)\n",
            "one-hot:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(9)\n",
            "one-hot:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
            "labels:  tensor(2)\n",
            "one-hot:  tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(1)\n",
            "one-hot:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(3)\n",
            "one-hot:  tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(1)\n",
            "one-hot:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels:  tensor(4)\n",
            "one-hot:  tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLj37Z7Y94CI"
      },
      "source": [
        "## Model plne prepojenej siete\n",
        "\n",
        "Najprv si teda vytvoríme model. Náš model nebude využívať komplikované prepojenia, takže môžeme použiť torch.nn.Sequential. Čo je konštruktor pre sieť v ktorej výstup z jednej vrstvy je vstupom iba do ďalšej atď.\n",
        "\n",
        "Ako jednotlivý prvky v modeli budú jeho vrstvy. Tu použijeme plne prepojené vrstvy, ktoré sa v kerase dajú nájsť pod torch.nn.Linear. Podobne budeme podtrebovať aj aktivačné funkcie napr. torch.nn.ReLU. Jednolivé plneprepojené vrstvy sa inicializujú s argumentmi s počtom vstupných a výstupných neurónov. V prvej vrstve teda potrebujeme 28 * 28 neurónov (rovnako ako v obrázku) a na výstupe 10 podľa počtu tried.\n",
        "\n",
        "### Stratová funkcia\n",
        "\n",
        "Ako stratovú funkciu použijeme tzv. categorical crossentropy. Tá má tvar\n",
        "\n",
        "$$L(\\vec{y}, \\hat{\\vec{y}}) = \\sum_{i=1}^n y_i \\text{log}(\\hat{\\vec{y}}),$$\n",
        "\n",
        "kde $\\vec{y}$ je výstup siete a $\\hat{\\vec{y}}$ sú one-hot anotácie. Na to aby to fungovalo, tak predpokladáme, že posledná vrstva je tzv. softmax, ktorá aplikuje funkciu $\\mathbb{R}^n \\mapsto \\mathbb{R}^n$ a pre vstupný vektor má jej $i$-tá zložka tvar:\n",
        "\n",
        "$$\\text{softmax}(\\vec{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}.$$\n",
        "\n",
        "Táto vrstva teda akosi normalizuje výstupy siete tak, aby sme ich mohli interpretovať ako pravdepodobnosti, keďže jej výsupy sa dajú sčítať na 1. Problémom je, že táto vrstva robí operáciu exp, ale v stratovej funkcii zas robíme log. Ak aplikujeme exp na nejaké veľké číslo môžeme dostať NaN a následny logaritmus bude tiež nan, aj keď plátí že:\n",
        "\n",
        "$$ \\text{log}(e^X) = X,$$\n",
        "\n",
        "teda sa dá NaNu vyhnúť. Preto budeme mať dva modely jeden bude pre trénovanie a nebude rovno sa v ňom spočíta loss zo vstupov do softmaxu a v druhom sa aplikuje len softmax.\n",
        "\n",
        "Ako loss teda môžeme použiť torch.nn.CrossEntropyLoss.\n",
        "\n",
        "\n",
        "### Optimalizácia\n",
        "\n",
        "V minulosti sme používali optimalizáciu, ktorú sme si vyrobili ručne. Torch však ponúka aj túto funkcionalitu rovno naimplementovanú. Rôzne optimalizačné postupy sú k dispozícii v torch.optim\n",
        "\n",
        "Zatiaľ použijeme torch.optim.SGD. Tomuto objektu ale treba aj dať vedieť, čo sú parametre modelu. Tie získame z už konštruovaného modelu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kGWlTc8-Y64"
      },
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
        "model = Sequential(Linear(28 * 28, 30),\n",
        "                   ReLU(),\n",
        "                   Linear(30, 20),\n",
        "                   ReLU(),\n",
        "                   Linear(20, 10))\n",
        "\n",
        "model_inference = Sequential(model, Softmax(dim=-1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaFScMoXbhCZ"
      },
      "source": [
        "Model môžeme aplikovať na vstupné dáta. Pritom sa však očakáva, že vstup bude veľkosti $n \\times (28*28)$. V jednoduchom prípade sieť zvládne aj vstup veľkosti $28 * 28$, ale netreba s tým rátať aj pri iných sieťach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no3xwBrkby30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9d65e1-a726-43cc-c9ad-2fd167ba3e12"
      },
      "source": [
        "print(model_inference(x[:3]))\n",
        "print(model_inference(x[0]))\n",
        "print(model_inference(x[None, 0]))\n",
        "\n",
        "print(model(x[:3]))\n",
        "print(model(x[0]))\n",
        "print(model(x[None, 0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0781, 0.1002, 0.0937, 0.1176, 0.0852, 0.1148, 0.1081, 0.0858, 0.1130,\n",
            "         0.1035],\n",
            "        [0.0784, 0.0984, 0.0940, 0.1207, 0.0823, 0.1145, 0.1081, 0.0853, 0.1141,\n",
            "         0.1042],\n",
            "        [0.0764, 0.1001, 0.0923, 0.1177, 0.0842, 0.1151, 0.1106, 0.0878, 0.1129,\n",
            "         0.1028]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0.0781, 0.1002, 0.0937, 0.1176, 0.0852, 0.1148, 0.1081, 0.0858, 0.1130,\n",
            "        0.1035], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.0781, 0.1002, 0.0937, 0.1176, 0.0852, 0.1148, 0.1081, 0.0858, 0.1130,\n",
            "         0.1035]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[-0.2110,  0.0379, -0.0292,  0.1983, -0.1241,  0.1737,  0.1137, -0.1176,\n",
            "          0.1579,  0.0697],\n",
            "        [-0.2022,  0.0245, -0.0208,  0.2294, -0.1536,  0.1764,  0.1192, -0.1183,\n",
            "          0.1726,  0.0817],\n",
            "        [-0.2416,  0.0294, -0.0516,  0.1913, -0.1435,  0.1687,  0.1291, -0.1018,\n",
            "          0.1498,  0.0560]], grad_fn=<AddmmBackward>)\n",
            "tensor([-0.2110,  0.0379, -0.0292,  0.1983, -0.1241,  0.1737,  0.1137, -0.1176,\n",
            "         0.1579,  0.0697], grad_fn=<AddBackward0>)\n",
            "tensor([[-0.2110,  0.0379, -0.0292,  0.1983, -0.1241,  0.1737,  0.1137, -0.1176,\n",
            "          0.1579,  0.0697]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvxup4rvrX5a"
      },
      "source": [
        "## Predbežné testovanie modelu\n",
        "\n",
        "Pri trénovaní modelu je nutné overiť si, že má dostatočnú kapacitu pre daný problém. Kapacita modelu je v tomto prípade vyjadrenie toho akú veľkú (resp. ako zložitú) množinu funkcií je možné modelovať. Ak náš model ani teoreticky nedokáže modelovať žiadanú funkciu (napr. klasifikátor), tak nieje vhodné ho použiť. Napr. používať lineárny klasifikátor na dáta o ktorých vieme, že sú lineárne neseparovateľné je príklad nedostatočnej kapacity modelu.\n",
        "\n",
        "Najjednoduchšie ako toto budeme testovať je tak, že vyberieme veľmi malú časť (napr. 1 minibatch) dát na ktorých chceme trénovať a overíme, či sa nám podarí model natrénovať na 100 percent na takejto malej vzorke.\n",
        "\n",
        "Týmto testovaním tiež overíme, že trénovanie funguje a nieje problém s dátami. Tento postup je vhodný prvý krok pri troubleshootingu sietí.\n",
        "\n",
        "*Pozn.:* Pri takomto testovaní je dôležité nemať všetky príklady z rovnakej kategórie, lebo v takom prípade ani netreba uvažovať o kapacite modelu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMFOevZGcPTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb10555-3473-42a5-902b-60935a31f06a"
      },
      "source": [
        "ce_loss = torch.nn.CrossEntropyLoss()\n",
        "x_mini = x[:32]\n",
        "labels_mini = labels[:32]\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "for i in range(500):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out = model(x_mini)\n",
        "  loss = ce_loss(out, labels_mini)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"Loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  out = model(x_mini)\n",
        "  out_classes = torch.argmax(out, dim=-1)\n",
        "  acc = torch.sum(out_classes == labels_mini)/32\n",
        "  print(\"Acc: \", acc.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 0: 2.3037843704223633\n",
            "Acc:  0.15625\n",
            "Loss at step 1: 2.294905424118042\n",
            "Acc:  0.15625\n",
            "Loss at step 2: 2.2864370346069336\n",
            "Acc:  0.15625\n",
            "Loss at step 3: 2.278319835662842\n",
            "Acc:  0.15625\n",
            "Loss at step 4: 2.2704594135284424\n",
            "Acc:  0.15625\n",
            "Loss at step 5: 2.2626800537109375\n",
            "Acc:  0.15625\n",
            "Loss at step 6: 2.254746675491333\n",
            "Acc:  0.15625\n",
            "Loss at step 7: 2.246400833129883\n",
            "Acc:  0.15625\n",
            "Loss at step 8: 2.2375941276550293\n",
            "Acc:  0.15625\n",
            "Loss at step 9: 2.2283811569213867\n",
            "Acc:  0.15625\n",
            "Loss at step 10: 2.218541383743286\n",
            "Acc:  0.15625\n",
            "Loss at step 11: 2.2085115909576416\n",
            "Acc:  0.15625\n",
            "Loss at step 12: 2.19791579246521\n",
            "Acc:  0.15625\n",
            "Loss at step 13: 2.186786651611328\n",
            "Acc:  0.15625\n",
            "Loss at step 14: 2.175187349319458\n",
            "Acc:  0.21875\n",
            "Loss at step 15: 2.162452459335327\n",
            "Acc:  0.21875\n",
            "Loss at step 16: 2.149012804031372\n",
            "Acc:  0.21875\n",
            "Loss at step 17: 2.134589433670044\n",
            "Acc:  0.21875\n",
            "Loss at step 18: 2.1190690994262695\n",
            "Acc:  0.21875\n",
            "Loss at step 19: 2.1022226810455322\n",
            "Acc:  0.25\n",
            "Loss at step 20: 2.0845494270324707\n",
            "Acc:  0.25\n",
            "Loss at step 21: 2.0651376247406006\n",
            "Acc:  0.25\n",
            "Loss at step 22: 2.044776678085327\n",
            "Acc:  0.25\n",
            "Loss at step 23: 2.0223753452301025\n",
            "Acc:  0.28125\n",
            "Loss at step 24: 1.9998127222061157\n",
            "Acc:  0.28125\n",
            "Loss at step 25: 1.9766017198562622\n",
            "Acc:  0.28125\n",
            "Loss at step 26: 1.9531452655792236\n",
            "Acc:  0.28125\n",
            "Loss at step 27: 1.9285081624984741\n",
            "Acc:  0.3125\n",
            "Loss at step 28: 1.9031875133514404\n",
            "Acc:  0.3125\n",
            "Loss at step 29: 1.876847267150879\n",
            "Acc:  0.3125\n",
            "Loss at step 30: 1.8498417139053345\n",
            "Acc:  0.34375\n",
            "Loss at step 31: 1.8218960762023926\n",
            "Acc:  0.375\n",
            "Loss at step 32: 1.7929238080978394\n",
            "Acc:  0.375\n",
            "Loss at step 33: 1.762334942817688\n",
            "Acc:  0.4375\n",
            "Loss at step 34: 1.7305928468704224\n",
            "Acc:  0.46875\n",
            "Loss at step 35: 1.6976910829544067\n",
            "Acc:  0.5625\n",
            "Loss at step 36: 1.6637965440750122\n",
            "Acc:  0.53125\n",
            "Loss at step 37: 1.6286675930023193\n",
            "Acc:  0.5625\n",
            "Loss at step 38: 1.5921512842178345\n",
            "Acc:  0.59375\n",
            "Loss at step 39: 1.5545344352722168\n",
            "Acc:  0.625\n",
            "Loss at step 40: 1.5156389474868774\n",
            "Acc:  0.625\n",
            "Loss at step 41: 1.4746012687683105\n",
            "Acc:  0.65625\n",
            "Loss at step 42: 1.4330941438674927\n",
            "Acc:  0.65625\n",
            "Loss at step 43: 1.391603708267212\n",
            "Acc:  0.75\n",
            "Loss at step 44: 1.349196434020996\n",
            "Acc:  0.78125\n",
            "Loss at step 45: 1.3063490390777588\n",
            "Acc:  0.78125\n",
            "Loss at step 46: 1.2632272243499756\n",
            "Acc:  0.78125\n",
            "Loss at step 47: 1.2198493480682373\n",
            "Acc:  0.78125\n",
            "Loss at step 48: 1.1768016815185547\n",
            "Acc:  0.78125\n",
            "Loss at step 49: 1.134650468826294\n",
            "Acc:  0.78125\n",
            "Loss at step 50: 1.093234896659851\n",
            "Acc:  0.8125\n",
            "Loss at step 51: 1.0532859563827515\n",
            "Acc:  0.8125\n",
            "Loss at step 52: 1.0140801668167114\n",
            "Acc:  0.8125\n",
            "Loss at step 53: 0.9751291275024414\n",
            "Acc:  0.8125\n",
            "Loss at step 54: 0.9378054141998291\n",
            "Acc:  0.8125\n",
            "Loss at step 55: 0.9019647836685181\n",
            "Acc:  0.8125\n",
            "Loss at step 56: 0.8668400645256042\n",
            "Acc:  0.8125\n",
            "Loss at step 57: 0.8335393071174622\n",
            "Acc:  0.8125\n",
            "Loss at step 58: 0.8024091720581055\n",
            "Acc:  0.8125\n",
            "Loss at step 59: 0.7716904878616333\n",
            "Acc:  0.8125\n",
            "Loss at step 60: 0.7422917485237122\n",
            "Acc:  0.875\n",
            "Loss at step 61: 0.714963972568512\n",
            "Acc:  0.875\n",
            "Loss at step 62: 0.688451886177063\n",
            "Acc:  0.875\n",
            "Loss at step 63: 0.6625002026557922\n",
            "Acc:  0.875\n",
            "Loss at step 64: 0.6373676061630249\n",
            "Acc:  0.875\n",
            "Loss at step 65: 0.6136627197265625\n",
            "Acc:  0.875\n",
            "Loss at step 66: 0.5909864902496338\n",
            "Acc:  0.875\n",
            "Loss at step 67: 0.5690290927886963\n",
            "Acc:  0.875\n",
            "Loss at step 68: 0.5484974384307861\n",
            "Acc:  0.875\n",
            "Loss at step 69: 0.5285184979438782\n",
            "Acc:  0.875\n",
            "Loss at step 70: 0.5093996524810791\n",
            "Acc:  0.875\n",
            "Loss at step 71: 0.49113112688064575\n",
            "Acc:  0.9375\n",
            "Loss at step 72: 0.47361403703689575\n",
            "Acc:  0.9375\n",
            "Loss at step 73: 0.45684194564819336\n",
            "Acc:  0.9375\n",
            "Loss at step 74: 0.44072237610816956\n",
            "Acc:  0.96875\n",
            "Loss at step 75: 0.42541226744651794\n",
            "Acc:  0.96875\n",
            "Loss at step 76: 0.4109959006309509\n",
            "Acc:  0.96875\n",
            "Loss at step 77: 0.3966473340988159\n",
            "Acc:  0.96875\n",
            "Loss at step 78: 0.38328495621681213\n",
            "Acc:  0.96875\n",
            "Loss at step 79: 0.36996760964393616\n",
            "Acc:  0.96875\n",
            "Loss at step 80: 0.3574753403663635\n",
            "Acc:  0.96875\n",
            "Loss at step 81: 0.34543222188949585\n",
            "Acc:  0.96875\n",
            "Loss at step 82: 0.333944708108902\n",
            "Acc:  0.96875\n",
            "Loss at step 83: 0.3229973018169403\n",
            "Acc:  0.96875\n",
            "Loss at step 84: 0.3125253915786743\n",
            "Acc:  0.96875\n",
            "Loss at step 85: 0.30222421884536743\n",
            "Acc:  0.96875\n",
            "Loss at step 86: 0.2926960587501526\n",
            "Acc:  0.96875\n",
            "Loss at step 87: 0.2834007441997528\n",
            "Acc:  0.96875\n",
            "Loss at step 88: 0.2743262052536011\n",
            "Acc:  0.96875\n",
            "Loss at step 89: 0.2658000886440277\n",
            "Acc:  0.96875\n",
            "Loss at step 90: 0.257625550031662\n",
            "Acc:  0.96875\n",
            "Loss at step 91: 0.2496289312839508\n",
            "Acc:  0.96875\n",
            "Loss at step 92: 0.2420169860124588\n",
            "Acc:  0.96875\n",
            "Loss at step 93: 0.23471973836421967\n",
            "Acc:  0.96875\n",
            "Loss at step 94: 0.22774918377399445\n",
            "Acc:  0.96875\n",
            "Loss at step 95: 0.2209809422492981\n",
            "Acc:  0.96875\n",
            "Loss at step 96: 0.21453548967838287\n",
            "Acc:  0.96875\n",
            "Loss at step 97: 0.20827236771583557\n",
            "Acc:  0.96875\n",
            "Loss at step 98: 0.20231610536575317\n",
            "Acc:  0.96875\n",
            "Loss at step 99: 0.1966390758752823\n",
            "Acc:  1.0\n",
            "Loss at step 100: 0.19119422137737274\n",
            "Acc:  1.0\n",
            "Loss at step 101: 0.18570035696029663\n",
            "Acc:  1.0\n",
            "Loss at step 102: 0.18053364753723145\n",
            "Acc:  1.0\n",
            "Loss at step 103: 0.17565852403640747\n",
            "Acc:  1.0\n",
            "Loss at step 104: 0.1709463745355606\n",
            "Acc:  1.0\n",
            "Loss at step 105: 0.16642440855503082\n",
            "Acc:  1.0\n",
            "Loss at step 106: 0.16218721866607666\n",
            "Acc:  1.0\n",
            "Loss at step 107: 0.15788044035434723\n",
            "Acc:  1.0\n",
            "Loss at step 108: 0.1538681983947754\n",
            "Acc:  1.0\n",
            "Loss at step 109: 0.14994648098945618\n",
            "Acc:  1.0\n",
            "Loss at step 110: 0.14627598226070404\n",
            "Acc:  1.0\n",
            "Loss at step 111: 0.14276082813739777\n",
            "Acc:  1.0\n",
            "Loss at step 112: 0.13919474184513092\n",
            "Acc:  1.0\n",
            "Loss at step 113: 0.13584154844284058\n",
            "Acc:  1.0\n",
            "Loss at step 114: 0.132601797580719\n",
            "Acc:  1.0\n",
            "Loss at step 115: 0.1295628398656845\n",
            "Acc:  1.0\n",
            "Loss at step 116: 0.12649230659008026\n",
            "Acc:  1.0\n",
            "Loss at step 117: 0.12355019897222519\n",
            "Acc:  1.0\n",
            "Loss at step 118: 0.12079848349094391\n",
            "Acc:  1.0\n",
            "Loss at step 119: 0.1181778833270073\n",
            "Acc:  1.0\n",
            "Loss at step 120: 0.11548013985157013\n",
            "Acc:  1.0\n",
            "Loss at step 121: 0.11299534887075424\n",
            "Acc:  1.0\n",
            "Loss at step 122: 0.11050204187631607\n",
            "Acc:  1.0\n",
            "Loss at step 123: 0.1081743836402893\n",
            "Acc:  1.0\n",
            "Loss at step 124: 0.10588064789772034\n",
            "Acc:  1.0\n",
            "Loss at step 125: 0.10374775528907776\n",
            "Acc:  1.0\n",
            "Loss at step 126: 0.1015569269657135\n",
            "Acc:  1.0\n",
            "Loss at step 127: 0.09950399398803711\n",
            "Acc:  1.0\n",
            "Loss at step 128: 0.09749139100313187\n",
            "Acc:  1.0\n",
            "Loss at step 129: 0.09557227790355682\n",
            "Acc:  1.0\n",
            "Loss at step 130: 0.09370456635951996\n",
            "Acc:  1.0\n",
            "Loss at step 131: 0.09181911498308182\n",
            "Acc:  1.0\n",
            "Loss at step 132: 0.09004952758550644\n",
            "Acc:  1.0\n",
            "Loss at step 133: 0.08834946155548096\n",
            "Acc:  1.0\n",
            "Loss at step 134: 0.08664634078741074\n",
            "Acc:  1.0\n",
            "Loss at step 135: 0.08501804620027542\n",
            "Acc:  1.0\n",
            "Loss at step 136: 0.08339739590883255\n",
            "Acc:  1.0\n",
            "Loss at step 137: 0.08183128386735916\n",
            "Acc:  1.0\n",
            "Loss at step 138: 0.08036234229803085\n",
            "Acc:  1.0\n",
            "Loss at step 139: 0.0788806676864624\n",
            "Acc:  1.0\n",
            "Loss at step 140: 0.07746192067861557\n",
            "Acc:  1.0\n",
            "Loss at step 141: 0.07603588700294495\n",
            "Acc:  1.0\n",
            "Loss at step 142: 0.07466697692871094\n",
            "Acc:  1.0\n",
            "Loss at step 143: 0.07338991016149521\n",
            "Acc:  1.0\n",
            "Loss at step 144: 0.07208959013223648\n",
            "Acc:  1.0\n",
            "Loss at step 145: 0.07083787024021149\n",
            "Acc:  1.0\n",
            "Loss at step 146: 0.06961558759212494\n",
            "Acc:  1.0\n",
            "Loss at step 147: 0.06841764599084854\n",
            "Acc:  1.0\n",
            "Loss at step 148: 0.06723388284444809\n",
            "Acc:  1.0\n",
            "Loss at step 149: 0.0660729631781578\n",
            "Acc:  1.0\n",
            "Loss at step 150: 0.06498388946056366\n",
            "Acc:  1.0\n",
            "Loss at step 151: 0.06392132490873337\n",
            "Acc:  1.0\n",
            "Loss at step 152: 0.06280303001403809\n",
            "Acc:  1.0\n",
            "Loss at step 153: 0.061793237924575806\n",
            "Acc:  1.0\n",
            "Loss at step 154: 0.06072971597313881\n",
            "Acc:  1.0\n",
            "Loss at step 155: 0.05974951386451721\n",
            "Acc:  1.0\n",
            "Loss at step 156: 0.058765966445207596\n",
            "Acc:  1.0\n",
            "Loss at step 157: 0.05778365582227707\n",
            "Acc:  1.0\n",
            "Loss at step 158: 0.05686870589852333\n",
            "Acc:  1.0\n",
            "Loss at step 159: 0.05592229589819908\n",
            "Acc:  1.0\n",
            "Loss at step 160: 0.05500361695885658\n",
            "Acc:  1.0\n",
            "Loss at step 161: 0.054086875170469284\n",
            "Acc:  1.0\n",
            "Loss at step 162: 0.05320977419614792\n",
            "Acc:  1.0\n",
            "Loss at step 163: 0.05233873054385185\n",
            "Acc:  1.0\n",
            "Loss at step 164: 0.05151941254734993\n",
            "Acc:  1.0\n",
            "Loss at step 165: 0.050650421530008316\n",
            "Acc:  1.0\n",
            "Loss at step 166: 0.04983890801668167\n",
            "Acc:  1.0\n",
            "Loss at step 167: 0.04908076301217079\n",
            "Acc:  1.0\n",
            "Loss at step 168: 0.04826894402503967\n",
            "Acc:  1.0\n",
            "Loss at step 169: 0.04750700667500496\n",
            "Acc:  1.0\n",
            "Loss at step 170: 0.046776577830314636\n",
            "Acc:  1.0\n",
            "Loss at step 171: 0.046015530824661255\n",
            "Acc:  1.0\n",
            "Loss at step 172: 0.04531698673963547\n",
            "Acc:  1.0\n",
            "Loss at step 173: 0.0446004681289196\n",
            "Acc:  1.0\n",
            "Loss at step 174: 0.04390017315745354\n",
            "Acc:  1.0\n",
            "Loss at step 175: 0.04324868321418762\n",
            "Acc:  1.0\n",
            "Loss at step 176: 0.04255623742938042\n",
            "Acc:  1.0\n",
            "Loss at step 177: 0.04191266745328903\n",
            "Acc:  1.0\n",
            "Loss at step 178: 0.04127122461795807\n",
            "Acc:  1.0\n",
            "Loss at step 179: 0.04062659293413162\n",
            "Acc:  1.0\n",
            "Loss at step 180: 0.040013816207647324\n",
            "Acc:  1.0\n",
            "Loss at step 181: 0.03939859941601753\n",
            "Acc:  1.0\n",
            "Loss at step 182: 0.038787998259067535\n",
            "Acc:  1.0\n",
            "Loss at step 183: 0.03823654353618622\n",
            "Acc:  1.0\n",
            "Loss at step 184: 0.03763989731669426\n",
            "Acc:  1.0\n",
            "Loss at step 185: 0.03706999868154526\n",
            "Acc:  1.0\n",
            "Loss at step 186: 0.0365486666560173\n",
            "Acc:  1.0\n",
            "Loss at step 187: 0.036002159118652344\n",
            "Acc:  1.0\n",
            "Loss at step 188: 0.03547415882349014\n",
            "Acc:  1.0\n",
            "Loss at step 189: 0.03495758771896362\n",
            "Acc:  1.0\n",
            "Loss at step 190: 0.03444644808769226\n",
            "Acc:  1.0\n",
            "Loss at step 191: 0.03396424278616905\n",
            "Acc:  1.0\n",
            "Loss at step 192: 0.033476561307907104\n",
            "Acc:  1.0\n",
            "Loss at step 193: 0.03300406038761139\n",
            "Acc:  1.0\n",
            "Loss at step 194: 0.032533254474401474\n",
            "Acc:  1.0\n",
            "Loss at step 195: 0.03206804767251015\n",
            "Acc:  1.0\n",
            "Loss at step 196: 0.031624678522348404\n",
            "Acc:  1.0\n",
            "Loss at step 197: 0.03117488883435726\n",
            "Acc:  1.0\n",
            "Loss at step 198: 0.03073781356215477\n",
            "Acc:  1.0\n",
            "Loss at step 199: 0.030332600697875023\n",
            "Acc:  1.0\n",
            "Loss at step 200: 0.029893886297941208\n",
            "Acc:  1.0\n",
            "Loss at step 201: 0.02949356660246849\n",
            "Acc:  1.0\n",
            "Loss at step 202: 0.02909131348133087\n",
            "Acc:  1.0\n",
            "Loss at step 203: 0.028687018901109695\n",
            "Acc:  1.0\n",
            "Loss at step 204: 0.02831042744219303\n",
            "Acc:  1.0\n",
            "Loss at step 205: 0.027913957834243774\n",
            "Acc:  1.0\n",
            "Loss at step 206: 0.027562247589230537\n",
            "Acc:  1.0\n",
            "Loss at step 207: 0.027178432792425156\n",
            "Acc:  1.0\n",
            "Loss at step 208: 0.026821348816156387\n",
            "Acc:  1.0\n",
            "Loss at step 209: 0.026475392282009125\n",
            "Acc:  1.0\n",
            "Loss at step 210: 0.02612733654677868\n",
            "Acc:  1.0\n",
            "Loss at step 211: 0.025779947638511658\n",
            "Acc:  1.0\n",
            "Loss at step 212: 0.02544339746236801\n",
            "Acc:  1.0\n",
            "Loss at step 213: 0.025128569453954697\n",
            "Acc:  1.0\n",
            "Loss at step 214: 0.024807438254356384\n",
            "Acc:  1.0\n",
            "Loss at step 215: 0.024486858397722244\n",
            "Acc:  1.0\n",
            "Loss at step 216: 0.024173924699425697\n",
            "Acc:  1.0\n",
            "Loss at step 217: 0.0238566342741251\n",
            "Acc:  1.0\n",
            "Loss at step 218: 0.02358047105371952\n",
            "Acc:  1.0\n",
            "Loss at step 219: 0.02326122671365738\n",
            "Acc:  1.0\n",
            "Loss at step 220: 0.022972745820879936\n",
            "Acc:  1.0\n",
            "Loss at step 221: 0.022700900211930275\n",
            "Acc:  1.0\n",
            "Loss at step 222: 0.02239941991865635\n",
            "Acc:  1.0\n",
            "Loss at step 223: 0.02212727628648281\n",
            "Acc:  1.0\n",
            "Loss at step 224: 0.02186097390949726\n",
            "Acc:  1.0\n",
            "Loss at step 225: 0.021579371765255928\n",
            "Acc:  1.0\n",
            "Loss at step 226: 0.021323414519429207\n",
            "Acc:  1.0\n",
            "Loss at step 227: 0.021062033250927925\n",
            "Acc:  1.0\n",
            "Loss at step 228: 0.020808998495340347\n",
            "Acc:  1.0\n",
            "Loss at step 229: 0.020555546507239342\n",
            "Acc:  1.0\n",
            "Loss at step 230: 0.02031630091369152\n",
            "Acc:  1.0\n",
            "Loss at step 231: 0.020072657614946365\n",
            "Acc:  1.0\n",
            "Loss at step 232: 0.019841497763991356\n",
            "Acc:  1.0\n",
            "Loss at step 233: 0.019602742046117783\n",
            "Acc:  1.0\n",
            "Loss at step 234: 0.019372601062059402\n",
            "Acc:  1.0\n",
            "Loss at step 235: 0.01915309578180313\n",
            "Acc:  1.0\n",
            "Loss at step 236: 0.01893829181790352\n",
            "Acc:  1.0\n",
            "Loss at step 237: 0.01871454156935215\n",
            "Acc:  1.0\n",
            "Loss at step 238: 0.018505169078707695\n",
            "Acc:  1.0\n",
            "Loss at step 239: 0.01829794980585575\n",
            "Acc:  1.0\n",
            "Loss at step 240: 0.018092218786478043\n",
            "Acc:  1.0\n",
            "Loss at step 241: 0.017895696684718132\n",
            "Acc:  1.0\n",
            "Loss at step 242: 0.017695706337690353\n",
            "Acc:  1.0\n",
            "Loss at step 243: 0.017504047602415085\n",
            "Acc:  1.0\n",
            "Loss at step 244: 0.01731492578983307\n",
            "Acc:  1.0\n",
            "Loss at step 245: 0.017135465517640114\n",
            "Acc:  1.0\n",
            "Loss at step 246: 0.016940582543611526\n",
            "Acc:  1.0\n",
            "Loss at step 247: 0.01676248386502266\n",
            "Acc:  1.0\n",
            "Loss at step 248: 0.01659027487039566\n",
            "Acc:  1.0\n",
            "Loss at step 249: 0.016407666727900505\n",
            "Acc:  1.0\n",
            "Loss at step 250: 0.016241205856204033\n",
            "Acc:  1.0\n",
            "Loss at step 251: 0.016068026423454285\n",
            "Acc:  1.0\n",
            "Loss at step 252: 0.015906408429145813\n",
            "Acc:  1.0\n",
            "Loss at step 253: 0.01573862135410309\n",
            "Acc:  1.0\n",
            "Loss at step 254: 0.015580352395772934\n",
            "Acc:  1.0\n",
            "Loss at step 255: 0.015422317199409008\n",
            "Acc:  1.0\n",
            "Loss at step 256: 0.015270333737134933\n",
            "Acc:  1.0\n",
            "Loss at step 257: 0.015111228451132774\n",
            "Acc:  1.0\n",
            "Loss at step 258: 0.014961937442421913\n",
            "Acc:  1.0\n",
            "Loss at step 259: 0.014814934693276882\n",
            "Acc:  1.0\n",
            "Loss at step 260: 0.014665884897112846\n",
            "Acc:  1.0\n",
            "Loss at step 261: 0.014524690806865692\n",
            "Acc:  1.0\n",
            "Loss at step 262: 0.01438616868108511\n",
            "Acc:  1.0\n",
            "Loss at step 263: 0.014240721240639687\n",
            "Acc:  1.0\n",
            "Loss at step 264: 0.01410684920847416\n",
            "Acc:  1.0\n",
            "Loss at step 265: 0.013968061655759811\n",
            "Acc:  1.0\n",
            "Loss at step 266: 0.013836359605193138\n",
            "Acc:  1.0\n",
            "Loss at step 267: 0.013706079684197903\n",
            "Acc:  1.0\n",
            "Loss at step 268: 0.013580511324107647\n",
            "Acc:  1.0\n",
            "Loss at step 269: 0.013449402526021004\n",
            "Acc:  1.0\n",
            "Loss at step 270: 0.013324066065251827\n",
            "Acc:  1.0\n",
            "Loss at step 271: 0.013205406256020069\n",
            "Acc:  1.0\n",
            "Loss at step 272: 0.013079685159027576\n",
            "Acc:  1.0\n",
            "Loss at step 273: 0.012963424436748028\n",
            "Acc:  1.0\n",
            "Loss at step 274: 0.012846451252698898\n",
            "Acc:  1.0\n",
            "Loss at step 275: 0.01272723451256752\n",
            "Acc:  1.0\n",
            "Loss at step 276: 0.012614917941391468\n",
            "Acc:  1.0\n",
            "Loss at step 277: 0.012499561533331871\n",
            "Acc:  1.0\n",
            "Loss at step 278: 0.01239120215177536\n",
            "Acc:  1.0\n",
            "Loss at step 279: 0.012281229719519615\n",
            "Acc:  1.0\n",
            "Loss at step 280: 0.01217640656977892\n",
            "Acc:  1.0\n",
            "Loss at step 281: 0.012068136595189571\n",
            "Acc:  1.0\n",
            "Loss at step 282: 0.011963668279349804\n",
            "Acc:  1.0\n",
            "Loss at step 283: 0.011863559484481812\n",
            "Acc:  1.0\n",
            "Loss at step 284: 0.011756815947592258\n",
            "Acc:  1.0\n",
            "Loss at step 285: 0.011661426164209843\n",
            "Acc:  1.0\n",
            "Loss at step 286: 0.011560192331671715\n",
            "Acc:  1.0\n",
            "Loss at step 287: 0.011463077738881111\n",
            "Acc:  1.0\n",
            "Loss at step 288: 0.01137020718306303\n",
            "Acc:  1.0\n",
            "Loss at step 289: 0.011275188066065311\n",
            "Acc:  1.0\n",
            "Loss at step 290: 0.011181413196027279\n",
            "Acc:  1.0\n",
            "Loss at step 291: 0.011088041588664055\n",
            "Acc:  1.0\n",
            "Loss at step 292: 0.011000098660588264\n",
            "Acc:  1.0\n",
            "Loss at step 293: 0.01090848445892334\n",
            "Acc:  1.0\n",
            "Loss at step 294: 0.010822398588061333\n",
            "Acc:  1.0\n",
            "Loss at step 295: 0.01073989924043417\n",
            "Acc:  1.0\n",
            "Loss at step 296: 0.010646449401974678\n",
            "Acc:  1.0\n",
            "Loss at step 297: 0.01056752447038889\n",
            "Acc:  1.0\n",
            "Loss at step 298: 0.010484147816896439\n",
            "Acc:  1.0\n",
            "Loss at step 299: 0.0103983199223876\n",
            "Acc:  1.0\n",
            "Loss at step 300: 0.010319421999156475\n",
            "Acc:  1.0\n",
            "Loss at step 301: 0.01023804396390915\n",
            "Acc:  1.0\n",
            "Loss at step 302: 0.0101620489731431\n",
            "Acc:  1.0\n",
            "Loss at step 303: 0.010079524479806423\n",
            "Acc:  1.0\n",
            "Loss at step 304: 0.010005886666476727\n",
            "Acc:  1.0\n",
            "Loss at step 305: 0.009931366890668869\n",
            "Acc:  1.0\n",
            "Loss at step 306: 0.009851221926510334\n",
            "Acc:  1.0\n",
            "Loss at step 307: 0.00978151150047779\n",
            "Acc:  1.0\n",
            "Loss at step 308: 0.009705745615065098\n",
            "Acc:  1.0\n",
            "Loss at step 309: 0.009635010734200478\n",
            "Acc:  1.0\n",
            "Loss at step 310: 0.009562626481056213\n",
            "Acc:  1.0\n",
            "Loss at step 311: 0.009494110010564327\n",
            "Acc:  1.0\n",
            "Loss at step 312: 0.009424228221178055\n",
            "Acc:  1.0\n",
            "Loss at step 313: 0.009355512447655201\n",
            "Acc:  1.0\n",
            "Loss at step 314: 0.009290565736591816\n",
            "Acc:  1.0\n",
            "Loss at step 315: 0.009219327010214329\n",
            "Acc:  1.0\n",
            "Loss at step 316: 0.009155606850981712\n",
            "Acc:  1.0\n",
            "Loss at step 317: 0.009090115316212177\n",
            "Acc:  1.0\n",
            "Loss at step 318: 0.009025750681757927\n",
            "Acc:  1.0\n",
            "Loss at step 319: 0.008961580693721771\n",
            "Acc:  1.0\n",
            "Loss at step 320: 0.008899591863155365\n",
            "Acc:  1.0\n",
            "Loss at step 321: 0.008837073110044003\n",
            "Acc:  1.0\n",
            "Loss at step 322: 0.008774678222835064\n",
            "Acc:  1.0\n",
            "Loss at step 323: 0.008715564385056496\n",
            "Acc:  1.0\n",
            "Loss at step 324: 0.008656485006213188\n",
            "Acc:  1.0\n",
            "Loss at step 325: 0.008595563471317291\n",
            "Acc:  1.0\n",
            "Loss at step 326: 0.008539322763681412\n",
            "Acc:  1.0\n",
            "Loss at step 327: 0.008479587733745575\n",
            "Acc:  1.0\n",
            "Loss at step 328: 0.00842318031936884\n",
            "Acc:  1.0\n",
            "Loss at step 329: 0.008365882560610771\n",
            "Acc:  1.0\n",
            "Loss at step 330: 0.008311688899993896\n",
            "Acc:  1.0\n",
            "Loss at step 331: 0.008255824446678162\n",
            "Acc:  1.0\n",
            "Loss at step 332: 0.008202122524380684\n",
            "Acc:  1.0\n",
            "Loss at step 333: 0.008150008507072926\n",
            "Acc:  1.0\n",
            "Loss at step 334: 0.00809342972934246\n",
            "Acc:  1.0\n",
            "Loss at step 335: 0.008041777648031712\n",
            "Acc:  1.0\n",
            "Loss at step 336: 0.007989822886884212\n",
            "Acc:  1.0\n",
            "Loss at step 337: 0.007939795032143593\n",
            "Acc:  1.0\n",
            "Loss at step 338: 0.007887868210673332\n",
            "Acc:  1.0\n",
            "Loss at step 339: 0.007836787961423397\n",
            "Acc:  1.0\n",
            "Loss at step 340: 0.007789288181811571\n",
            "Acc:  1.0\n",
            "Loss at step 341: 0.007737325970083475\n",
            "Acc:  1.0\n",
            "Loss at step 342: 0.00769063038751483\n",
            "Acc:  1.0\n",
            "Loss at step 343: 0.007641103584319353\n",
            "Acc:  1.0\n",
            "Loss at step 344: 0.007592966314405203\n",
            "Acc:  1.0\n",
            "Loss at step 345: 0.007547419983893633\n",
            "Acc:  1.0\n",
            "Loss at step 346: 0.007501091342419386\n",
            "Acc:  1.0\n",
            "Loss at step 347: 0.007453911006450653\n",
            "Acc:  1.0\n",
            "Loss at step 348: 0.0074091567657887936\n",
            "Acc:  1.0\n",
            "Loss at step 349: 0.007364003453403711\n",
            "Acc:  1.0\n",
            "Loss at step 350: 0.00731859402731061\n",
            "Acc:  1.0\n",
            "Loss at step 351: 0.00727480323985219\n",
            "Acc:  1.0\n",
            "Loss at step 352: 0.007231452036648989\n",
            "Acc:  1.0\n",
            "Loss at step 353: 0.007187720388174057\n",
            "Acc:  1.0\n",
            "Loss at step 354: 0.007145533803850412\n",
            "Acc:  1.0\n",
            "Loss at step 355: 0.007103647105395794\n",
            "Acc:  1.0\n",
            "Loss at step 356: 0.007061233278363943\n",
            "Acc:  1.0\n",
            "Loss at step 357: 0.007019952870905399\n",
            "Acc:  1.0\n",
            "Loss at step 358: 0.006978534162044525\n",
            "Acc:  1.0\n",
            "Loss at step 359: 0.00693806167691946\n",
            "Acc:  1.0\n",
            "Loss at step 360: 0.006897320970892906\n",
            "Acc:  1.0\n",
            "Loss at step 361: 0.0068588946014642715\n",
            "Acc:  1.0\n",
            "Loss at step 362: 0.0068189832381904125\n",
            "Acc:  1.0\n",
            "Loss at step 363: 0.006779450457543135\n",
            "Acc:  1.0\n",
            "Loss at step 364: 0.00674151536077261\n",
            "Acc:  1.0\n",
            "Loss at step 365: 0.006703183986246586\n",
            "Acc:  1.0\n",
            "Loss at step 366: 0.006667114328593016\n",
            "Acc:  1.0\n",
            "Loss at step 367: 0.0066270786337554455\n",
            "Acc:  1.0\n",
            "Loss at step 368: 0.006591247860342264\n",
            "Acc:  1.0\n",
            "Loss at step 369: 0.0065540443174541\n",
            "Acc:  1.0\n",
            "Loss at step 370: 0.006517369300127029\n",
            "Acc:  1.0\n",
            "Loss at step 371: 0.006481969263404608\n",
            "Acc:  1.0\n",
            "Loss at step 372: 0.006446062587201595\n",
            "Acc:  1.0\n",
            "Loss at step 373: 0.006411930546164513\n",
            "Acc:  1.0\n",
            "Loss at step 374: 0.0063760667107999325\n",
            "Acc:  1.0\n",
            "Loss at step 375: 0.006340995896607637\n",
            "Acc:  1.0\n",
            "Loss at step 376: 0.006308795418590307\n",
            "Acc:  1.0\n",
            "Loss at step 377: 0.006272915285080671\n",
            "Acc:  1.0\n",
            "Loss at step 378: 0.0062399972230196\n",
            "Acc:  1.0\n",
            "Loss at step 379: 0.006206332705914974\n",
            "Acc:  1.0\n",
            "Loss at step 380: 0.006173484493046999\n",
            "Acc:  1.0\n",
            "Loss at step 381: 0.006141296587884426\n",
            "Acc:  1.0\n",
            "Loss at step 382: 0.0061081997118890285\n",
            "Acc:  1.0\n",
            "Loss at step 383: 0.006076638586819172\n",
            "Acc:  1.0\n",
            "Loss at step 384: 0.006043965928256512\n",
            "Acc:  1.0\n",
            "Loss at step 385: 0.006013966631144285\n",
            "Acc:  1.0\n",
            "Loss at step 386: 0.00598146440461278\n",
            "Acc:  1.0\n",
            "Loss at step 387: 0.005950713064521551\n",
            "Acc:  1.0\n",
            "Loss at step 388: 0.005920838098973036\n",
            "Acc:  1.0\n",
            "Loss at step 389: 0.0058895074762403965\n",
            "Acc:  1.0\n",
            "Loss at step 390: 0.005861237179487944\n",
            "Acc:  1.0\n",
            "Loss at step 391: 0.005830918438732624\n",
            "Acc:  1.0\n",
            "Loss at step 392: 0.005800389219075441\n",
            "Acc:  1.0\n",
            "Loss at step 393: 0.005771835800260305\n",
            "Acc:  1.0\n",
            "Loss at step 394: 0.005742290522903204\n",
            "Acc:  1.0\n",
            "Loss at step 395: 0.0057144309394061565\n",
            "Acc:  1.0\n",
            "Loss at step 396: 0.005684835836291313\n",
            "Acc:  1.0\n",
            "Loss at step 397: 0.005657872650772333\n",
            "Acc:  1.0\n",
            "Loss at step 398: 0.00562873762100935\n",
            "Acc:  1.0\n",
            "Loss at step 399: 0.005601875018328428\n",
            "Acc:  1.0\n",
            "Loss at step 400: 0.0055742422118783\n",
            "Acc:  1.0\n",
            "Loss at step 401: 0.005546612199395895\n",
            "Acc:  1.0\n",
            "Loss at step 402: 0.005520082078874111\n",
            "Acc:  1.0\n",
            "Loss at step 403: 0.0054926034063100815\n",
            "Acc:  1.0\n",
            "Loss at step 404: 0.00546649657189846\n",
            "Acc:  1.0\n",
            "Loss at step 405: 0.005439820233732462\n",
            "Acc:  1.0\n",
            "Loss at step 406: 0.005413446109741926\n",
            "Acc:  1.0\n",
            "Loss at step 407: 0.005388342309743166\n",
            "Acc:  1.0\n",
            "Loss at step 408: 0.005363182630389929\n",
            "Acc:  1.0\n",
            "Loss at step 409: 0.005337052047252655\n",
            "Acc:  1.0\n",
            "Loss at step 410: 0.0053117843344807625\n",
            "Acc:  1.0\n",
            "Loss at step 411: 0.005287158768624067\n",
            "Acc:  1.0\n",
            "Loss at step 412: 0.0052618179470300674\n",
            "Acc:  1.0\n",
            "Loss at step 413: 0.0052375406958162785\n",
            "Acc:  1.0\n",
            "Loss at step 414: 0.005213405005633831\n",
            "Acc:  1.0\n",
            "Loss at step 415: 0.005188620649278164\n",
            "Acc:  1.0\n",
            "Loss at step 416: 0.005165124777704477\n",
            "Acc:  1.0\n",
            "Loss at step 417: 0.005140801426023245\n",
            "Acc:  1.0\n",
            "Loss at step 418: 0.0051174890249967575\n",
            "Acc:  1.0\n",
            "Loss at step 419: 0.005094108171761036\n",
            "Acc:  1.0\n",
            "Loss at step 420: 0.005070574581623077\n",
            "Acc:  1.0\n",
            "Loss at step 421: 0.005048197694122791\n",
            "Acc:  1.0\n",
            "Loss at step 422: 0.005026007071137428\n",
            "Acc:  1.0\n",
            "Loss at step 423: 0.005002111662179232\n",
            "Acc:  1.0\n",
            "Loss at step 424: 0.004980059340596199\n",
            "Acc:  1.0\n",
            "Loss at step 425: 0.004957763943821192\n",
            "Acc:  1.0\n",
            "Loss at step 426: 0.0049354746006429195\n",
            "Acc:  1.0\n",
            "Loss at step 427: 0.004913643002510071\n",
            "Acc:  1.0\n",
            "Loss at step 428: 0.00489230128005147\n",
            "Acc:  1.0\n",
            "Loss at step 429: 0.004870211705565453\n",
            "Acc:  1.0\n",
            "Loss at step 430: 0.004848719108849764\n",
            "Acc:  1.0\n",
            "Loss at step 431: 0.004828410688787699\n",
            "Acc:  1.0\n",
            "Loss at step 432: 0.004806245677173138\n",
            "Acc:  1.0\n",
            "Loss at step 433: 0.004785953555256128\n",
            "Acc:  1.0\n",
            "Loss at step 434: 0.004764578305184841\n",
            "Acc:  1.0\n",
            "Loss at step 435: 0.004744107369333506\n",
            "Acc:  1.0\n",
            "Loss at step 436: 0.004724368453025818\n",
            "Acc:  1.0\n",
            "Loss at step 437: 0.004703799728304148\n",
            "Acc:  1.0\n",
            "Loss at step 438: 0.004683253820985556\n",
            "Acc:  1.0\n",
            "Loss at step 439: 0.004663675092160702\n",
            "Acc:  1.0\n",
            "Loss at step 440: 0.004643420223146677\n",
            "Acc:  1.0\n",
            "Loss at step 441: 0.004624251741915941\n",
            "Acc:  1.0\n",
            "Loss at step 442: 0.0046043978072702885\n",
            "Acc:  1.0\n",
            "Loss at step 443: 0.004585971124470234\n",
            "Acc:  1.0\n",
            "Loss at step 444: 0.004565639887005091\n",
            "Acc:  1.0\n",
            "Loss at step 445: 0.004546924959868193\n",
            "Acc:  1.0\n",
            "Loss at step 446: 0.00452854810282588\n",
            "Acc:  1.0\n",
            "Loss at step 447: 0.004508849699050188\n",
            "Acc:  1.0\n",
            "Loss at step 448: 0.004490547813475132\n",
            "Acc:  1.0\n",
            "Loss at step 449: 0.004472160711884499\n",
            "Acc:  1.0\n",
            "Loss at step 450: 0.004453582689166069\n",
            "Acc:  1.0\n",
            "Loss at step 451: 0.0044357082806527615\n",
            "Acc:  1.0\n",
            "Loss at step 452: 0.004417166113853455\n",
            "Acc:  1.0\n",
            "Loss at step 453: 0.004399591125547886\n",
            "Acc:  1.0\n",
            "Loss at step 454: 0.0043812962248921394\n",
            "Acc:  1.0\n",
            "Loss at step 455: 0.004364170134067535\n",
            "Acc:  1.0\n",
            "Loss at step 456: 0.004346102476119995\n",
            "Acc:  1.0\n",
            "Loss at step 457: 0.004328632727265358\n",
            "Acc:  1.0\n",
            "Loss at step 458: 0.00431135343387723\n",
            "Acc:  1.0\n",
            "Loss at step 459: 0.004294496960937977\n",
            "Acc:  1.0\n",
            "Loss at step 460: 0.004277348984032869\n",
            "Acc:  1.0\n",
            "Loss at step 461: 0.004259851295500994\n",
            "Acc:  1.0\n",
            "Loss at step 462: 0.004243639763444662\n",
            "Acc:  1.0\n",
            "Loss at step 463: 0.004226629622280598\n",
            "Acc:  1.0\n",
            "Loss at step 464: 0.004210157785564661\n",
            "Acc:  1.0\n",
            "Loss at step 465: 0.004193819593638182\n",
            "Acc:  1.0\n",
            "Loss at step 466: 0.004176951013505459\n",
            "Acc:  1.0\n",
            "Loss at step 467: 0.004161180928349495\n",
            "Acc:  1.0\n",
            "Loss at step 468: 0.004144709557294846\n",
            "Acc:  1.0\n",
            "Loss at step 469: 0.004128650762140751\n",
            "Acc:  1.0\n",
            "Loss at step 470: 0.004112682770937681\n",
            "Acc:  1.0\n",
            "Loss at step 471: 0.0040970053523778915\n",
            "Acc:  1.0\n",
            "Loss at step 472: 0.004081409424543381\n",
            "Acc:  1.0\n",
            "Loss at step 473: 0.004066264722496271\n",
            "Acc:  1.0\n",
            "Loss at step 474: 0.004050282761454582\n",
            "Acc:  1.0\n",
            "Loss at step 475: 0.004034717567265034\n",
            "Acc:  1.0\n",
            "Loss at step 476: 0.004019914660602808\n",
            "Acc:  1.0\n",
            "Loss at step 477: 0.0040041799657046795\n",
            "Acc:  1.0\n",
            "Loss at step 478: 0.0039892662316560745\n",
            "Acc:  1.0\n",
            "Loss at step 479: 0.003974147140979767\n",
            "Acc:  1.0\n",
            "Loss at step 480: 0.003960215020924807\n",
            "Acc:  1.0\n",
            "Loss at step 481: 0.003944629803299904\n",
            "Acc:  1.0\n",
            "Loss at step 482: 0.003930179867893457\n",
            "Acc:  1.0\n",
            "Loss at step 483: 0.003915662411600351\n",
            "Acc:  1.0\n",
            "Loss at step 484: 0.0039010723121464252\n",
            "Acc:  1.0\n",
            "Loss at step 485: 0.0038865492679178715\n",
            "Acc:  1.0\n",
            "Loss at step 486: 0.0038727843202650547\n",
            "Acc:  1.0\n",
            "Loss at step 487: 0.0038579569663852453\n",
            "Acc:  1.0\n",
            "Loss at step 488: 0.003843907034024596\n",
            "Acc:  1.0\n",
            "Loss at step 489: 0.003830073634162545\n",
            "Acc:  1.0\n",
            "Loss at step 490: 0.0038160791154950857\n",
            "Acc:  1.0\n",
            "Loss at step 491: 0.0038025309331715107\n",
            "Acc:  1.0\n",
            "Loss at step 492: 0.003788529895246029\n",
            "Acc:  1.0\n",
            "Loss at step 493: 0.0037751428317278624\n",
            "Acc:  1.0\n",
            "Loss at step 494: 0.0037614786997437477\n",
            "Acc:  1.0\n",
            "Loss at step 495: 0.003748056013137102\n",
            "Acc:  1.0\n",
            "Loss at step 496: 0.00373473996296525\n",
            "Acc:  1.0\n",
            "Loss at step 497: 0.0037215801421552896\n",
            "Acc:  1.0\n",
            "Loss at step 498: 0.003708499949425459\n",
            "Acc:  1.0\n",
            "Loss at step 499: 0.0036950386129319668\n",
            "Acc:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tutEaWQY8s"
      },
      "source": [
        "Ak si vytvoríme model ktorý nemá dostatočnú kapacitu. Napr. ak použijeme lineárny klasifikátor, tak sa nebude sieť vedieť naučiť na 100 percentnú presnosť ani na malej vzorke dát."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm5da6FaQvCS",
        "outputId": "a44d2cd5-2571-4081-b533-a01b2476f12f"
      },
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
        "model = Sequential(Linear(28 * 28, 30))\n",
        "\n",
        "ce_loss = torch.nn.CrossEntropyLoss()\n",
        "x_mini = x[:32]\n",
        "labels_mini = labels[:32]\n",
        "print(labels_mini)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "for i in range(1000):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out = model(x_mini)\n",
        "  loss = ce_loss(out, labels_mini)  \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"Loss at step {}: {}\".format(i, loss.item()))\n",
        "\n",
        "  out = model_inference(x_mini)\n",
        "  out_classes = torch.argmax(out, dim=-1)\n",
        "  acc = torch.sum(out_classes == labels_mini)/32\n",
        "  print(\"Acc: \", acc.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
            "        1, 2, 4, 3, 2, 7, 3, 8])\n",
            "Loss at step 0: 3.4615650177001953\n",
            "Acc:  0.9375\n",
            "Loss at step 1: 2.946012020111084\n",
            "Acc:  0.9375\n",
            "Loss at step 2: 2.542705535888672\n",
            "Acc:  0.9375\n",
            "Loss at step 3: 2.2401232719421387\n",
            "Acc:  0.9375\n",
            "Loss at step 4: 2.0013139247894287\n",
            "Acc:  0.9375\n",
            "Loss at step 5: 1.8021131753921509\n",
            "Acc:  0.9375\n",
            "Loss at step 6: 1.6321531534194946\n",
            "Acc:  0.9375\n",
            "Loss at step 7: 1.4857443571090698\n",
            "Acc:  0.9375\n",
            "Loss at step 8: 1.3587912321090698\n",
            "Acc:  0.9375\n",
            "Loss at step 9: 1.2480053901672363\n",
            "Acc:  0.9375\n",
            "Loss at step 10: 1.1507062911987305\n",
            "Acc:  0.9375\n",
            "Loss at step 11: 1.0647341012954712\n",
            "Acc:  0.9375\n",
            "Loss at step 12: 0.9883557558059692\n",
            "Acc:  0.9375\n",
            "Loss at step 13: 0.920174777507782\n",
            "Acc:  0.9375\n",
            "Loss at step 14: 0.8590525388717651\n",
            "Acc:  0.9375\n",
            "Loss at step 15: 0.8040497303009033\n",
            "Acc:  0.9375\n",
            "Loss at step 16: 0.7543814182281494\n",
            "Acc:  0.9375\n",
            "Loss at step 17: 0.7093861103057861\n",
            "Acc:  0.9375\n",
            "Loss at step 18: 0.6685011982917786\n",
            "Acc:  0.9375\n",
            "Loss at step 19: 0.6312450170516968\n",
            "Acc:  0.9375\n",
            "Loss at step 20: 0.5972034931182861\n",
            "Acc:  0.9375\n",
            "Loss at step 21: 0.5660183429718018\n",
            "Acc:  0.9375\n",
            "Loss at step 22: 0.5373789668083191\n",
            "Acc:  0.9375\n",
            "Loss at step 23: 0.5110151171684265\n",
            "Acc:  0.9375\n",
            "Loss at step 24: 0.48669061064720154\n",
            "Acc:  0.9375\n",
            "Loss at step 25: 0.4641989469528198\n",
            "Acc:  0.9375\n",
            "Loss at step 26: 0.4433589577674866\n",
            "Acc:  0.9375\n",
            "Loss at step 27: 0.4240112602710724\n",
            "Acc:  0.9375\n",
            "Loss at step 28: 0.40601539611816406\n",
            "Acc:  0.9375\n",
            "Loss at step 29: 0.3892471194267273\n",
            "Acc:  0.9375\n",
            "Loss at step 30: 0.37359628081321716\n",
            "Acc:  0.9375\n",
            "Loss at step 31: 0.35896509885787964\n",
            "Acc:  0.9375\n",
            "Loss at step 32: 0.345266193151474\n",
            "Acc:  0.9375\n",
            "Loss at step 33: 0.33242174983024597\n",
            "Acc:  0.9375\n",
            "Loss at step 34: 0.320361852645874\n",
            "Acc:  0.9375\n",
            "Loss at step 35: 0.3090236186981201\n",
            "Acc:  0.9375\n",
            "Loss at step 36: 0.29835057258605957\n",
            "Acc:  0.9375\n",
            "Loss at step 37: 0.28829142451286316\n",
            "Acc:  0.9375\n",
            "Loss at step 38: 0.2787998914718628\n",
            "Acc:  0.9375\n",
            "Loss at step 39: 0.26983410120010376\n",
            "Acc:  0.9375\n",
            "Loss at step 40: 0.26135557889938354\n",
            "Acc:  0.9375\n",
            "Loss at step 41: 0.25332948565483093\n",
            "Acc:  0.9375\n",
            "Loss at step 42: 0.2457241714000702\n",
            "Acc:  0.9375\n",
            "Loss at step 43: 0.23851025104522705\n",
            "Acc:  0.9375\n",
            "Loss at step 44: 0.23166117072105408\n",
            "Acc:  0.9375\n",
            "Loss at step 45: 0.22515246272087097\n",
            "Acc:  0.9375\n",
            "Loss at step 46: 0.21896152198314667\n",
            "Acc:  0.9375\n",
            "Loss at step 47: 0.2130676954984665\n",
            "Acc:  0.9375\n",
            "Loss at step 48: 0.20745182037353516\n",
            "Acc:  0.9375\n",
            "Loss at step 49: 0.20209647715091705\n",
            "Acc:  0.9375\n",
            "Loss at step 50: 0.19698525965213776\n",
            "Acc:  0.9375\n",
            "Loss at step 51: 0.19210317730903625\n",
            "Acc:  0.9375\n",
            "Loss at step 52: 0.1874363124370575\n",
            "Acc:  0.9375\n",
            "Loss at step 53: 0.18297183513641357\n",
            "Acc:  0.9375\n",
            "Loss at step 54: 0.17869776487350464\n",
            "Acc:  0.9375\n",
            "Loss at step 55: 0.17460301518440247\n",
            "Acc:  0.9375\n",
            "Loss at step 56: 0.17067734897136688\n",
            "Acc:  0.9375\n",
            "Loss at step 57: 0.16691119968891144\n",
            "Acc:  0.9375\n",
            "Loss at step 58: 0.16329553723335266\n",
            "Acc:  0.9375\n",
            "Loss at step 59: 0.1598222404718399\n",
            "Acc:  0.9375\n",
            "Loss at step 60: 0.15648341178894043\n",
            "Acc:  0.9375\n",
            "Loss at step 61: 0.15327192842960358\n",
            "Acc:  0.9375\n",
            "Loss at step 62: 0.15018105506896973\n",
            "Acc:  0.9375\n",
            "Loss at step 63: 0.14720454812049866\n",
            "Acc:  0.9375\n",
            "Loss at step 64: 0.14433644711971283\n",
            "Acc:  0.9375\n",
            "Loss at step 65: 0.14157120883464813\n",
            "Acc:  0.9375\n",
            "Loss at step 66: 0.13890376687049866\n",
            "Acc:  0.9375\n",
            "Loss at step 67: 0.1363292783498764\n",
            "Acc:  0.9375\n",
            "Loss at step 68: 0.13384312391281128\n",
            "Acc:  0.9375\n",
            "Loss at step 69: 0.1314411461353302\n",
            "Acc:  0.9375\n",
            "Loss at step 70: 0.12911924719810486\n",
            "Acc:  0.9375\n",
            "Loss at step 71: 0.1268737018108368\n",
            "Acc:  0.9375\n",
            "Loss at step 72: 0.12470097839832306\n",
            "Acc:  0.9375\n",
            "Loss at step 73: 0.12259773910045624\n",
            "Acc:  0.9375\n",
            "Loss at step 74: 0.12056082487106323\n",
            "Acc:  0.9375\n",
            "Loss at step 75: 0.11858730018138885\n",
            "Acc:  0.9375\n",
            "Loss at step 76: 0.11667433381080627\n",
            "Acc:  0.9375\n",
            "Loss at step 77: 0.11481937021017075\n",
            "Acc:  0.9375\n",
            "Loss at step 78: 0.11301977932453156\n",
            "Acc:  0.9375\n",
            "Loss at step 79: 0.11127332597970963\n",
            "Acc:  0.9375\n",
            "Loss at step 80: 0.10957776010036469\n",
            "Acc:  0.9375\n",
            "Loss at step 81: 0.10793085396289825\n",
            "Acc:  0.9375\n",
            "Loss at step 82: 0.1063307598233223\n",
            "Acc:  0.9375\n",
            "Loss at step 83: 0.10477553308010101\n",
            "Acc:  0.9375\n",
            "Loss at step 84: 0.10326334089040756\n",
            "Acc:  0.9375\n",
            "Loss at step 85: 0.10179252922534943\n",
            "Acc:  0.9375\n",
            "Loss at step 86: 0.10036139190196991\n",
            "Acc:  0.9375\n",
            "Loss at step 87: 0.0989684909582138\n",
            "Acc:  0.9375\n",
            "Loss at step 88: 0.09761229157447815\n",
            "Acc:  0.9375\n",
            "Loss at step 89: 0.09629146009683609\n",
            "Acc:  0.9375\n",
            "Loss at step 90: 0.09500466287136078\n",
            "Acc:  0.9375\n",
            "Loss at step 91: 0.09375063329935074\n",
            "Acc:  0.9375\n",
            "Loss at step 92: 0.09252814948558807\n",
            "Acc:  0.9375\n",
            "Loss at step 93: 0.09133610129356384\n",
            "Acc:  0.9375\n",
            "Loss at step 94: 0.09017334133386612\n",
            "Acc:  0.9375\n",
            "Loss at step 95: 0.0890389159321785\n",
            "Acc:  0.9375\n",
            "Loss at step 96: 0.08793173730373383\n",
            "Acc:  0.9375\n",
            "Loss at step 97: 0.08685091137886047\n",
            "Acc:  0.9375\n",
            "Loss at step 98: 0.08579555153846741\n",
            "Acc:  0.9375\n",
            "Loss at step 99: 0.08476478606462479\n",
            "Acc:  0.9375\n",
            "Loss at step 100: 0.08375773578882217\n",
            "Acc:  0.9375\n",
            "Loss at step 101: 0.08277364075183868\n",
            "Acc:  0.9375\n",
            "Loss at step 102: 0.08181175589561462\n",
            "Acc:  0.9375\n",
            "Loss at step 103: 0.08087136596441269\n",
            "Acc:  0.9375\n",
            "Loss at step 104: 0.07995174080133438\n",
            "Acc:  0.9375\n",
            "Loss at step 105: 0.07905226200819016\n",
            "Acc:  0.9375\n",
            "Loss at step 106: 0.0781722366809845\n",
            "Acc:  0.9375\n",
            "Loss at step 107: 0.07731103897094727\n",
            "Acc:  0.9375\n",
            "Loss at step 108: 0.07646815478801727\n",
            "Acc:  0.9375\n",
            "Loss at step 109: 0.07564303278923035\n",
            "Acc:  0.9375\n",
            "Loss at step 110: 0.07483502477407455\n",
            "Acc:  0.9375\n",
            "Loss at step 111: 0.07404369115829468\n",
            "Acc:  0.9375\n",
            "Loss at step 112: 0.07326849550008774\n",
            "Acc:  0.9375\n",
            "Loss at step 113: 0.0725090429186821\n",
            "Acc:  0.9375\n",
            "Loss at step 114: 0.07176470011472702\n",
            "Acc:  0.9375\n",
            "Loss at step 115: 0.07103518396615982\n",
            "Acc:  0.9375\n",
            "Loss at step 116: 0.07032003253698349\n",
            "Acc:  0.9375\n",
            "Loss at step 117: 0.0696188360452652\n",
            "Acc:  0.9375\n",
            "Loss at step 118: 0.06893113255500793\n",
            "Acc:  0.9375\n",
            "Loss at step 119: 0.06825660914182663\n",
            "Acc:  0.9375\n",
            "Loss at step 120: 0.06759486347436905\n",
            "Acc:  0.9375\n",
            "Loss at step 121: 0.06694556772708893\n",
            "Acc:  0.9375\n",
            "Loss at step 122: 0.06630834937095642\n",
            "Acc:  0.9375\n",
            "Loss at step 123: 0.06568292528390884\n",
            "Acc:  0.9375\n",
            "Loss at step 124: 0.06506894528865814\n",
            "Acc:  0.9375\n",
            "Loss at step 125: 0.06446607410907745\n",
            "Acc:  0.9375\n",
            "Loss at step 126: 0.06387408077716827\n",
            "Acc:  0.9375\n",
            "Loss at step 127: 0.06329260766506195\n",
            "Acc:  0.9375\n",
            "Loss at step 128: 0.06272148340940475\n",
            "Acc:  0.9375\n",
            "Loss at step 129: 0.06216033548116684\n",
            "Acc:  0.9375\n",
            "Loss at step 130: 0.061608996242284775\n",
            "Acc:  0.9375\n",
            "Loss at step 131: 0.06106717139482498\n",
            "Acc:  0.9375\n",
            "Loss at step 132: 0.060534555464982986\n",
            "Acc:  0.9375\n",
            "Loss at step 133: 0.06001105159521103\n",
            "Acc:  0.9375\n",
            "Loss at step 134: 0.05949633568525314\n",
            "Acc:  0.9375\n",
            "Loss at step 135: 0.05899021029472351\n",
            "Acc:  0.9375\n",
            "Loss at step 136: 0.05849252641201019\n",
            "Acc:  0.9375\n",
            "Loss at step 137: 0.05800297483801842\n",
            "Acc:  0.9375\n",
            "Loss at step 138: 0.057521432638168335\n",
            "Acc:  0.9375\n",
            "Loss at step 139: 0.05704765021800995\n",
            "Acc:  0.9375\n",
            "Loss at step 140: 0.0565815232694149\n",
            "Acc:  0.9375\n",
            "Loss at step 141: 0.056122854351997375\n",
            "Acc:  0.9375\n",
            "Loss at step 142: 0.055671386420726776\n",
            "Acc:  0.9375\n",
            "Loss at step 143: 0.055227022618055344\n",
            "Acc:  0.9375\n",
            "Loss at step 144: 0.05478961020708084\n",
            "Acc:  0.9375\n",
            "Loss at step 145: 0.05435897409915924\n",
            "Acc:  0.9375\n",
            "Loss at step 146: 0.053934935480356216\n",
            "Acc:  0.9375\n",
            "Loss at step 147: 0.05351736396551132\n",
            "Acc:  0.9375\n",
            "Loss at step 148: 0.05310610681772232\n",
            "Acc:  0.9375\n",
            "Loss at step 149: 0.05270102992653847\n",
            "Acc:  0.9375\n",
            "Loss at step 150: 0.05230199545621872\n",
            "Acc:  0.9375\n",
            "Loss at step 151: 0.051908913999795914\n",
            "Acc:  0.9375\n",
            "Loss at step 152: 0.05152153968811035\n",
            "Acc:  0.9375\n",
            "Loss at step 153: 0.051139891147613525\n",
            "Acc:  0.9375\n",
            "Loss at step 154: 0.05076375603675842\n",
            "Acc:  0.9375\n",
            "Loss at step 155: 0.0503930039703846\n",
            "Acc:  0.9375\n",
            "Loss at step 156: 0.05002758279442787\n",
            "Acc:  0.9375\n",
            "Loss at step 157: 0.049667373299598694\n",
            "Acc:  0.9375\n",
            "Loss at step 158: 0.049312226474285126\n",
            "Acc:  0.9375\n",
            "Loss at step 159: 0.0489620678126812\n",
            "Acc:  0.9375\n",
            "Loss at step 160: 0.04861677810549736\n",
            "Acc:  0.9375\n",
            "Loss at step 161: 0.04827624559402466\n",
            "Acc:  0.9375\n",
            "Loss at step 162: 0.0479404516518116\n",
            "Acc:  0.9375\n",
            "Loss at step 163: 0.047609202563762665\n",
            "Acc:  0.9375\n",
            "Loss at step 164: 0.047282397747039795\n",
            "Acc:  0.9375\n",
            "Loss at step 165: 0.04696004092693329\n",
            "Acc:  0.9375\n",
            "Loss at step 166: 0.046642005443573\n",
            "Acc:  0.9375\n",
            "Loss at step 167: 0.04632817953824997\n",
            "Acc:  0.9375\n",
            "Loss at step 168: 0.04601852968335152\n",
            "Acc:  0.9375\n",
            "Loss at step 169: 0.0457129031419754\n",
            "Acc:  0.9375\n",
            "Loss at step 170: 0.04541124403476715\n",
            "Acc:  0.9375\n",
            "Loss at step 171: 0.04511357098817825\n",
            "Acc:  0.9375\n",
            "Loss at step 172: 0.04481968656182289\n",
            "Acc:  0.9375\n",
            "Loss at step 173: 0.04452958330512047\n",
            "Acc:  0.9375\n",
            "Loss at step 174: 0.04424316808581352\n",
            "Acc:  0.9375\n",
            "Loss at step 175: 0.0439603291451931\n",
            "Acc:  0.9375\n",
            "Loss at step 176: 0.04368109256029129\n",
            "Acc:  0.9375\n",
            "Loss at step 177: 0.04340536147356033\n",
            "Acc:  0.9375\n",
            "Loss at step 178: 0.043132975697517395\n",
            "Acc:  0.9375\n",
            "Loss at step 179: 0.04286406561732292\n",
            "Acc:  0.9375\n",
            "Loss at step 180: 0.04259839281439781\n",
            "Acc:  0.9375\n",
            "Loss at step 181: 0.04233596846461296\n",
            "Acc:  0.9375\n",
            "Loss at step 182: 0.04207669571042061\n",
            "Acc:  0.9375\n",
            "Loss at step 183: 0.041820574551820755\n",
            "Acc:  0.9375\n",
            "Loss at step 184: 0.04156755283474922\n",
            "Acc:  0.9375\n",
            "Loss at step 185: 0.04131746664643288\n",
            "Acc:  0.9375\n",
            "Loss at step 186: 0.04107041284441948\n",
            "Acc:  0.9375\n",
            "Loss at step 187: 0.0408262275159359\n",
            "Acc:  0.9375\n",
            "Loss at step 188: 0.04058488830924034\n",
            "Acc:  0.9375\n",
            "Loss at step 189: 0.04034637287259102\n",
            "Acc:  0.9375\n",
            "Loss at step 190: 0.04011067748069763\n",
            "Acc:  0.9375\n",
            "Loss at step 191: 0.039877645671367645\n",
            "Acc:  0.9375\n",
            "Loss at step 192: 0.03964730724692345\n",
            "Acc:  0.9375\n",
            "Loss at step 193: 0.03941952437162399\n",
            "Acc:  0.9375\n",
            "Loss at step 194: 0.03919442743062973\n",
            "Acc:  0.9375\n",
            "Loss at step 195: 0.038971755653619766\n",
            "Acc:  0.9375\n",
            "Loss at step 196: 0.038751665502786636\n",
            "Acc:  0.9375\n",
            "Loss at step 197: 0.038533978164196014\n",
            "Acc:  0.9375\n",
            "Loss at step 198: 0.0383187010884285\n",
            "Acc:  0.9375\n",
            "Loss at step 199: 0.03810583055019379\n",
            "Acc:  0.9375\n",
            "Loss at step 200: 0.037895288318395615\n",
            "Acc:  0.9375\n",
            "Loss at step 201: 0.037687018513679504\n",
            "Acc:  0.9375\n",
            "Loss at step 202: 0.037480998784303665\n",
            "Acc:  0.9375\n",
            "Loss at step 203: 0.0372772179543972\n",
            "Acc:  0.9375\n",
            "Loss at step 204: 0.03707567974925041\n",
            "Acc:  0.9375\n",
            "Loss at step 205: 0.03687623143196106\n",
            "Acc:  0.9375\n",
            "Loss at step 206: 0.036678917706012726\n",
            "Acc:  0.9375\n",
            "Loss at step 207: 0.03648361936211586\n",
            "Acc:  0.9375\n",
            "Loss at step 208: 0.03629047051072121\n",
            "Acc:  0.9375\n",
            "Loss at step 209: 0.03609931841492653\n",
            "Acc:  0.9375\n",
            "Loss at step 210: 0.03591012582182884\n",
            "Acc:  0.9375\n",
            "Loss at step 211: 0.03572290763258934\n",
            "Acc:  0.9375\n",
            "Loss at step 212: 0.03553764149546623\n",
            "Acc:  0.9375\n",
            "Loss at step 213: 0.03535420820116997\n",
            "Acc:  0.9375\n",
            "Loss at step 214: 0.0351727157831192\n",
            "Acc:  0.9375\n",
            "Loss at step 215: 0.03499303013086319\n",
            "Acc:  0.9375\n",
            "Loss at step 216: 0.03481518477201462\n",
            "Acc:  0.9375\n",
            "Loss at step 217: 0.03463909029960632\n",
            "Acc:  0.9375\n",
            "Loss at step 218: 0.0344647616147995\n",
            "Acc:  0.9375\n",
            "Loss at step 219: 0.03429219499230385\n",
            "Acc:  0.9375\n",
            "Loss at step 220: 0.034121301025152206\n",
            "Acc:  0.9375\n",
            "Loss at step 221: 0.03395214304327965\n",
            "Acc:  0.9375\n",
            "Loss at step 222: 0.03378460183739662\n",
            "Acc:  0.9375\n",
            "Loss at step 223: 0.03361869975924492\n",
            "Acc:  0.9375\n",
            "Loss at step 224: 0.03345441073179245\n",
            "Acc:  0.9375\n",
            "Loss at step 225: 0.03329171612858772\n",
            "Acc:  0.9375\n",
            "Loss at step 226: 0.03313059359788895\n",
            "Acc:  0.9375\n",
            "Loss at step 227: 0.03297099098563194\n",
            "Acc:  0.9375\n",
            "Loss at step 228: 0.03281291201710701\n",
            "Acc:  0.9375\n",
            "Loss at step 229: 0.032656341791152954\n",
            "Acc:  0.9375\n",
            "Loss at step 230: 0.03250127658247948\n",
            "Acc:  0.9375\n",
            "Loss at step 231: 0.03234763815999031\n",
            "Acc:  0.9375\n",
            "Loss at step 232: 0.03219545632600784\n",
            "Acc:  0.9375\n",
            "Loss at step 233: 0.0320446714758873\n",
            "Acc:  0.9375\n",
            "Loss at step 234: 0.03189529478549957\n",
            "Acc:  0.9375\n",
            "Loss at step 235: 0.03174725919961929\n",
            "Acc:  0.9375\n",
            "Loss at step 236: 0.03160065412521362\n",
            "Acc:  0.9375\n",
            "Loss at step 237: 0.03145528957247734\n",
            "Acc:  0.9375\n",
            "Loss at step 238: 0.03131137788295746\n",
            "Acc:  0.9375\n",
            "Loss at step 239: 0.03116866946220398\n",
            "Acc:  0.9375\n",
            "Loss at step 240: 0.031027313321828842\n",
            "Acc:  0.9375\n",
            "Loss at step 241: 0.030887220054864883\n",
            "Acc:  0.9375\n",
            "Loss at step 242: 0.03074834495782852\n",
            "Acc:  0.9375\n",
            "Loss at step 243: 0.030610715970396996\n",
            "Acc:  0.9375\n",
            "Loss at step 244: 0.030474290251731873\n",
            "Acc:  0.9375\n",
            "Loss at step 245: 0.03033911995589733\n",
            "Acc:  0.9375\n",
            "Loss at step 246: 0.03020511008799076\n",
            "Acc:  0.9375\n",
            "Loss at step 247: 0.030072268098592758\n",
            "Acc:  0.9375\n",
            "Loss at step 248: 0.02994059957563877\n",
            "Acc:  0.9375\n",
            "Loss at step 249: 0.02981005795300007\n",
            "Acc:  0.9375\n",
            "Loss at step 250: 0.029680613428354263\n",
            "Acc:  0.9375\n",
            "Loss at step 251: 0.02955234795808792\n",
            "Acc:  0.9375\n",
            "Loss at step 252: 0.029425127431750298\n",
            "Acc:  0.9375\n",
            "Loss at step 253: 0.029299059882760048\n",
            "Acc:  0.9375\n",
            "Loss at step 254: 0.029173992574214935\n",
            "Acc:  0.9375\n",
            "Loss at step 255: 0.02904999628663063\n",
            "Acc:  0.9375\n",
            "Loss at step 256: 0.02892708219587803\n",
            "Acc:  0.9375\n",
            "Loss at step 257: 0.028805147856473923\n",
            "Acc:  0.9375\n",
            "Loss at step 258: 0.02868429571390152\n",
            "Acc:  0.9375\n",
            "Loss at step 259: 0.02856440655887127\n",
            "Acc:  0.9375\n",
            "Loss at step 260: 0.02844548039138317\n",
            "Acc:  0.9375\n",
            "Loss at step 261: 0.028327621519565582\n",
            "Acc:  0.9375\n",
            "Loss at step 262: 0.028210708871483803\n",
            "Acc:  0.9375\n",
            "Loss at step 263: 0.028094692155718803\n",
            "Acc:  0.9375\n",
            "Loss at step 264: 0.027979662641882896\n",
            "Acc:  0.9375\n",
            "Loss at step 265: 0.027865570038557053\n",
            "Acc:  0.9375\n",
            "Loss at step 266: 0.027752384543418884\n",
            "Acc:  0.9375\n",
            "Loss at step 267: 0.027640139684081078\n",
            "Acc:  0.9375\n",
            "Loss at step 268: 0.027528785169124603\n",
            "Acc:  0.9375\n",
            "Loss at step 269: 0.027418293058872223\n",
            "Acc:  0.9375\n",
            "Loss at step 270: 0.0273087490350008\n",
            "Acc:  0.9375\n",
            "Loss at step 271: 0.027199989184737206\n",
            "Acc:  0.9375\n",
            "Loss at step 272: 0.027092140167951584\n",
            "Acc:  0.9375\n",
            "Loss at step 273: 0.02698511630296707\n",
            "Acc:  0.9375\n",
            "Loss at step 274: 0.026878904551267624\n",
            "Acc:  0.9375\n",
            "Loss at step 275: 0.026773568242788315\n",
            "Acc:  0.9375\n",
            "Loss at step 276: 0.026669077575206757\n",
            "Acc:  0.9375\n",
            "Loss at step 277: 0.02656535804271698\n",
            "Acc:  0.9375\n",
            "Loss at step 278: 0.026462430134415627\n",
            "Acc:  0.9375\n",
            "Loss at step 279: 0.02636032924056053\n",
            "Acc:  0.9375\n",
            "Loss at step 280: 0.026258954778313637\n",
            "Acc:  0.9375\n",
            "Loss at step 281: 0.026158392429351807\n",
            "Acc:  0.9375\n",
            "Loss at step 282: 0.026058590039610863\n",
            "Acc:  0.9375\n",
            "Loss at step 283: 0.02595953457057476\n",
            "Acc:  0.9375\n",
            "Loss at step 284: 0.025861259549856186\n",
            "Acc:  0.9375\n",
            "Loss at step 285: 0.025763679295778275\n",
            "Acc:  0.9375\n",
            "Loss at step 286: 0.025666866451501846\n",
            "Acc:  0.9375\n",
            "Loss at step 287: 0.02557077817618847\n",
            "Acc:  0.9375\n",
            "Loss at step 288: 0.02547539956867695\n",
            "Acc:  0.9375\n",
            "Loss at step 289: 0.025380712002515793\n",
            "Acc:  0.9375\n",
            "Loss at step 290: 0.025286728516221046\n",
            "Acc:  0.9375\n",
            "Loss at step 291: 0.025193432345986366\n",
            "Acc:  0.9375\n",
            "Loss at step 292: 0.025100845843553543\n",
            "Acc:  0.9375\n",
            "Loss at step 293: 0.02500886656343937\n",
            "Acc:  0.9375\n",
            "Loss at step 294: 0.024917636066675186\n",
            "Acc:  0.9375\n",
            "Loss at step 295: 0.024827031418681145\n",
            "Acc:  0.9375\n",
            "Loss at step 296: 0.024737073108553886\n",
            "Acc:  0.9375\n",
            "Loss at step 297: 0.02464776486158371\n",
            "Acc:  0.9375\n",
            "Loss at step 298: 0.024559171870350838\n",
            "Acc:  0.9375\n",
            "Loss at step 299: 0.02447107993066311\n",
            "Acc:  0.9375\n",
            "Loss at step 300: 0.02438371255993843\n",
            "Acc:  0.9375\n",
            "Loss at step 301: 0.024296937510371208\n",
            "Acc:  0.9375\n",
            "Loss at step 302: 0.024210764095187187\n",
            "Acc:  0.9375\n",
            "Loss at step 303: 0.024125222116708755\n",
            "Acc:  0.9375\n",
            "Loss at step 304: 0.02404027059674263\n",
            "Acc:  0.9375\n",
            "Loss at step 305: 0.023955900222063065\n",
            "Acc:  0.9375\n",
            "Loss at step 306: 0.023872151970863342\n",
            "Acc:  0.9375\n",
            "Loss at step 307: 0.02378895878791809\n",
            "Acc:  0.9375\n",
            "Loss at step 308: 0.023706350475549698\n",
            "Acc:  0.9375\n",
            "Loss at step 309: 0.023624293506145477\n",
            "Acc:  0.9375\n",
            "Loss at step 310: 0.023542817682027817\n",
            "Acc:  0.9375\n",
            "Loss at step 311: 0.02346191368997097\n",
            "Acc:  0.9375\n",
            "Loss at step 312: 0.023381536826491356\n",
            "Acc:  0.9375\n",
            "Loss at step 313: 0.023301705718040466\n",
            "Acc:  0.9375\n",
            "Loss at step 314: 0.02322245016694069\n",
            "Acc:  0.9375\n",
            "Loss at step 315: 0.023143699392676353\n",
            "Acc:  0.9375\n",
            "Loss at step 316: 0.023065494373440742\n",
            "Acc:  0.9375\n",
            "Loss at step 317: 0.022987807169556618\n",
            "Acc:  0.9375\n",
            "Loss at step 318: 0.022910647094249725\n",
            "Acc:  0.9375\n",
            "Loss at step 319: 0.022833989933133125\n",
            "Acc:  0.9375\n",
            "Loss at step 320: 0.022757891565561295\n",
            "Acc:  0.9375\n",
            "Loss at step 321: 0.02268221788108349\n",
            "Acc:  0.9375\n",
            "Loss at step 322: 0.02260708063840866\n",
            "Acc:  0.9375\n",
            "Loss at step 323: 0.022532442584633827\n",
            "Acc:  0.9375\n",
            "Loss at step 324: 0.02245834469795227\n",
            "Acc:  0.9375\n",
            "Loss at step 325: 0.0223846398293972\n",
            "Acc:  0.9375\n",
            "Loss at step 326: 0.02231147699058056\n",
            "Acc:  0.9375\n",
            "Loss at step 327: 0.02223876677453518\n",
            "Acc:  0.9375\n",
            "Loss at step 328: 0.022166553884744644\n",
            "Acc:  0.9375\n",
            "Loss at step 329: 0.022094763815402985\n",
            "Acc:  0.9375\n",
            "Loss at step 330: 0.022023489698767662\n",
            "Acc:  0.9375\n",
            "Loss at step 331: 0.02195264957845211\n",
            "Acc:  0.9375\n",
            "Loss at step 332: 0.021882222965359688\n",
            "Acc:  0.9375\n",
            "Loss at step 333: 0.021812312304973602\n",
            "Acc:  0.9375\n",
            "Loss at step 334: 0.02174280770123005\n",
            "Acc:  0.9375\n",
            "Loss at step 335: 0.021673763170838356\n",
            "Acc:  0.9375\n",
            "Loss at step 336: 0.02160515822470188\n",
            "Acc:  0.9375\n",
            "Loss at step 337: 0.021536972373723984\n",
            "Acc:  0.9375\n",
            "Loss at step 338: 0.02146921493113041\n",
            "Acc:  0.9375\n",
            "Loss at step 339: 0.021401844918727875\n",
            "Acc:  0.9375\n",
            "Loss at step 340: 0.0213349387049675\n",
            "Acc:  0.9375\n",
            "Loss at step 341: 0.021268440410494804\n",
            "Acc:  0.9375\n",
            "Loss at step 342: 0.021202361211180687\n",
            "Acc:  0.9375\n",
            "Loss at step 343: 0.021136661991477013\n",
            "Acc:  0.9375\n",
            "Loss at step 344: 0.021071428433060646\n",
            "Acc:  0.9375\n",
            "Loss at step 345: 0.02100653573870659\n",
            "Acc:  0.9375\n",
            "Loss at step 346: 0.020942075178027153\n",
            "Acc:  0.9375\n",
            "Loss at step 347: 0.020878007635474205\n",
            "Acc:  0.9375\n",
            "Loss at step 348: 0.02081427350640297\n",
            "Acc:  0.9375\n",
            "Loss at step 349: 0.020751025527715683\n",
            "Acc:  0.9375\n",
            "Loss at step 350: 0.02068810909986496\n",
            "Acc:  0.9375\n",
            "Loss at step 351: 0.02062559500336647\n",
            "Acc:  0.9375\n",
            "Loss at step 352: 0.020563384518027306\n",
            "Acc:  0.9375\n",
            "Loss at step 353: 0.02050160989165306\n",
            "Acc:  0.9375\n",
            "Loss at step 354: 0.02044018730521202\n",
            "Acc:  0.9375\n",
            "Loss at step 355: 0.020379142835736275\n",
            "Acc:  0.9375\n",
            "Loss at step 356: 0.020318444818258286\n",
            "Acc:  0.9375\n",
            "Loss at step 357: 0.020258160308003426\n",
            "Acc:  0.9375\n",
            "Loss at step 358: 0.020198162645101547\n",
            "Acc:  0.9375\n",
            "Loss at step 359: 0.020138539373874664\n",
            "Acc:  0.9375\n",
            "Loss at step 360: 0.02007927931845188\n",
            "Acc:  0.9375\n",
            "Loss at step 361: 0.02002035640180111\n",
            "Acc:  0.9375\n",
            "Loss at step 362: 0.01996176689863205\n",
            "Acc:  0.9375\n",
            "Loss at step 363: 0.019903529435396194\n",
            "Acc:  0.9375\n",
            "Loss at step 364: 0.01984564960002899\n",
            "Acc:  0.9375\n",
            "Loss at step 365: 0.019788088276982307\n",
            "Acc:  0.9375\n",
            "Loss at step 366: 0.019730854779481888\n",
            "Acc:  0.9375\n",
            "Loss at step 367: 0.01967395283281803\n",
            "Acc:  0.9375\n",
            "Loss at step 368: 0.01961737871170044\n",
            "Acc:  0.9375\n",
            "Loss at step 369: 0.019561143592000008\n",
            "Acc:  0.9375\n",
            "Loss at step 370: 0.019505243748426437\n",
            "Acc:  0.9375\n",
            "Loss at step 371: 0.019449619576334953\n",
            "Acc:  0.9375\n",
            "Loss at step 372: 0.01939433068037033\n",
            "Acc:  0.9375\n",
            "Loss at step 373: 0.019339310005307198\n",
            "Acc:  0.9375\n",
            "Loss at step 374: 0.01928471215069294\n",
            "Acc:  0.9375\n",
            "Loss at step 375: 0.019230322912335396\n",
            "Acc:  0.9375\n",
            "Loss at step 376: 0.019176285713911057\n",
            "Acc:  0.9375\n",
            "Loss at step 377: 0.019122526049613953\n",
            "Acc:  0.9375\n",
            "Loss at step 378: 0.019069107249379158\n",
            "Acc:  0.9375\n",
            "Loss at step 379: 0.019015902653336525\n",
            "Acc:  0.9375\n",
            "Loss at step 380: 0.01896306499838829\n",
            "Acc:  0.9375\n",
            "Loss at step 381: 0.01891045644879341\n",
            "Acc:  0.9375\n",
            "Loss at step 382: 0.01885824464261532\n",
            "Acc:  0.9375\n",
            "Loss at step 383: 0.01880624145269394\n",
            "Acc:  0.9375\n",
            "Loss at step 384: 0.01875453069806099\n",
            "Acc:  0.9375\n",
            "Loss at step 385: 0.018703125417232513\n",
            "Acc:  0.9375\n",
            "Loss at step 386: 0.01865200325846672\n",
            "Acc:  0.9375\n",
            "Loss at step 387: 0.018601126968860626\n",
            "Acc:  0.9375\n",
            "Loss at step 388: 0.018550531938672066\n",
            "Acc:  0.9375\n",
            "Loss at step 389: 0.01850023865699768\n",
            "Acc:  0.9375\n",
            "Loss at step 390: 0.018450213596224785\n",
            "Acc:  0.9375\n",
            "Loss at step 391: 0.01840045489370823\n",
            "Acc:  0.9375\n",
            "Loss at step 392: 0.018350988626480103\n",
            "Acc:  0.9375\n",
            "Loss at step 393: 0.018301736563444138\n",
            "Acc:  0.9375\n",
            "Loss at step 394: 0.018252769485116005\n",
            "Acc:  0.9375\n",
            "Loss at step 395: 0.018204057589173317\n",
            "Acc:  0.9375\n",
            "Loss at step 396: 0.018155638128519058\n",
            "Acc:  0.9375\n",
            "Loss at step 397: 0.018107421696186066\n",
            "Acc:  0.9375\n",
            "Loss at step 398: 0.01805947534739971\n",
            "Acc:  0.9375\n",
            "Loss at step 399: 0.018011782318353653\n",
            "Acc:  0.9375\n",
            "Loss at step 400: 0.017964398488402367\n",
            "Acc:  0.9375\n",
            "Loss at step 401: 0.01791716367006302\n",
            "Acc:  0.9375\n",
            "Loss at step 402: 0.01787027157843113\n",
            "Acc:  0.9375\n",
            "Loss at step 403: 0.017823558300733566\n",
            "Acc:  0.9375\n",
            "Loss at step 404: 0.01777711883187294\n",
            "Acc:  0.9375\n",
            "Loss at step 405: 0.017730921506881714\n",
            "Acc:  0.9375\n",
            "Loss at step 406: 0.017684951424598694\n",
            "Acc:  0.9375\n",
            "Loss at step 407: 0.01763923652470112\n",
            "Acc:  0.9375\n",
            "Loss at step 408: 0.01759372092783451\n",
            "Acc:  0.9375\n",
            "Loss at step 409: 0.0175484512001276\n",
            "Acc:  0.9375\n",
            "Loss at step 410: 0.01750345341861248\n",
            "Acc:  0.9375\n",
            "Loss at step 411: 0.017458660528063774\n",
            "Acc:  0.9375\n",
            "Loss at step 412: 0.017414117231965065\n",
            "Acc:  0.9375\n",
            "Loss at step 413: 0.017369722947478294\n",
            "Acc:  0.9375\n",
            "Loss at step 414: 0.017325647175312042\n",
            "Acc:  0.9375\n",
            "Loss at step 415: 0.01728176325559616\n",
            "Acc:  0.9375\n",
            "Loss at step 416: 0.01723807118833065\n",
            "Acc:  0.9375\n",
            "Loss at step 417: 0.01719464734196663\n",
            "Acc:  0.9375\n",
            "Loss at step 418: 0.017151374369859695\n",
            "Acc:  0.9375\n",
            "Loss at step 419: 0.017108360305428505\n",
            "Acc:  0.9375\n",
            "Loss at step 420: 0.017065592110157013\n",
            "Acc:  0.9375\n",
            "Loss at step 421: 0.017023026943206787\n",
            "Acc:  0.9375\n",
            "Loss at step 422: 0.016980603337287903\n",
            "Acc:  0.9375\n",
            "Loss at step 423: 0.016938453540205956\n",
            "Acc:  0.9375\n",
            "Loss at step 424: 0.016896503046154976\n",
            "Acc:  0.9375\n",
            "Loss at step 425: 0.016854777932167053\n",
            "Acc:  0.9375\n",
            "Loss at step 426: 0.01681322231888771\n",
            "Acc:  0.9375\n",
            "Loss at step 427: 0.016771884635090828\n",
            "Acc:  0.9375\n",
            "Loss at step 428: 0.016730787232518196\n",
            "Acc:  0.9375\n",
            "Loss at step 429: 0.016689825803041458\n",
            "Acc:  0.9375\n",
            "Loss at step 430: 0.01664910651743412\n",
            "Acc:  0.9375\n",
            "Loss at step 431: 0.01660856418311596\n",
            "Acc:  0.9375\n",
            "Loss at step 432: 0.01656821370124817\n",
            "Acc:  0.9375\n",
            "Loss at step 433: 0.016528096050024033\n",
            "Acc:  0.9375\n",
            "Loss at step 434: 0.01648814044892788\n",
            "Acc:  0.9375\n",
            "Loss at step 435: 0.0164483692497015\n",
            "Acc:  0.9375\n",
            "Loss at step 436: 0.016408797353506088\n",
            "Acc:  0.9375\n",
            "Loss at step 437: 0.01636941358447075\n",
            "Acc:  0.9375\n",
            "Loss at step 438: 0.01633027009665966\n",
            "Acc:  0.9375\n",
            "Loss at step 439: 0.016291271895170212\n",
            "Acc:  0.9375\n",
            "Loss at step 440: 0.016252456232905388\n",
            "Acc:  0.9375\n",
            "Loss at step 441: 0.016213849186897278\n",
            "Acc:  0.9375\n",
            "Loss at step 442: 0.016175387427210808\n",
            "Acc:  0.9375\n",
            "Loss at step 443: 0.01613713428378105\n",
            "Acc:  0.9375\n",
            "Loss at step 444: 0.016099048778414726\n",
            "Acc:  0.9375\n",
            "Loss at step 445: 0.01606113463640213\n",
            "Acc:  0.9375\n",
            "Loss at step 446: 0.016023453325033188\n",
            "Acc:  0.9375\n",
            "Loss at step 447: 0.015985896810889244\n",
            "Acc:  0.9375\n",
            "Loss at step 448: 0.015948504209518433\n",
            "Acc:  0.9375\n",
            "Loss at step 449: 0.01591131091117859\n",
            "Acc:  0.9375\n",
            "Loss at step 450: 0.01587429642677307\n",
            "Acc:  0.9375\n",
            "Loss at step 451: 0.015837421640753746\n",
            "Acc:  0.9375\n",
            "Loss at step 452: 0.015800755470991135\n",
            "Acc:  0.9375\n",
            "Loss at step 453: 0.015764256939291954\n",
            "Acc:  0.9375\n",
            "Loss at step 454: 0.015727883204817772\n",
            "Acc:  0.9375\n",
            "Loss at step 455: 0.015691740438342094\n",
            "Acc:  0.9375\n",
            "Loss at step 456: 0.015655741095542908\n",
            "Acc:  0.9375\n",
            "Loss at step 457: 0.015619868412613869\n",
            "Acc:  0.9375\n",
            "Loss at step 458: 0.015584179200232029\n",
            "Acc:  0.9375\n",
            "Loss at step 459: 0.015548693016171455\n",
            "Acc:  0.9375\n",
            "Loss at step 460: 0.015513330698013306\n",
            "Acc:  0.9375\n",
            "Loss at step 461: 0.015478143468499184\n",
            "Acc:  0.9375\n",
            "Loss at step 462: 0.015443065203726292\n",
            "Acc:  0.9375\n",
            "Loss at step 463: 0.015408211387693882\n",
            "Acc:  0.9375\n",
            "Loss at step 464: 0.015373509377241135\n",
            "Acc:  0.9375\n",
            "Loss at step 465: 0.015338890254497528\n",
            "Acc:  0.9375\n",
            "Loss at step 466: 0.015304520726203918\n",
            "Acc:  0.9375\n",
            "Loss at step 467: 0.015270298346877098\n",
            "Acc:  0.9375\n",
            "Loss at step 468: 0.015236156061291695\n",
            "Acc:  0.9375\n",
            "Loss at step 469: 0.015202220529317856\n",
            "Acc:  0.9375\n",
            "Loss at step 470: 0.015168444253504276\n",
            "Acc:  0.9375\n",
            "Loss at step 471: 0.015134740620851517\n",
            "Acc:  0.9375\n",
            "Loss at step 472: 0.015101264230906963\n",
            "Acc:  0.9375\n",
            "Loss at step 473: 0.015067915432155132\n",
            "Acc:  0.9375\n",
            "Loss at step 474: 0.01503470353782177\n",
            "Acc:  0.9375\n",
            "Loss at step 475: 0.015001688152551651\n",
            "Acc:  0.9375\n",
            "Loss at step 476: 0.01496876124292612\n",
            "Acc:  0.9375\n",
            "Loss at step 477: 0.014935963787138462\n",
            "Acc:  0.9375\n",
            "Loss at step 478: 0.014903364703059196\n",
            "Acc:  0.9375\n",
            "Loss at step 479: 0.014870863407850266\n",
            "Acc:  0.9375\n",
            "Loss at step 480: 0.014838553965091705\n",
            "Acc:  0.9375\n",
            "Loss at step 481: 0.014806325547397137\n",
            "Acc:  0.9375\n",
            "Loss at step 482: 0.014774252660572529\n",
            "Acc:  0.9375\n",
            "Loss at step 483: 0.014742342755198479\n",
            "Acc:  0.9375\n",
            "Loss at step 484: 0.014710551127791405\n",
            "Acc:  0.9375\n",
            "Loss at step 485: 0.014678899198770523\n",
            "Acc:  0.9375\n",
            "Loss at step 486: 0.014647402800619602\n",
            "Acc:  0.9375\n",
            "Loss at step 487: 0.014616035856306553\n",
            "Acc:  0.9375\n",
            "Loss at step 488: 0.014584782533347607\n",
            "Acc:  0.9375\n",
            "Loss at step 489: 0.014553669840097427\n",
            "Acc:  0.9375\n",
            "Loss at step 490: 0.01452271081507206\n",
            "Acc:  0.9375\n",
            "Loss at step 491: 0.014491822570562363\n",
            "Acc:  0.9375\n",
            "Loss at step 492: 0.014461109414696693\n",
            "Acc:  0.9375\n",
            "Loss at step 493: 0.01443056482821703\n",
            "Acc:  0.9375\n",
            "Loss at step 494: 0.014400068670511246\n",
            "Acc:  0.9375\n",
            "Loss at step 495: 0.014369742013514042\n",
            "Acc:  0.9375\n",
            "Loss at step 496: 0.014339564368128777\n",
            "Acc:  0.9375\n",
            "Loss at step 497: 0.014309488236904144\n",
            "Acc:  0.9375\n",
            "Loss at step 498: 0.014279542490839958\n",
            "Acc:  0.9375\n",
            "Loss at step 499: 0.014249737374484539\n",
            "Acc:  0.9375\n",
            "Loss at step 500: 0.014220038428902626\n",
            "Acc:  0.9375\n",
            "Loss at step 501: 0.01419046800583601\n",
            "Acc:  0.9375\n",
            "Loss at step 502: 0.01416100561618805\n",
            "Acc:  0.9375\n",
            "Loss at step 503: 0.014131613075733185\n",
            "Acc:  0.9375\n",
            "Loss at step 504: 0.014102490618824959\n",
            "Acc:  0.9375\n",
            "Loss at step 505: 0.014073400758206844\n",
            "Acc:  0.9375\n",
            "Loss at step 506: 0.014044391922652721\n",
            "Acc:  0.9375\n",
            "Loss at step 507: 0.014015604741871357\n",
            "Acc:  0.9375\n",
            "Loss at step 508: 0.01398683711886406\n",
            "Acc:  0.9375\n",
            "Loss at step 509: 0.0139582185074687\n",
            "Acc:  0.9375\n",
            "Loss at step 510: 0.013929737731814384\n",
            "Acc:  0.9375\n",
            "Loss at step 511: 0.013901387341320515\n",
            "Acc:  0.9375\n",
            "Loss at step 512: 0.013873127289116383\n",
            "Acc:  0.9375\n",
            "Loss at step 513: 0.013844966888427734\n",
            "Acc:  0.9375\n",
            "Loss at step 514: 0.01381690427660942\n",
            "Acc:  0.9375\n",
            "Loss at step 515: 0.013789026997983456\n",
            "Acc:  0.9375\n",
            "Loss at step 516: 0.013761180453002453\n",
            "Acc:  0.9375\n",
            "Loss at step 517: 0.013733508065342903\n",
            "Acc:  0.9375\n",
            "Loss at step 518: 0.013705874793231487\n",
            "Acc:  0.9375\n",
            "Loss at step 519: 0.013678466901183128\n",
            "Acc:  0.9375\n",
            "Loss at step 520: 0.013651032000780106\n",
            "Acc:  0.9375\n",
            "Loss at step 521: 0.01362381037324667\n",
            "Acc:  0.9375\n",
            "Loss at step 522: 0.013596721924841404\n",
            "Acc:  0.9375\n",
            "Loss at step 523: 0.013569668866693974\n",
            "Acc:  0.9375\n",
            "Loss at step 524: 0.013542696833610535\n",
            "Acc:  0.9375\n",
            "Loss at step 525: 0.013515905477106571\n",
            "Acc:  0.9375\n",
            "Loss at step 526: 0.013489189557731152\n",
            "Acc:  0.9375\n",
            "Loss at step 527: 0.013462577015161514\n",
            "Acc:  0.9375\n",
            "Loss at step 528: 0.013436052948236465\n",
            "Acc:  0.9375\n",
            "Loss at step 529: 0.013409636914730072\n",
            "Acc:  0.9375\n",
            "Loss at step 530: 0.013383368030190468\n",
            "Acc:  0.9375\n",
            "Loss at step 531: 0.013357153162360191\n",
            "Acc:  0.9375\n",
            "Loss at step 532: 0.013331052847206593\n",
            "Acc:  0.9375\n",
            "Loss at step 533: 0.01330510713160038\n",
            "Acc:  0.9375\n",
            "Loss at step 534: 0.01327919214963913\n",
            "Acc:  0.9375\n",
            "Loss at step 535: 0.01325339823961258\n",
            "Acc:  0.9375\n",
            "Loss at step 536: 0.013227736577391624\n",
            "Acc:  0.9375\n",
            "Loss at step 537: 0.013202131725847721\n",
            "Acc:  0.9375\n",
            "Loss at step 538: 0.013176629319787025\n",
            "Acc:  0.9375\n",
            "Loss at step 539: 0.01315127220004797\n",
            "Acc:  0.9375\n",
            "Loss at step 540: 0.013125945813953876\n",
            "Acc:  0.9375\n",
            "Loss at step 541: 0.013100779615342617\n",
            "Acc:  0.9375\n",
            "Loss at step 542: 0.013075658120214939\n",
            "Acc:  0.9375\n",
            "Loss at step 543: 0.013050650246441364\n",
            "Acc:  0.9375\n",
            "Loss at step 544: 0.013025722466409206\n",
            "Acc:  0.9375\n",
            "Loss at step 545: 0.013000911101698875\n",
            "Acc:  0.9375\n",
            "Loss at step 546: 0.012976202182471752\n",
            "Acc:  0.9375\n",
            "Loss at step 547: 0.012951573356986046\n",
            "Acc:  0.9375\n",
            "Loss at step 548: 0.0129270413890481\n",
            "Acc:  0.9375\n",
            "Loss at step 549: 0.012902600690722466\n",
            "Acc:  0.9375\n",
            "Loss at step 550: 0.012878281995654106\n",
            "Acc:  0.9375\n",
            "Loss at step 551: 0.012854021973907948\n",
            "Acc:  0.9375\n",
            "Loss at step 552: 0.01282986719161272\n",
            "Acc:  0.9375\n",
            "Loss at step 553: 0.012805781327188015\n",
            "Acc:  0.9375\n",
            "Loss at step 554: 0.012781801633536816\n",
            "Acc:  0.9375\n",
            "Loss at step 555: 0.012757937423884869\n",
            "Acc:  0.9375\n",
            "Loss at step 556: 0.012734134681522846\n",
            "Acc:  0.9375\n",
            "Loss at step 557: 0.012710390612483025\n",
            "Acc:  0.9375\n",
            "Loss at step 558: 0.012686771340668201\n",
            "Acc:  0.9375\n",
            "Loss at step 559: 0.012663215398788452\n",
            "Acc:  0.9375\n",
            "Loss at step 560: 0.012639783322811127\n",
            "Acc:  0.9375\n",
            "Loss at step 561: 0.012616424821317196\n",
            "Acc:  0.9375\n",
            "Loss at step 562: 0.012593120336532593\n",
            "Acc:  0.9375\n",
            "Loss at step 563: 0.0125699732452631\n",
            "Acc:  0.9375\n",
            "Loss at step 564: 0.012546803802251816\n",
            "Acc:  0.9375\n",
            "Loss at step 565: 0.012523818761110306\n",
            "Acc:  0.9375\n",
            "Loss at step 566: 0.012500893324613571\n",
            "Acc:  0.9375\n",
            "Loss at step 567: 0.012478044256567955\n",
            "Acc:  0.9375\n",
            "Loss at step 568: 0.012455275282263756\n",
            "Acc:  0.9375\n",
            "Loss at step 569: 0.012432577088475227\n",
            "Acc:  0.9375\n",
            "Loss at step 570: 0.012409955263137817\n",
            "Acc:  0.9375\n",
            "Loss at step 571: 0.012387451715767384\n",
            "Acc:  0.9375\n",
            "Loss at step 572: 0.012365026399493217\n",
            "Acc:  0.9375\n",
            "Loss at step 573: 0.012342621572315693\n",
            "Acc:  0.9375\n",
            "Loss at step 574: 0.012320341542363167\n",
            "Acc:  0.9375\n",
            "Loss at step 575: 0.01229819841682911\n",
            "Acc:  0.9375\n",
            "Loss at step 576: 0.012276079505681992\n",
            "Acc:  0.9375\n",
            "Loss at step 577: 0.012254003435373306\n",
            "Acc:  0.9375\n",
            "Loss at step 578: 0.01223207451403141\n",
            "Acc:  0.9375\n",
            "Loss at step 579: 0.01221019309014082\n",
            "Acc:  0.9375\n",
            "Loss at step 580: 0.012188397347927094\n",
            "Acc:  0.9375\n",
            "Loss at step 581: 0.012166672386229038\n",
            "Acc:  0.9375\n",
            "Loss at step 582: 0.012145005166530609\n",
            "Acc:  0.9375\n",
            "Loss at step 583: 0.01212344691157341\n",
            "Acc:  0.9375\n",
            "Loss at step 584: 0.012101957574486732\n",
            "Acc:  0.9375\n",
            "Loss at step 585: 0.012080562300980091\n",
            "Acc:  0.9375\n",
            "Loss at step 586: 0.012059234082698822\n",
            "Acc:  0.9375\n",
            "Loss at step 587: 0.012037955224514008\n",
            "Acc:  0.9375\n",
            "Loss at step 588: 0.012016764841973782\n",
            "Acc:  0.9375\n",
            "Loss at step 589: 0.011995641514658928\n",
            "Acc:  0.9375\n",
            "Loss at step 590: 0.011974608525633812\n",
            "Acc:  0.9375\n",
            "Loss at step 591: 0.011953621171414852\n",
            "Acc:  0.9375\n",
            "Loss at step 592: 0.01193271391093731\n",
            "Acc:  0.9375\n",
            "Loss at step 593: 0.011911921203136444\n",
            "Acc:  0.9375\n",
            "Loss at step 594: 0.01189110241830349\n",
            "Acc:  0.9375\n",
            "Loss at step 595: 0.011870458722114563\n",
            "Acc:  0.9375\n",
            "Loss at step 596: 0.011849855072796345\n",
            "Acc:  0.9375\n",
            "Loss at step 597: 0.011829349212348461\n",
            "Acc:  0.9375\n",
            "Loss at step 598: 0.011808875016868114\n",
            "Acc:  0.9375\n",
            "Loss at step 599: 0.011788499541580677\n",
            "Acc:  0.9375\n",
            "Loss at step 600: 0.011768161319196224\n",
            "Acc:  0.9375\n",
            "Loss at step 601: 0.01174790970981121\n",
            "Acc:  0.9375\n",
            "Loss at step 602: 0.011727719567716122\n",
            "Acc:  0.9375\n",
            "Loss at step 603: 0.01170763187110424\n",
            "Acc:  0.9375\n",
            "Loss at step 604: 0.011687605641782284\n",
            "Acc:  0.9375\n",
            "Loss at step 605: 0.01166761014610529\n",
            "Acc:  0.9375\n",
            "Loss at step 606: 0.011647720821201801\n",
            "Acc:  0.9375\n",
            "Loss at step 607: 0.011627878062427044\n",
            "Acc:  0.9375\n",
            "Loss at step 608: 0.011608118191361427\n",
            "Acc:  0.9375\n",
            "Loss at step 609: 0.011588389985263348\n",
            "Acc:  0.9375\n",
            "Loss at step 610: 0.011568761430680752\n",
            "Acc:  0.9375\n",
            "Loss at step 611: 0.011549192480742931\n",
            "Acc:  0.9375\n",
            "Loss at step 612: 0.011529672890901566\n",
            "Acc:  0.9375\n",
            "Loss at step 613: 0.011510251089930534\n",
            "Acc:  0.9375\n",
            "Loss at step 614: 0.011490877717733383\n",
            "Acc:  0.9375\n",
            "Loss at step 615: 0.011471557430922985\n",
            "Acc:  0.9375\n",
            "Loss at step 616: 0.011452320031821728\n",
            "Acc:  0.9375\n",
            "Loss at step 617: 0.011433154344558716\n",
            "Acc:  0.9375\n",
            "Loss at step 618: 0.011414050124585629\n",
            "Acc:  0.9375\n",
            "Loss at step 619: 0.011394974775612354\n",
            "Acc:  0.9375\n",
            "Loss at step 620: 0.011376009322702885\n",
            "Acc:  0.9375\n",
            "Loss at step 621: 0.011357077397406101\n",
            "Acc:  0.9375\n",
            "Loss at step 622: 0.01133820228278637\n",
            "Acc:  0.9375\n",
            "Loss at step 623: 0.011319415643811226\n",
            "Acc:  0.9375\n",
            "Loss at step 624: 0.011300669051706791\n",
            "Acc:  0.9375\n",
            "Loss at step 625: 0.011282027699053288\n",
            "Acc:  0.9375\n",
            "Loss at step 626: 0.011263422667980194\n",
            "Acc:  0.9375\n",
            "Loss at step 627: 0.01124490611255169\n",
            "Acc:  0.9375\n",
            "Loss at step 628: 0.011226372793316841\n",
            "Acc:  0.9375\n",
            "Loss at step 629: 0.01120798010379076\n",
            "Acc:  0.9375\n",
            "Loss at step 630: 0.011189587414264679\n",
            "Acc:  0.9375\n",
            "Loss at step 631: 0.011171302758157253\n",
            "Acc:  0.9375\n",
            "Loss at step 632: 0.011153021827340126\n",
            "Acc:  0.9375\n",
            "Loss at step 633: 0.011134816333651543\n",
            "Acc:  0.9375\n",
            "Loss at step 634: 0.01111669186502695\n",
            "Acc:  0.9375\n",
            "Loss at step 635: 0.011098666116595268\n",
            "Acc:  0.9375\n",
            "Loss at step 636: 0.011080664582550526\n",
            "Acc:  0.9375\n",
            "Loss at step 637: 0.011062722653150558\n",
            "Acc:  0.9375\n",
            "Loss at step 638: 0.011044804006814957\n",
            "Acc:  0.9375\n",
            "Loss at step 639: 0.011026952415704727\n",
            "Acc:  0.9375\n",
            "Loss at step 640: 0.011009196750819683\n",
            "Acc:  0.9375\n",
            "Loss at step 641: 0.010991493239998817\n",
            "Acc:  0.9375\n",
            "Loss at step 642: 0.01097377110272646\n",
            "Acc:  0.9375\n",
            "Loss at step 643: 0.010956230573356152\n",
            "Acc:  0.9375\n",
            "Loss at step 644: 0.010938627645373344\n",
            "Acc:  0.9375\n",
            "Loss at step 645: 0.010921146720647812\n",
            "Acc:  0.9375\n",
            "Loss at step 646: 0.010903671383857727\n",
            "Acc:  0.9375\n",
            "Loss at step 647: 0.010886321775615215\n",
            "Acc:  0.9375\n",
            "Loss at step 648: 0.010868992656469345\n",
            "Acc:  0.9375\n",
            "Loss at step 649: 0.01085173524916172\n",
            "Acc:  0.9375\n",
            "Loss at step 650: 0.010834496468305588\n",
            "Acc:  0.9375\n",
            "Loss at step 651: 0.010817356407642365\n",
            "Acc:  0.9375\n",
            "Loss at step 652: 0.01080025639384985\n",
            "Acc:  0.9375\n",
            "Loss at step 653: 0.010783195495605469\n",
            "Acc:  0.9375\n",
            "Loss at step 654: 0.010766175575554371\n",
            "Acc:  0.9375\n",
            "Loss at step 655: 0.010749240405857563\n",
            "Acc:  0.9375\n",
            "Loss at step 656: 0.010732412338256836\n",
            "Acc:  0.9375\n",
            "Loss at step 657: 0.0107155442237854\n",
            "Acc:  0.9375\n",
            "Loss at step 658: 0.010698728263378143\n",
            "Acc:  0.9375\n",
            "Loss at step 659: 0.010682029649615288\n",
            "Acc:  0.9375\n",
            "Loss at step 660: 0.010665355250239372\n",
            "Acc:  0.9375\n",
            "Loss at step 661: 0.010648681782186031\n",
            "Acc:  0.9375\n",
            "Loss at step 662: 0.010632093995809555\n",
            "Acc:  0.9375\n",
            "Loss at step 663: 0.01061556302011013\n",
            "Acc:  0.9375\n",
            "Loss at step 664: 0.010599134489893913\n",
            "Acc:  0.9375\n",
            "Loss at step 665: 0.010582688264548779\n",
            "Acc:  0.9375\n",
            "Loss at step 666: 0.010566315613687038\n",
            "Acc:  0.9375\n",
            "Loss at step 667: 0.010550020262598991\n",
            "Acc:  0.9375\n",
            "Loss at step 668: 0.01053372398018837\n",
            "Acc:  0.9375\n",
            "Loss at step 669: 0.010517526417970657\n",
            "Acc:  0.9375\n",
            "Loss at step 670: 0.010501330718398094\n",
            "Acc:  0.9375\n",
            "Loss at step 671: 0.010485229082405567\n",
            "Acc:  0.9375\n",
            "Loss at step 672: 0.01046913955360651\n",
            "Acc:  0.9375\n",
            "Loss at step 673: 0.010453158058226109\n",
            "Acc:  0.9375\n",
            "Loss at step 674: 0.010437136515974998\n",
            "Acc:  0.9375\n",
            "Loss at step 675: 0.010421249084174633\n",
            "Acc:  0.9375\n",
            "Loss at step 676: 0.01040536630898714\n",
            "Acc:  0.9375\n",
            "Loss at step 677: 0.010389525443315506\n",
            "Acc:  0.9375\n",
            "Loss at step 678: 0.01037377119064331\n",
            "Acc:  0.9375\n",
            "Loss at step 679: 0.01035802997648716\n",
            "Acc:  0.9375\n",
            "Loss at step 680: 0.010342339985072613\n",
            "Acc:  0.9375\n",
            "Loss at step 681: 0.010326719842851162\n",
            "Acc:  0.9375\n",
            "Loss at step 682: 0.010311109945178032\n",
            "Acc:  0.9375\n",
            "Loss at step 683: 0.010295562446117401\n",
            "Acc:  0.9375\n",
            "Loss at step 684: 0.010280085727572441\n",
            "Acc:  0.9375\n",
            "Loss at step 685: 0.01026463694870472\n",
            "Acc:  0.9375\n",
            "Loss at step 686: 0.01024923101067543\n",
            "Acc:  0.9375\n",
            "Loss at step 687: 0.010233885608613491\n",
            "Acc:  0.9375\n",
            "Loss at step 688: 0.010218583047389984\n",
            "Acc:  0.9375\n",
            "Loss at step 689: 0.0102032870054245\n",
            "Acc:  0.9375\n",
            "Loss at step 690: 0.010188094340264797\n",
            "Acc:  0.9375\n",
            "Loss at step 691: 0.01017287839204073\n",
            "Acc:  0.9375\n",
            "Loss at step 692: 0.010157792828977108\n",
            "Acc:  0.9375\n",
            "Loss at step 693: 0.01014269795268774\n",
            "Acc:  0.9375\n",
            "Loss at step 694: 0.010127670131623745\n",
            "Acc:  0.9375\n",
            "Loss at step 695: 0.010112665593624115\n",
            "Acc:  0.9375\n",
            "Loss at step 696: 0.010097691789269447\n",
            "Acc:  0.9375\n",
            "Loss at step 697: 0.010082827880978584\n",
            "Acc:  0.9375\n",
            "Loss at step 698: 0.010067930445075035\n",
            "Acc:  0.9375\n",
            "Loss at step 699: 0.010053125210106373\n",
            "Acc:  0.9375\n",
            "Loss at step 700: 0.010038351640105247\n",
            "Acc:  0.9375\n",
            "Loss at step 701: 0.010023655369877815\n",
            "Acc:  0.9375\n",
            "Loss at step 702: 0.0100089805200696\n",
            "Acc:  0.9375\n",
            "Loss at step 703: 0.009994328953325748\n",
            "Acc:  0.9375\n",
            "Loss at step 704: 0.009979712776839733\n",
            "Acc:  0.9375\n",
            "Loss at step 705: 0.009965138509869576\n",
            "Acc:  0.9375\n",
            "Loss at step 706: 0.009950641542673111\n",
            "Acc:  0.9375\n",
            "Loss at step 707: 0.009936220943927765\n",
            "Acc:  0.9375\n",
            "Loss at step 708: 0.009921765886247158\n",
            "Acc:  0.9375\n",
            "Loss at step 709: 0.009907332248985767\n",
            "Acc:  0.9375\n",
            "Loss at step 710: 0.009892994537949562\n",
            "Acc:  0.9375\n",
            "Loss at step 711: 0.00987868383526802\n",
            "Acc:  0.9375\n",
            "Loss at step 712: 0.009864436462521553\n",
            "Acc:  0.9375\n",
            "Loss at step 713: 0.009850210510194302\n",
            "Acc:  0.9375\n",
            "Loss at step 714: 0.009836035780608654\n",
            "Acc:  0.9375\n",
            "Loss at step 715: 0.009821888990700245\n",
            "Acc:  0.9375\n",
            "Loss at step 716: 0.009807777591049671\n",
            "Acc:  0.9375\n",
            "Loss at step 717: 0.009793746285140514\n",
            "Acc:  0.9375\n",
            "Loss at step 718: 0.00977973360568285\n",
            "Acc:  0.9375\n",
            "Loss at step 719: 0.009765753522515297\n",
            "Acc:  0.9375\n",
            "Loss at step 720: 0.009751814417541027\n",
            "Acc:  0.9375\n",
            "Loss at step 721: 0.009737890213727951\n",
            "Acc:  0.9375\n",
            "Loss at step 722: 0.009724024683237076\n",
            "Acc:  0.9375\n",
            "Loss at step 723: 0.009710210375487804\n",
            "Acc:  0.9375\n",
            "Loss at step 724: 0.00969642959535122\n",
            "Acc:  0.9375\n",
            "Loss at step 725: 0.009682734496891499\n",
            "Acc:  0.9375\n",
            "Loss at step 726: 0.009669025428593159\n",
            "Acc:  0.9375\n",
            "Loss at step 727: 0.009655322879552841\n",
            "Acc:  0.9375\n",
            "Loss at step 728: 0.00964169017970562\n",
            "Acc:  0.9375\n",
            "Loss at step 729: 0.009628159925341606\n",
            "Acc:  0.9375\n",
            "Loss at step 730: 0.009614549577236176\n",
            "Acc:  0.9375\n",
            "Loss at step 731: 0.009601056575775146\n",
            "Acc:  0.9375\n",
            "Loss at step 732: 0.009587610140442848\n",
            "Acc:  0.9375\n",
            "Loss at step 733: 0.009574179537594318\n",
            "Acc:  0.9375\n",
            "Loss at step 734: 0.009560739621520042\n",
            "Acc:  0.9375\n",
            "Loss at step 735: 0.009547416120767593\n",
            "Acc:  0.9375\n",
            "Loss at step 736: 0.009534088894724846\n",
            "Acc:  0.9375\n",
            "Loss at step 737: 0.009520797058939934\n",
            "Acc:  0.9375\n",
            "Loss at step 738: 0.009507574141025543\n",
            "Acc:  0.9375\n",
            "Loss at step 739: 0.009494377300143242\n",
            "Acc:  0.9375\n",
            "Loss at step 740: 0.00948116835206747\n",
            "Acc:  0.9375\n",
            "Loss at step 741: 0.009468037635087967\n",
            "Acc:  0.9375\n",
            "Loss at step 742: 0.00945492833852768\n",
            "Acc:  0.9375\n",
            "Loss at step 743: 0.009441884234547615\n",
            "Acc:  0.9375\n",
            "Loss at step 744: 0.009428855963051319\n",
            "Acc:  0.9375\n",
            "Loss at step 745: 0.009415841661393642\n",
            "Acc:  0.9375\n",
            "Loss at step 746: 0.009402889758348465\n",
            "Acc:  0.9375\n",
            "Loss at step 747: 0.009389981627464294\n",
            "Acc:  0.9375\n",
            "Loss at step 748: 0.009377101436257362\n",
            "Acc:  0.9375\n",
            "Loss at step 749: 0.009364264085888863\n",
            "Acc:  0.9375\n",
            "Loss at step 750: 0.009351417422294617\n",
            "Acc:  0.9375\n",
            "Loss at step 751: 0.009338644333183765\n",
            "Acc:  0.9375\n",
            "Loss at step 752: 0.009325900115072727\n",
            "Acc:  0.9375\n",
            "Loss at step 753: 0.009313182905316353\n",
            "Acc:  0.9375\n",
            "Loss at step 754: 0.009300523437559605\n",
            "Acc:  0.9375\n",
            "Loss at step 755: 0.009287870489060879\n",
            "Acc:  0.9375\n",
            "Loss at step 756: 0.009275288321077824\n",
            "Acc:  0.9375\n",
            "Loss at step 757: 0.00926271267235279\n",
            "Acc:  0.9375\n",
            "Loss at step 758: 0.00925015565007925\n",
            "Acc:  0.9375\n",
            "Loss at step 759: 0.009237667545676231\n",
            "Acc:  0.9375\n",
            "Loss at step 760: 0.009225196205079556\n",
            "Acc:  0.9375\n",
            "Loss at step 761: 0.009212806820869446\n",
            "Acc:  0.9375\n",
            "Loss at step 762: 0.009200353175401688\n",
            "Acc:  0.9375\n",
            "Loss at step 763: 0.009187981486320496\n",
            "Acc:  0.9375\n",
            "Loss at step 764: 0.009175651706755161\n",
            "Acc:  0.9375\n",
            "Loss at step 765: 0.00916337687522173\n",
            "Acc:  0.9375\n",
            "Loss at step 766: 0.009151086211204529\n",
            "Acc:  0.9375\n",
            "Loss at step 767: 0.009138866327702999\n",
            "Acc:  0.9375\n",
            "Loss at step 768: 0.009126631543040276\n",
            "Acc:  0.9375\n",
            "Loss at step 769: 0.009114461950957775\n",
            "Acc:  0.9375\n",
            "Loss at step 770: 0.00910230167210102\n",
            "Acc:  0.9375\n",
            "Loss at step 771: 0.009090222418308258\n",
            "Acc:  0.9375\n",
            "Loss at step 772: 0.00907811988145113\n",
            "Acc:  0.9375\n",
            "Loss at step 773: 0.00906610768288374\n",
            "Acc:  0.9375\n",
            "Loss at step 774: 0.009054126217961311\n",
            "Acc:  0.9375\n",
            "Loss at step 775: 0.009042135439813137\n",
            "Acc:  0.9375\n",
            "Loss at step 776: 0.0090301763266325\n",
            "Acc:  0.9375\n",
            "Loss at step 777: 0.009018270298838615\n",
            "Acc:  0.9375\n",
            "Loss at step 778: 0.009006383828818798\n",
            "Acc:  0.9375\n",
            "Loss at step 779: 0.008994495496153831\n",
            "Acc:  0.9375\n",
            "Loss at step 780: 0.008982675150036812\n",
            "Acc:  0.9375\n",
            "Loss at step 781: 0.008970938622951508\n",
            "Acc:  0.9375\n",
            "Loss at step 782: 0.008959183469414711\n",
            "Acc:  0.9375\n",
            "Loss at step 783: 0.008947433903813362\n",
            "Acc:  0.9375\n",
            "Loss at step 784: 0.008935749530792236\n",
            "Acc:  0.9375\n",
            "Loss at step 785: 0.008924095891416073\n",
            "Acc:  0.9375\n",
            "Loss at step 786: 0.008912450633943081\n",
            "Acc:  0.9375\n",
            "Loss at step 787: 0.008900804445147514\n",
            "Acc:  0.9375\n",
            "Loss at step 788: 0.008889245800673962\n",
            "Acc:  0.9375\n",
            "Loss at step 789: 0.008877722546458244\n",
            "Acc:  0.9375\n",
            "Loss at step 790: 0.00886619370430708\n",
            "Acc:  0.9375\n",
            "Loss at step 791: 0.008854687213897705\n",
            "Acc:  0.9375\n",
            "Loss at step 792: 0.008843235671520233\n",
            "Acc:  0.9375\n",
            "Loss at step 793: 0.008831828832626343\n",
            "Acc:  0.9375\n",
            "Loss at step 794: 0.008820440620183945\n",
            "Acc:  0.9375\n",
            "Loss at step 795: 0.008809072896838188\n",
            "Acc:  0.9375\n",
            "Loss at step 796: 0.008797741495072842\n",
            "Acc:  0.9375\n",
            "Loss at step 797: 0.008786396123468876\n",
            "Acc:  0.9375\n",
            "Loss at step 798: 0.008775139227509499\n",
            "Acc:  0.9375\n",
            "Loss at step 799: 0.00876388605684042\n",
            "Acc:  0.9375\n",
            "Loss at step 800: 0.008752698078751564\n",
            "Acc:  0.9375\n",
            "Loss at step 801: 0.00874149240553379\n",
            "Acc:  0.9375\n",
            "Loss at step 802: 0.008730296976864338\n",
            "Acc:  0.9375\n",
            "Loss at step 803: 0.00871919747442007\n",
            "Acc:  0.9375\n",
            "Loss at step 804: 0.00870808307081461\n",
            "Acc:  0.9375\n",
            "Loss at step 805: 0.008697008714079857\n",
            "Acc:  0.9375\n",
            "Loss at step 806: 0.00868591945618391\n",
            "Acc:  0.9375\n",
            "Loss at step 807: 0.008674955926835537\n",
            "Acc:  0.9375\n",
            "Loss at step 808: 0.00866390299052\n",
            "Acc:  0.9375\n",
            "Loss at step 809: 0.008652979508042336\n",
            "Acc:  0.9375\n",
            "Loss at step 810: 0.008642041124403477\n",
            "Acc:  0.9375\n",
            "Loss at step 811: 0.008631112985312939\n",
            "Acc:  0.9375\n",
            "Loss at step 812: 0.008620188571512699\n",
            "Acc:  0.9375\n",
            "Loss at step 813: 0.008609375916421413\n",
            "Acc:  0.9375\n",
            "Loss at step 814: 0.00859853159636259\n",
            "Acc:  0.9375\n",
            "Loss at step 815: 0.008587758056819439\n",
            "Acc:  0.9375\n",
            "Loss at step 816: 0.008576955646276474\n",
            "Acc:  0.9375\n",
            "Loss at step 817: 0.008566237054765224\n",
            "Acc:  0.9375\n",
            "Loss at step 818: 0.008555521257221699\n",
            "Acc:  0.9375\n",
            "Loss at step 819: 0.00854482501745224\n",
            "Acc:  0.9375\n",
            "Loss at step 820: 0.008534164167940617\n",
            "Acc:  0.9375\n",
            "Loss at step 821: 0.008523519150912762\n",
            "Acc:  0.9375\n",
            "Loss at step 822: 0.008512873202562332\n",
            "Acc:  0.9375\n",
            "Loss at step 823: 0.00850231945514679\n",
            "Acc:  0.9375\n",
            "Loss at step 824: 0.008491750806570053\n",
            "Acc:  0.9375\n",
            "Loss at step 825: 0.008481205441057682\n",
            "Acc:  0.9375\n",
            "Loss at step 826: 0.008470739237964153\n",
            "Acc:  0.9375\n",
            "Loss at step 827: 0.00846022553741932\n",
            "Acc:  0.9375\n",
            "Loss at step 828: 0.008449801243841648\n",
            "Acc:  0.9375\n",
            "Loss at step 829: 0.008439341560006142\n",
            "Acc:  0.9375\n",
            "Loss at step 830: 0.008428934030234814\n",
            "Acc:  0.9375\n",
            "Loss at step 831: 0.008418586105108261\n",
            "Acc:  0.9375\n",
            "Loss at step 832: 0.008408223278820515\n",
            "Acc:  0.9375\n",
            "Loss at step 833: 0.008397848345339298\n",
            "Acc:  0.9375\n",
            "Loss at step 834: 0.00838759820908308\n",
            "Acc:  0.9375\n",
            "Loss at step 835: 0.00837733130902052\n",
            "Acc:  0.9375\n",
            "Loss at step 836: 0.008367029018700123\n",
            "Acc:  0.9375\n",
            "Loss at step 837: 0.00835683848708868\n",
            "Acc:  0.9375\n",
            "Loss at step 838: 0.008346633054316044\n",
            "Acc:  0.9375\n",
            "Loss at step 839: 0.008336475118994713\n",
            "Acc:  0.9375\n",
            "Loss at step 840: 0.008326294831931591\n",
            "Acc:  0.9375\n",
            "Loss at step 841: 0.008316191844642162\n",
            "Acc:  0.9375\n",
            "Loss at step 842: 0.00830605998635292\n",
            "Acc:  0.9375\n",
            "Loss at step 843: 0.00829600915312767\n",
            "Acc:  0.9375\n",
            "Loss at step 844: 0.008285916410386562\n",
            "Acc:  0.9375\n",
            "Loss at step 845: 0.008275912143290043\n",
            "Acc:  0.9375\n",
            "Loss at step 846: 0.008265919052064419\n",
            "Acc:  0.9375\n",
            "Loss at step 847: 0.008255925960838795\n",
            "Acc:  0.9375\n",
            "Loss at step 848: 0.008245988748967648\n",
            "Acc:  0.9375\n",
            "Loss at step 849: 0.008236046880483627\n",
            "Acc:  0.9375\n",
            "Loss at step 850: 0.008226115256547928\n",
            "Acc:  0.9375\n",
            "Loss at step 851: 0.008216221816837788\n",
            "Acc:  0.9375\n",
            "Loss at step 852: 0.008206394501030445\n",
            "Acc:  0.9375\n",
            "Loss at step 853: 0.008196532726287842\n",
            "Acc:  0.9375\n",
            "Loss at step 854: 0.008186755701899529\n",
            "Acc:  0.9375\n",
            "Loss at step 855: 0.008176939561963081\n",
            "Acc:  0.9375\n",
            "Loss at step 856: 0.008167188614606857\n",
            "Acc:  0.9375\n",
            "Loss at step 857: 0.00815744511783123\n",
            "Acc:  0.9375\n",
            "Loss at step 858: 0.008147742599248886\n",
            "Acc:  0.9375\n",
            "Loss at step 859: 0.008138056844472885\n",
            "Acc:  0.9375\n",
            "Loss at step 860: 0.008128353394567966\n",
            "Acc:  0.9375\n",
            "Loss at step 861: 0.008118709549307823\n",
            "Acc:  0.9375\n",
            "Loss at step 862: 0.008109083399176598\n",
            "Acc:  0.9375\n",
            "Loss at step 863: 0.008099488914012909\n",
            "Acc:  0.9375\n",
            "Loss at step 864: 0.008089887909591198\n",
            "Acc:  0.9375\n",
            "Loss at step 865: 0.00808031391352415\n",
            "Acc:  0.9375\n",
            "Loss at step 866: 0.008070772513747215\n",
            "Acc:  0.9375\n",
            "Loss at step 867: 0.008061288855969906\n",
            "Acc:  0.9375\n",
            "Loss at step 868: 0.008051757700741291\n",
            "Acc:  0.9375\n",
            "Loss at step 869: 0.008042343892157078\n",
            "Acc:  0.9375\n",
            "Loss at step 870: 0.00803290493786335\n",
            "Acc:  0.9375\n",
            "Loss at step 871: 0.008023463189601898\n",
            "Acc:  0.9375\n",
            "Loss at step 872: 0.008014055900275707\n",
            "Acc:  0.9375\n",
            "Loss at step 873: 0.008004695177078247\n",
            "Acc:  0.9375\n",
            "Loss at step 874: 0.00799531675875187\n",
            "Acc:  0.9375\n",
            "Loss at step 875: 0.007985969074070454\n",
            "Acc:  0.9375\n",
            "Loss at step 876: 0.007976694963872433\n",
            "Acc:  0.9375\n",
            "Loss at step 877: 0.007967389188706875\n",
            "Acc:  0.9375\n",
            "Loss at step 878: 0.007958096452057362\n",
            "Acc:  0.9375\n",
            "Loss at step 879: 0.007948891259729862\n",
            "Acc:  0.9375\n",
            "Loss at step 880: 0.00793964322656393\n",
            "Acc:  0.9375\n",
            "Loss at step 881: 0.007930416613817215\n",
            "Acc:  0.9375\n",
            "Loss at step 882: 0.007921244949102402\n",
            "Acc:  0.9375\n",
            "Loss at step 883: 0.007912129163742065\n",
            "Acc:  0.9375\n",
            "Loss at step 884: 0.007902979850769043\n",
            "Acc:  0.9375\n",
            "Loss at step 885: 0.007893837057054043\n",
            "Acc:  0.9375\n",
            "Loss at step 886: 0.0078847361728549\n",
            "Acc:  0.9375\n",
            "Loss at step 887: 0.007875646464526653\n",
            "Acc:  0.9375\n",
            "Loss at step 888: 0.007866576313972473\n",
            "Acc:  0.9375\n",
            "Loss at step 889: 0.007857540622353554\n",
            "Acc:  0.9375\n",
            "Loss at step 890: 0.007848542183637619\n",
            "Acc:  0.9375\n",
            "Loss at step 891: 0.007839520461857319\n",
            "Acc:  0.9375\n",
            "Loss at step 892: 0.007830529473721981\n",
            "Acc:  0.9375\n",
            "Loss at step 893: 0.007821607403457165\n",
            "Acc:  0.9375\n",
            "Loss at step 894: 0.007812689058482647\n",
            "Acc:  0.9375\n",
            "Loss at step 895: 0.007803771179169416\n",
            "Acc:  0.9375\n",
            "Loss at step 896: 0.007794882636517286\n",
            "Acc:  0.9375\n",
            "Loss at step 897: 0.007785986643284559\n",
            "Acc:  0.9375\n",
            "Loss at step 898: 0.007777148392051458\n",
            "Acc:  0.9375\n",
            "Loss at step 899: 0.007768329698592424\n",
            "Acc:  0.9375\n",
            "Loss at step 900: 0.007759492378681898\n",
            "Acc:  0.9375\n",
            "Loss at step 901: 0.007750731892883778\n",
            "Acc:  0.9375\n",
            "Loss at step 902: 0.007741937879472971\n",
            "Acc:  0.9375\n",
            "Loss at step 903: 0.0077332076616585255\n",
            "Acc:  0.9375\n",
            "Loss at step 904: 0.007724442984908819\n",
            "Acc:  0.9375\n",
            "Loss at step 905: 0.007715778425335884\n",
            "Acc:  0.9375\n",
            "Loss at step 906: 0.007707069627940655\n",
            "Acc:  0.9375\n",
            "Loss at step 907: 0.007698397617787123\n",
            "Acc:  0.9375\n",
            "Loss at step 908: 0.007689731661230326\n",
            "Acc:  0.9375\n",
            "Loss at step 909: 0.00768109317868948\n",
            "Acc:  0.9375\n",
            "Loss at step 910: 0.007672482635825872\n",
            "Acc:  0.9375\n",
            "Loss at step 911: 0.007663884200155735\n",
            "Acc:  0.9375\n",
            "Loss at step 912: 0.007655322086066008\n",
            "Acc:  0.9375\n",
            "Loss at step 913: 0.0076467012986540794\n",
            "Acc:  0.9375\n",
            "Loss at step 914: 0.007638193666934967\n",
            "Acc:  0.9375\n",
            "Loss at step 915: 0.00762966088950634\n",
            "Acc:  0.9375\n",
            "Loss at step 916: 0.0076212203130126\n",
            "Acc:  0.9375\n",
            "Loss at step 917: 0.007612716406583786\n",
            "Acc:  0.9375\n",
            "Loss at step 918: 0.00760424230247736\n",
            "Acc:  0.9375\n",
            "Loss at step 919: 0.007595809176564217\n",
            "Acc:  0.9375\n",
            "Loss at step 920: 0.007587397005409002\n",
            "Acc:  0.9375\n",
            "Loss at step 921: 0.00757897412404418\n",
            "Acc:  0.9375\n",
            "Loss at step 922: 0.007570551708340645\n",
            "Acc:  0.9375\n",
            "Loss at step 923: 0.007562176790088415\n",
            "Acc:  0.9375\n",
            "Loss at step 924: 0.007553849369287491\n",
            "Acc:  0.9375\n",
            "Loss at step 925: 0.007545537315309048\n",
            "Acc:  0.9375\n",
            "Loss at step 926: 0.0075372024439275265\n",
            "Acc:  0.9375\n",
            "Loss at step 927: 0.007528937887400389\n",
            "Acc:  0.9375\n",
            "Loss at step 928: 0.007520635612308979\n",
            "Acc:  0.9375\n",
            "Loss at step 929: 0.0075123971328139305\n",
            "Acc:  0.9375\n",
            "Loss at step 930: 0.0075041428208351135\n",
            "Acc:  0.9375\n",
            "Loss at step 931: 0.007495937403291464\n",
            "Acc:  0.9375\n",
            "Loss at step 932: 0.0074877385050058365\n",
            "Acc:  0.9375\n",
            "Loss at step 933: 0.007479554507881403\n",
            "Acc:  0.9375\n",
            "Loss at step 934: 0.007471389602869749\n",
            "Acc:  0.9375\n",
            "Loss at step 935: 0.00746323773637414\n",
            "Acc:  0.9375\n",
            "Loss at step 936: 0.007455090992152691\n",
            "Acc:  0.9375\n",
            "Loss at step 937: 0.007446968462318182\n",
            "Acc:  0.9375\n",
            "Loss at step 938: 0.007438902743160725\n",
            "Acc:  0.9375\n",
            "Loss at step 939: 0.007430796045809984\n",
            "Acc:  0.9375\n",
            "Loss at step 940: 0.007422728464007378\n",
            "Acc:  0.9375\n",
            "Loss at step 941: 0.007414692081511021\n",
            "Acc:  0.9375\n",
            "Loss at step 942: 0.0074066887609660625\n",
            "Acc:  0.9375\n",
            "Loss at step 943: 0.007398680783808231\n",
            "Acc:  0.9375\n",
            "Loss at step 944: 0.007390687707811594\n",
            "Acc:  0.9375\n",
            "Loss at step 945: 0.00738268718123436\n",
            "Acc:  0.9375\n",
            "Loss at step 946: 0.0073747532442212105\n",
            "Acc:  0.9375\n",
            "Loss at step 947: 0.007366814650595188\n",
            "Acc:  0.9375\n",
            "Loss at step 948: 0.007358873263001442\n",
            "Acc:  0.9375\n",
            "Loss at step 949: 0.007351001258939505\n",
            "Acc:  0.9375\n",
            "Loss at step 950: 0.0073430780321359634\n",
            "Acc:  0.9375\n",
            "Loss at step 951: 0.007335235830396414\n",
            "Acc:  0.9375\n",
            "Loss at step 952: 0.007327334024012089\n",
            "Acc:  0.9375\n",
            "Loss at step 953: 0.007319532334804535\n",
            "Acc:  0.9375\n",
            "Loss at step 954: 0.007311711553484201\n",
            "Acc:  0.9375\n",
            "Loss at step 955: 0.007303920574486256\n",
            "Acc:  0.9375\n",
            "Loss at step 956: 0.007296118885278702\n",
            "Acc:  0.9375\n",
            "Loss at step 957: 0.007288338150829077\n",
            "Acc:  0.9375\n",
            "Loss at step 958: 0.007280606310814619\n",
            "Acc:  0.9375\n",
            "Loss at step 959: 0.0072728414088487625\n",
            "Acc:  0.9375\n",
            "Loss at step 960: 0.007265119813382626\n",
            "Acc:  0.9375\n",
            "Loss at step 961: 0.007257417310029268\n",
            "Acc:  0.9375\n",
            "Loss at step 962: 0.007249746937304735\n",
            "Acc:  0.9375\n",
            "Loss at step 963: 0.0072420923970639706\n",
            "Acc:  0.9375\n",
            "Loss at step 964: 0.007234411779791117\n",
            "Acc:  0.9375\n",
            "Loss at step 965: 0.007226781919598579\n",
            "Acc:  0.9375\n",
            "Loss at step 966: 0.007219159975647926\n",
            "Acc:  0.9375\n",
            "Loss at step 967: 0.007211526855826378\n",
            "Acc:  0.9375\n",
            "Loss at step 968: 0.007203985936939716\n",
            "Acc:  0.9375\n",
            "Loss at step 969: 0.007196374237537384\n",
            "Acc:  0.9375\n",
            "Loss at step 970: 0.0071888267993927\n",
            "Acc:  0.9375\n",
            "Loss at step 971: 0.0071812779642641544\n",
            "Acc:  0.9375\n",
            "Loss at step 972: 0.007173736114054918\n",
            "Acc:  0.9375\n",
            "Loss at step 973: 0.007166227791458368\n",
            "Acc:  0.9375\n",
            "Loss at step 974: 0.007158734370023012\n",
            "Acc:  0.9375\n",
            "Loss at step 975: 0.007151233498007059\n",
            "Acc:  0.9375\n",
            "Loss at step 976: 0.007143798749893904\n",
            "Acc:  0.9375\n",
            "Loss at step 977: 0.00713632395491004\n",
            "Acc:  0.9375\n",
            "Loss at step 978: 0.007128908298909664\n",
            "Acc:  0.9375\n",
            "Loss at step 979: 0.007121458649635315\n",
            "Acc:  0.9375\n",
            "Loss at step 980: 0.007114071864634752\n",
            "Acc:  0.9375\n",
            "Loss at step 981: 0.0071066999807953835\n",
            "Acc:  0.9375\n",
            "Loss at step 982: 0.007099301554262638\n",
            "Acc:  0.9375\n",
            "Loss at step 983: 0.007091977167874575\n",
            "Acc:  0.9375\n",
            "Loss at step 984: 0.007084630895406008\n",
            "Acc:  0.9375\n",
            "Loss at step 985: 0.007077251095324755\n",
            "Acc:  0.9375\n",
            "Loss at step 986: 0.007069981656968594\n",
            "Acc:  0.9375\n",
            "Loss at step 987: 0.007062698248773813\n",
            "Acc:  0.9375\n",
            "Loss at step 988: 0.0070554171688854694\n",
            "Acc:  0.9375\n",
            "Loss at step 989: 0.0070481193251907825\n",
            "Acc:  0.9375\n",
            "Loss at step 990: 0.007040856871753931\n",
            "Acc:  0.9375\n",
            "Loss at step 991: 0.007033658213913441\n",
            "Acc:  0.9375\n",
            "Loss at step 992: 0.007026443723589182\n",
            "Acc:  0.9375\n",
            "Loss at step 993: 0.007019244600087404\n",
            "Acc:  0.9375\n",
            "Loss at step 994: 0.00701205525547266\n",
            "Acc:  0.9375\n",
            "Loss at step 995: 0.007004881743341684\n",
            "Acc:  0.9375\n",
            "Loss at step 996: 0.006997700314968824\n",
            "Acc:  0.9375\n",
            "Loss at step 997: 0.00699057150632143\n",
            "Acc:  0.9375\n",
            "Loss at step 998: 0.006983411964029074\n",
            "Acc:  0.9375\n",
            "Loss at step 999: 0.00697631249204278\n",
            "Acc:  0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP_vMF6eplCL"
      },
      "source": [
        "## train/val/test\n",
        "\n",
        "Pri ďalšej práci s kapacitou modelu budeme potrebovať vymedziť tzv. split našich dát. Tento split robíme preto aby sme vedeli verifikovať, či náš model naozaj dokáže dosahovať dobré výsledky na dátach ktoré neboli použíté na trénovanie. \n",
        "\n",
        "Dáta ktoré model nevidel budeme označovať ako testovacie. Tieto dáta používame len na overenie výsledkov.\n",
        "\n",
        "Ostali nám teda dáta na trénovanie. Problém ale je, že máme dva druhy optimalizácie. Jednou optimalizáciou (napr. SGD) vyberáme samotné parametre modelu. Druhú optimalizáciu robíme ako uživatelia a to tým, že vyberáme optimalizačný algoritmus pre prvý druh optimalizácie. Takisto vyberáme aj parametre tejto optimalizácie a iné veci ako napríklad samotný výber architektúry modelu, inicializačný algoritmus, veľkosť minibatch, dĺžku trénovania atď. Tieto veci častokrát označujeme za tzv. hyperparametre.\n",
        "\n",
        "Aby sme mohli takto vyberať, tak vždy najprv trénujeme (prvý druh optimalizácie)na tzv. trénovacej množine a takto natrénované parametre potom overíme na tzv. validačnej množine. Tieto výsledky nám potom umožnia realizovať druhý druh optimalizácie a porovnávať tak výsledky pre rôzne druhy optimalizácie.\n",
        "\n",
        "Dôvod prečo to nerobíme na testovacej množine je ten, že sa môže stať že výber hyperparametrov je tiež taký, že sme ich vybrali zrovna tak, že sa presne hodí na validačnú množinu.\n",
        "\n",
        "Typicky delíme dáta pomerom 80/10/10, alebo 60/20/20 atď, ale záleží na danej úlohe a množstve dát. Podobne si rozdelíme dataset MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tLr2FjDpn1u"
      },
      "source": [
        "x_train = x[:50000]\n",
        "labels_train = labels[:50000]\n",
        "x_val = x[50000:60000]\n",
        "labels_val = labels[50000:60000]\n",
        "x_test = x[60000:]\n",
        "labels_test = labels[60000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-NAWQd8UE3S"
      },
      "source": [
        "Pre iterovanie použijeme PyTorch utils pre dáta a to tak, že si najprv vytvoríme objekt datasetu a následne objekt dataloadera, ktorý nám poskytne iterátor, cez ktorý budeme dáta iterovať po tzv. dávkach, alebo minibatchoch.\n",
        "\n",
        "V minibatchoch spočíva aj princíp metódy SGD (stochastic gradient descent). V nej neoptimalizujeme cez celú trénovaciu množinu, ale cez náhodne vybraté menšie množtvo vzoriek. Toto má výhodu v tom, že celý proces má tak menšie pamäťové nároky a zároveň to vnáša element náhody do optimalizácie, čo umožňuje dostať sa von z nevhodných oblastí parametrického priestoru akými sú lokálne minimá.\n",
        "\n",
        "Pri trénovaní budeme teda iterovať cez náhodne vybraté príklady z trénovacej množiny až kým neprejdeme všetky. To sa volá jedna epocha trénovania. Potom si necháme vypísať presnosť a loss na validačnej množine aby sme si overili, že sieť sa naozaj učí. Pri tom použijeme torch.no_grad(), aby PyTorch vedel, že netreba rátať gradienty a nemíňal tak pamäť a čas na medzivýpočty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHqfea454_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc048b83-265f-42b4-cb78-be8aa0804242"
      },
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "data_train = TensorDataset(x_train, labels_train)\n",
        "dataloader_train = DataLoader(data_train, batch_size=32, shuffle=True)\n",
        "\n",
        "data_val = TensorDataset(x_val, labels_val)\n",
        "dataloader_val = DataLoader(data_val, batch_size=32)\n",
        "\n",
        "data_test = TensorDataset(x_test, labels_test)\n",
        "dataloader_test = DataLoader(data_test, batch_size=32)\n",
        "\n",
        "data_val = TensorDataset(x_val, labels_val)\n",
        "\n",
        "\n",
        "model = Sequential(Linear(28 * 28, 30),\n",
        "                   ReLU(),\n",
        "                   Linear(30, 20),\n",
        "                   ReLU(),\n",
        "                   Linear(20, 10))\n",
        "\n",
        "model_inference = Sequential(model, Softmax(dim=-1)) \n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "for e in range(10):\n",
        "  for i, batch in enumerate(dataloader_train):  \n",
        "    x, y = batch  \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x)\n",
        "    loss = ce_loss(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0:\n",
        "      print(\"Loss at epoch: {} step {}: {}\".format(e, i, loss.item()))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_losses = []\n",
        "    correct = 0\n",
        "    for i, batch in enumerate(dataloader_val):  \n",
        "      x, y = batch  \n",
        "\n",
        "      out = model(x)\n",
        "      loss = ce_loss(out, y)\n",
        "      acc = torch.sum(torch.argmax(out_classes, dim=-1) == y)\n",
        "      correct += acc.item()\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "    print(\"Val loss at epoch {}: {}\".format(e, np.mean(val_losses)))\n",
        "    print(\"Val acc at epoch {}: {}\".format(e, correct / 10000))\n",
        "\n",
        "  out = model_inference(x_mini)\n",
        "  out_classes = torch.argmax(out, dim=-1)\n",
        "  acc = torch.sum(out_classes == labels_mini)/32\n",
        "  print(\"Acc: \", acc.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch: 0 step 0: 2.329540491104126\n",
            "Loss at epoch: 0 step 100: 2.2945926189422607\n",
            "Loss at epoch: 0 step 200: 2.2437427043914795\n",
            "Loss at epoch: 0 step 300: 2.160517692565918\n",
            "Loss at epoch: 0 step 400: 2.0804035663604736\n",
            "Loss at epoch: 0 step 500: 1.9418731927871704\n",
            "Loss at epoch: 0 step 600: 1.4974390268325806\n",
            "Loss at epoch: 0 step 700: 1.4106457233428955\n",
            "Loss at epoch: 0 step 800: 1.1532241106033325\n",
            "Loss at epoch: 0 step 900: 1.1497275829315186\n",
            "Loss at epoch: 0 step 1000: 0.8165332674980164\n",
            "Loss at epoch: 0 step 1100: 0.6553091406822205\n",
            "Loss at epoch: 0 step 1200: 0.4725855588912964\n",
            "Loss at epoch: 0 step 1300: 0.5473076701164246\n",
            "Loss at epoch: 0 step 1400: 0.4920465350151062\n",
            "Loss at epoch: 0 step 1500: 0.4737679362297058\n",
            "Val loss at epoch 0: 0.5226034473258847\n",
            "Val acc at epoch 0: 0.0983\n",
            "Acc:  0.75\n",
            "Loss at epoch: 1 step 0: 0.4501376748085022\n",
            "Loss at epoch: 1 step 100: 0.4405437707901001\n",
            "Loss at epoch: 1 step 200: 0.46432769298553467\n",
            "Loss at epoch: 1 step 300: 0.3153669238090515\n",
            "Loss at epoch: 1 step 400: 0.43933454155921936\n",
            "Loss at epoch: 1 step 500: 0.3729570806026459\n",
            "Loss at epoch: 1 step 600: 0.6616063117980957\n",
            "Loss at epoch: 1 step 700: 0.2593197226524353\n",
            "Loss at epoch: 1 step 800: 0.49129849672317505\n",
            "Loss at epoch: 1 step 900: 0.4897475242614746\n",
            "Loss at epoch: 1 step 1000: 0.46783530712127686\n",
            "Loss at epoch: 1 step 1100: 0.3182787299156189\n",
            "Loss at epoch: 1 step 1200: 0.5182767510414124\n",
            "Loss at epoch: 1 step 1300: 0.34034180641174316\n",
            "Loss at epoch: 1 step 1400: 0.6987234950065613\n",
            "Loss at epoch: 1 step 1500: 0.22562623023986816\n",
            "Val loss at epoch 1: 0.38355062649653743\n",
            "Val acc at epoch 1: 0.0\n",
            "Acc:  0.78125\n",
            "Loss at epoch: 2 step 0: 0.3004703223705292\n",
            "Loss at epoch: 2 step 100: 0.22150398790836334\n",
            "Loss at epoch: 2 step 200: 0.4949866533279419\n",
            "Loss at epoch: 2 step 300: 0.3295558989048004\n",
            "Loss at epoch: 2 step 400: 0.5343586802482605\n",
            "Loss at epoch: 2 step 500: 0.4420354664325714\n",
            "Loss at epoch: 2 step 600: 0.43962499499320984\n",
            "Loss at epoch: 2 step 700: 0.36741694808006287\n",
            "Loss at epoch: 2 step 800: 1.2193418741226196\n",
            "Loss at epoch: 2 step 900: 0.5228376388549805\n",
            "Loss at epoch: 2 step 1000: 0.3731117248535156\n",
            "Loss at epoch: 2 step 1100: 0.4167359471321106\n",
            "Loss at epoch: 2 step 1200: 0.3808485269546509\n",
            "Loss at epoch: 2 step 1300: 0.27419421076774597\n",
            "Loss at epoch: 2 step 1400: 0.5290888547897339\n",
            "Loss at epoch: 2 step 1500: 0.1852451115846634\n",
            "Val loss at epoch 2: 0.32594144479416237\n",
            "Val acc at epoch 2: 0.099\n",
            "Acc:  0.84375\n",
            "Loss at epoch: 3 step 0: 0.4458891451358795\n",
            "Loss at epoch: 3 step 100: 0.4880896210670471\n",
            "Loss at epoch: 3 step 200: 0.18478356301784515\n",
            "Loss at epoch: 3 step 300: 0.1281367540359497\n",
            "Loss at epoch: 3 step 400: 0.2998325824737549\n",
            "Loss at epoch: 3 step 500: 0.3218469023704529\n",
            "Loss at epoch: 3 step 600: 0.624936580657959\n",
            "Loss at epoch: 3 step 700: 0.24822507798671722\n",
            "Loss at epoch: 3 step 800: 0.23946107923984528\n",
            "Loss at epoch: 3 step 900: 0.3132452964782715\n",
            "Loss at epoch: 3 step 1000: 0.3579537570476532\n",
            "Loss at epoch: 3 step 1100: 0.2568913400173187\n",
            "Loss at epoch: 3 step 1200: 0.6063168048858643\n",
            "Loss at epoch: 3 step 1300: 0.46313583850860596\n",
            "Loss at epoch: 3 step 1400: 0.2770141065120697\n",
            "Loss at epoch: 3 step 1500: 0.1945372074842453\n",
            "Val loss at epoch 3: 0.2905620667047965\n",
            "Val acc at epoch 3: 0.099\n",
            "Acc:  0.9375\n",
            "Loss at epoch: 4 step 0: 0.5018616914749146\n",
            "Loss at epoch: 4 step 100: 0.24329976737499237\n",
            "Loss at epoch: 4 step 200: 0.20144622027873993\n",
            "Loss at epoch: 4 step 300: 0.17171142995357513\n",
            "Loss at epoch: 4 step 400: 0.17733192443847656\n",
            "Loss at epoch: 4 step 500: 0.19652095437049866\n",
            "Loss at epoch: 4 step 600: 0.2566404342651367\n",
            "Loss at epoch: 4 step 700: 0.23386205732822418\n",
            "Loss at epoch: 4 step 800: 0.370195597410202\n",
            "Loss at epoch: 4 step 900: 0.17637751996517181\n",
            "Loss at epoch: 4 step 1000: 0.2869862914085388\n",
            "Loss at epoch: 4 step 1100: 0.31230783462524414\n",
            "Loss at epoch: 4 step 1200: 0.14645060896873474\n",
            "Loss at epoch: 4 step 1300: 0.08615583181381226\n",
            "Loss at epoch: 4 step 1400: 0.5033962726593018\n",
            "Loss at epoch: 4 step 1500: 0.2292719930410385\n",
            "Val loss at epoch 4: 0.26817660768941853\n",
            "Val acc at epoch 4: 0.0983\n",
            "Acc:  0.90625\n",
            "Loss at epoch: 5 step 0: 0.29219764471054077\n",
            "Loss at epoch: 5 step 100: 0.08610846102237701\n",
            "Loss at epoch: 5 step 200: 0.3007557690143585\n",
            "Loss at epoch: 5 step 300: 0.31222474575042725\n",
            "Loss at epoch: 5 step 400: 0.19349202513694763\n",
            "Loss at epoch: 5 step 500: 0.3024389147758484\n",
            "Loss at epoch: 5 step 600: 0.2613048553466797\n",
            "Loss at epoch: 5 step 700: 0.26905331015586853\n",
            "Loss at epoch: 5 step 800: 0.13312658667564392\n",
            "Loss at epoch: 5 step 900: 0.10954806208610535\n",
            "Loss at epoch: 5 step 1000: 0.08843062818050385\n",
            "Loss at epoch: 5 step 1100: 0.159637913107872\n",
            "Loss at epoch: 5 step 1200: 0.3354507088661194\n",
            "Loss at epoch: 5 step 1300: 0.6593921780586243\n",
            "Loss at epoch: 5 step 1400: 0.2809433341026306\n",
            "Loss at epoch: 5 step 1500: 0.17836497724056244\n",
            "Val loss at epoch 5: 0.24695324403242752\n",
            "Val acc at epoch 5: 0.0983\n",
            "Acc:  0.9375\n",
            "Loss at epoch: 6 step 0: 0.36681488156318665\n",
            "Loss at epoch: 6 step 100: 0.3083569407463074\n",
            "Loss at epoch: 6 step 200: 0.22488954663276672\n",
            "Loss at epoch: 6 step 300: 0.3113926947116852\n",
            "Loss at epoch: 6 step 400: 0.13518111407756805\n",
            "Loss at epoch: 6 step 500: 0.29546022415161133\n",
            "Loss at epoch: 6 step 600: 0.09390728920698166\n",
            "Loss at epoch: 6 step 700: 0.21745461225509644\n",
            "Loss at epoch: 6 step 800: 0.10581698268651962\n",
            "Loss at epoch: 6 step 900: 0.17167818546295166\n",
            "Loss at epoch: 6 step 1000: 0.33064210414886475\n",
            "Loss at epoch: 6 step 1100: 0.4646366536617279\n",
            "Loss at epoch: 6 step 1200: 0.14890122413635254\n",
            "Loss at epoch: 6 step 1300: 0.31294935941696167\n",
            "Loss at epoch: 6 step 1400: 0.37082439661026\n",
            "Loss at epoch: 6 step 1500: 0.06569043546915054\n",
            "Val loss at epoch 6: 0.2262880739059668\n",
            "Val acc at epoch 6: 0.0983\n",
            "Acc:  0.9375\n",
            "Loss at epoch: 7 step 0: 0.19818654656410217\n",
            "Loss at epoch: 7 step 100: 0.19702163338661194\n",
            "Loss at epoch: 7 step 200: 0.14450395107269287\n",
            "Loss at epoch: 7 step 300: 0.09929382055997849\n",
            "Loss at epoch: 7 step 400: 0.14581219851970673\n",
            "Loss at epoch: 7 step 500: 0.15385940670967102\n",
            "Loss at epoch: 7 step 600: 0.1838720440864563\n",
            "Loss at epoch: 7 step 700: 0.4408952593803406\n",
            "Loss at epoch: 7 step 800: 0.5099819302558899\n",
            "Loss at epoch: 7 step 900: 0.17563141882419586\n",
            "Loss at epoch: 7 step 1000: 0.12120864540338516\n",
            "Loss at epoch: 7 step 1100: 0.14687013626098633\n",
            "Loss at epoch: 7 step 1200: 0.10991690307855606\n",
            "Loss at epoch: 7 step 1300: 0.2311495840549469\n",
            "Loss at epoch: 7 step 1400: 0.18140336871147156\n",
            "Loss at epoch: 7 step 1500: 0.1750251054763794\n",
            "Val loss at epoch 7: 0.2131264472035126\n",
            "Val acc at epoch 7: 0.0983\n",
            "Acc:  0.9375\n",
            "Loss at epoch: 8 step 0: 0.12886835634708405\n",
            "Loss at epoch: 8 step 100: 0.18422186374664307\n",
            "Loss at epoch: 8 step 200: 0.48433661460876465\n",
            "Loss at epoch: 8 step 300: 0.4042171835899353\n",
            "Loss at epoch: 8 step 400: 0.31169408559799194\n",
            "Loss at epoch: 8 step 500: 0.10332740843296051\n",
            "Loss at epoch: 8 step 600: 0.37281373143196106\n",
            "Loss at epoch: 8 step 700: 0.12880662083625793\n",
            "Loss at epoch: 8 step 800: 0.28373295068740845\n",
            "Loss at epoch: 8 step 900: 0.06022541970014572\n",
            "Loss at epoch: 8 step 1000: 0.214736670255661\n",
            "Loss at epoch: 8 step 1100: 0.1774255931377411\n",
            "Loss at epoch: 8 step 1200: 0.5437663197517395\n",
            "Loss at epoch: 8 step 1300: 0.13977879285812378\n",
            "Loss at epoch: 8 step 1400: 0.05904687941074371\n",
            "Loss at epoch: 8 step 1500: 0.17993931472301483\n",
            "Val loss at epoch 8: 0.20076135043411875\n",
            "Val acc at epoch 8: 0.0983\n",
            "Acc:  0.9375\n",
            "Loss at epoch: 9 step 0: 0.11557547003030777\n",
            "Loss at epoch: 9 step 100: 0.1887354850769043\n",
            "Loss at epoch: 9 step 200: 0.2722901999950409\n",
            "Loss at epoch: 9 step 300: 0.24649660289287567\n",
            "Loss at epoch: 9 step 400: 0.21565181016921997\n",
            "Loss at epoch: 9 step 500: 0.3296375274658203\n",
            "Loss at epoch: 9 step 600: 0.21971969306468964\n",
            "Loss at epoch: 9 step 700: 0.13721084594726562\n",
            "Loss at epoch: 9 step 800: 0.42360565066337585\n",
            "Loss at epoch: 9 step 900: 0.03483419120311737\n",
            "Loss at epoch: 9 step 1000: 0.20797888934612274\n",
            "Loss at epoch: 9 step 1100: 0.06656040996313095\n",
            "Loss at epoch: 9 step 1200: 0.10238930583000183\n",
            "Loss at epoch: 9 step 1300: 0.24186120927333832\n",
            "Loss at epoch: 9 step 1400: 0.038646090775728226\n",
            "Loss at epoch: 9 step 1500: 0.2720620334148407\n",
            "Val loss at epoch 9: 0.20259884142052062\n",
            "Val acc at epoch 9: 0.0983\n",
            "Acc:  0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzVjhoM9aYe8"
      },
      "source": [
        "## Ukladanie modelu\n",
        "\n",
        "Samozrejme model by sme si ideálne radi uložili. To sa dá docieliť, tak že vytiahneme parametre modelu a uložíme ich. Model si však nepamätá štruktúru, je teda vhodné napr. v kóde vytvoriť triedu na tvorenie modelu. To si však ukážeme neskôr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ZbINFd56bXMQ",
        "outputId": "c4594d17-99d5-46f2-8a16-454dfc4b85bb"
      },
      "source": [
        "torch.save(model.state_dict(), 'saved_model.pth')\n",
        "\n",
        "softmax = Softmax(dim=-1)\n",
        "\n",
        "new_model = Sequential(Linear(28 * 28, 30),\n",
        "                       ReLU(),\n",
        "                       Linear(30, 20),\n",
        "                       ReLU(),\n",
        "                       Linear(20, 10))\n",
        "\n",
        "print(softmax(new_model(x[0])))\n",
        "\n",
        "new_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "\n",
        "print(softmax(new_model(x[0])))\n",
        "\n",
        "newer_model = Sequential(Linear(28 * 28, 60),\n",
        "                       ReLU(),\n",
        "                       Linear(60, 20),\n",
        "                       ReLU(),\n",
        "                       Linear(20, 10))\n",
        "\n",
        "# nebude fungovať\n",
        "newer_model.load_state_dict(torch.load('saved_model.pth'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0921, 0.1065, 0.0899, 0.1214, 0.0965, 0.1042, 0.0883, 0.1014, 0.1179,\n",
            "        0.0818], grad_fn=<SoftmaxBackward>)\n",
            "tensor([4.8162e-07, 9.8317e-01, 4.1781e-03, 3.7117e-03, 6.8570e-05, 6.9618e-04,\n",
            "        1.5931e-04, 1.4426e-03, 6.5146e-03, 6.2836e-05],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ae5d6238a10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# nebude fungovať\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mnewer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1407\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([30, 784]) from checkpoint, the shape in current model is torch.Size([60, 784]).\n\tsize mismatch for 0.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for 2.weight: copying a param with shape torch.Size([20, 30]) from checkpoint, the shape in current model is torch.Size([20, 60])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C8jlu4_ZNP5"
      },
      "source": [
        "## 1. Úloha - optimalizačné hyperparamtre a testovanie\n",
        "\n",
        "Modifikujte kód vyššie a skúste nájsť optimálne hyperparametre trénovania. Teda v tomto prípade skúste meniť learning rate v konštruktore optimalizátora, tak aby ste dosiahli najlepšiu presnosť na validačných dátach.\n",
        "\n",
        "Napíšte potom kód ktorý výstupny model otestuje na testovacích dátach.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH9m2zWHaXz0"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rYxw9Avb5pS"
      },
      "source": [
        "## 2. Úloha - learning rate scheduler\n",
        "\n",
        "Pri trénovaní je niekedy vhodné začať s väčším trénovacím krokom a neskôr prejsť na menší. Pre tento účel je vhodné využiť variabilné nastavenie kroku učenia. Skúste si to naimplementovať podľa dokumentácie napr. [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html), alebo si môžete vybrať aj iný."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nHGojdVeak1"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em_VmNgFnuDV"
      },
      "source": [
        "## Keras\n",
        "\n",
        "Na záver si ukážeme ako vytvoriť plne prepojenú sieť v kerase. Keras je API pre tensorflow, ktoré je na rozdiel od tensorflowu príjemnejšie na užívanie. Jeho použitie je podobné PyTorchu, avšak netreba si písať vlastný cyklus trénovania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OahEGQgrnjH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d36e19f-5321-42c3-e582-39b76408d615"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(30, activation='sigmoid'))\n",
        "model.add(keras.layers.Dense(20, activation='sigmoid'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "loss = keras.losses.categorical_crossentropy\n",
        "model.compile(loss=loss,\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "y_keras = keras.utils.to_categorical(labels_np)\n",
        "\n",
        "history = model.fit(x_np[:50000], y_keras[:50000], validation_data = (x_np[50000:60000], y_keras[50000:60000]), batch_size=32, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2648 - accuracy: 0.2168 - val_loss: 2.1949 - val_accuracy: 0.4148\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 2.0900 - accuracy: 0.3927 - val_loss: 1.9447 - val_accuracy: 0.4754\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7921 - accuracy: 0.5235 - val_loss: 1.6217 - val_accuracy: 0.6047\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.4848 - accuracy: 0.6275 - val_loss: 1.3212 - val_accuracy: 0.6861\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2196 - accuracy: 0.6909 - val_loss: 1.0852 - val_accuracy: 0.7307\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0281 - accuracy: 0.7330 - val_loss: 0.9222 - val_accuracy: 0.7881\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.8953 - accuracy: 0.7712 - val_loss: 0.8053 - val_accuracy: 0.7988\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.7969 - accuracy: 0.7947 - val_loss: 0.7168 - val_accuracy: 0.8253\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.7196 - accuracy: 0.8166 - val_loss: 0.6460 - val_accuracy: 0.8413\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.6563 - accuracy: 0.8335 - val_loss: 0.5876 - val_accuracy: 0.8571\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.6024 - accuracy: 0.8485 - val_loss: 0.5384 - val_accuracy: 0.8682\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.5562 - accuracy: 0.8609 - val_loss: 0.4965 - val_accuracy: 0.8799\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.5170 - accuracy: 0.8702 - val_loss: 0.4620 - val_accuracy: 0.8869\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4840 - accuracy: 0.8781 - val_loss: 0.4335 - val_accuracy: 0.8921\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4563 - accuracy: 0.8840 - val_loss: 0.4094 - val_accuracy: 0.8960\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4331 - accuracy: 0.8886 - val_loss: 0.3898 - val_accuracy: 0.8999\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.4135 - accuracy: 0.8929 - val_loss: 0.3735 - val_accuracy: 0.9022\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3967 - accuracy: 0.8964 - val_loss: 0.3586 - val_accuracy: 0.9059\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3821 - accuracy: 0.8995 - val_loss: 0.3466 - val_accuracy: 0.9071\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3694 - accuracy: 0.9021 - val_loss: 0.3357 - val_accuracy: 0.9095\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3580 - accuracy: 0.9042 - val_loss: 0.3259 - val_accuracy: 0.9119\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3477 - accuracy: 0.9069 - val_loss: 0.3169 - val_accuracy: 0.9139\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3383 - accuracy: 0.9089 - val_loss: 0.3091 - val_accuracy: 0.9157\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3297 - accuracy: 0.9105 - val_loss: 0.3016 - val_accuracy: 0.9166\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3218 - accuracy: 0.9122 - val_loss: 0.2953 - val_accuracy: 0.9184\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3144 - accuracy: 0.9135 - val_loss: 0.2883 - val_accuracy: 0.9199\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3074 - accuracy: 0.9148 - val_loss: 0.2828 - val_accuracy: 0.9217\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3009 - accuracy: 0.9162 - val_loss: 0.2775 - val_accuracy: 0.9229\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2946 - accuracy: 0.9179 - val_loss: 0.2724 - val_accuracy: 0.9239\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2887 - accuracy: 0.9196 - val_loss: 0.2671 - val_accuracy: 0.9258\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2832 - accuracy: 0.9208 - val_loss: 0.2625 - val_accuracy: 0.9267\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2778 - accuracy: 0.9220 - val_loss: 0.2577 - val_accuracy: 0.9283\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2727 - accuracy: 0.9237 - val_loss: 0.2542 - val_accuracy: 0.9277\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2679 - accuracy: 0.9243 - val_loss: 0.2493 - val_accuracy: 0.9297\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2633 - accuracy: 0.9264 - val_loss: 0.2457 - val_accuracy: 0.9306\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2588 - accuracy: 0.9275 - val_loss: 0.2423 - val_accuracy: 0.9309\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2545 - accuracy: 0.9290 - val_loss: 0.2385 - val_accuracy: 0.9327\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2504 - accuracy: 0.9303 - val_loss: 0.2359 - val_accuracy: 0.9324\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2464 - accuracy: 0.9309 - val_loss: 0.2317 - val_accuracy: 0.9343\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2426 - accuracy: 0.9325 - val_loss: 0.2289 - val_accuracy: 0.9348\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2390 - accuracy: 0.9326 - val_loss: 0.2253 - val_accuracy: 0.9345\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2355 - accuracy: 0.9342 - val_loss: 0.2226 - val_accuracy: 0.9370\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2322 - accuracy: 0.9351 - val_loss: 0.2201 - val_accuracy: 0.9364\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2288 - accuracy: 0.9360 - val_loss: 0.2175 - val_accuracy: 0.9386\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2256 - accuracy: 0.9367 - val_loss: 0.2147 - val_accuracy: 0.9383\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2226 - accuracy: 0.9376 - val_loss: 0.2122 - val_accuracy: 0.9390\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2197 - accuracy: 0.9390 - val_loss: 0.2103 - val_accuracy: 0.9390\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2168 - accuracy: 0.9398 - val_loss: 0.2075 - val_accuracy: 0.9402\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2140 - accuracy: 0.9406 - val_loss: 0.2051 - val_accuracy: 0.9399\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2113 - accuracy: 0.9410 - val_loss: 0.2028 - val_accuracy: 0.9425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YUh8_cNqq1-"
      },
      "source": [
        "Výsledok trénovania si môžeme aj uložiť."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWw-yIlxqqCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "d00730bb-ccd7-4529-b7f0-783a3abe591b"
      },
      "source": [
        "plt.plot(np.arange(50), history.history['val_loss'])\n",
        "plt.plot(np.arange(50), history.history['loss'])\n",
        "plt.show()\n",
        "plt.plot(np.arange(50), history.history['val_accuracy'])\n",
        "plt.plot(np.arange(50), history.history['accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}